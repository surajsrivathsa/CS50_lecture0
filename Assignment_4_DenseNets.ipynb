{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4: DenseNets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VH1GhRfVTMU4",
        "EfXPwgGOSgOv",
        "E5S24GKeDTEc",
        "5qUzBirycM_-",
        "PdL2QFiXhfOW",
        "wlYhLwQbi2-1"
      ],
      "authorship_tag": "ABX9TyO0pNI+7MSc23uXG5xY8Bk6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajsrivathsa/ovgu_deeplearning/blob/master/Assignment_4_DenseNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_9dD0WiFPt",
        "colab_type": "text"
      },
      "source": [
        "Deep Learning programming task\n",
        "\n",
        "**Assignment 4:** DenseNets\n",
        "\n",
        "**Team members:**\n",
        "1. Sanjeeth Busnur Indushekar: 224133 : sanjeeth.busnur@st.ovgu.de\n",
        "2. Aditya Dey : 230580 : aditya.dey@st.ovgu.de\n",
        "3. Suraj Shashidhar: 230052 : suraj.shashidhar@st.ovgu.de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16GTsDxJd8Dt",
        "colab_type": "text"
      },
      "source": [
        "**Tasks to be done:**\n",
        "\n",
        "1) DenseNet. Thoroughly experiment with (hyper)parameters. Try to achieve the best performance you can on CIFAR10/100.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2) Use tf.function to record improvements in execution time for couple of cases\n",
        "\n",
        "**Densenet paper summary:**\n",
        "\n",
        "\n",
        "1) Separate dense block where feature maps are concatenated and transition block where feature maps are reduced and pooled\n",
        "\n",
        "2) Inside dense block we can enable bottlenect layer of 1 - 1 conv before 3 - 3 conv to reduce number of parameters and also increase some accuracy by removing some really redundant maps.\n",
        "\n",
        "3) compression used inside transition block to dictate number of featuremaps that would be passed from one dense block to other. This acts as again redundant map reducer.\n",
        "\n",
        "4) growth rate used to control the number of layers and concatenations inside dense block. For lesser growth rate we will have less layers and less concatenations, hence less feature maps.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adh3WsE1dmkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zPOCe1KwKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import initializers\n",
        "import tensorboard\n",
        "import time\n",
        "from datetime import datetime\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0iZWwrneCIk",
        "colab_type": "code",
        "outputId": "3dd15497-3b10-448f-c919-744b55d857f6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7f4f298f-230d-43e0-ad7c-a595761bd43b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7f4f298f-230d-43e0-ad7c-a595761bd43b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving datasets.py to datasets.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'datasets.py': b'import numpy as np\\n\\n\\nclass MNISTDataset:\\n    \"\"\"\\'Bare minimum\\' class to wrap MNIST numpy arrays into a dataset.\"\"\"\\n    def __init__(self, train_imgs, train_lbs, test_imgs, test_lbls, batch_size,\\n                 to01=True, shuffle=True, seed=None):\\n        \"\"\"\\n        Use seed optionally to always get the same shuffling (-> reproducible\\n        results).\\n        \"\"\"\\n        self.batch_size = batch_size\\n        self.train_data = train_imgs\\n        self.train_labels = train_lbs.astype(np.int32)\\n        self.test_data = test_imgs\\n        self.test_labels = test_lbls.astype(np.int32)\\n\\n        if to01:\\n            # int in [0, 255] -> float in [0, 1]\\n            self.train_data = self.train_data.astype(np.float32) / 255\\n            self.test_data = self.test_data.astype(np.float32) / 255\\n\\n        self.size = self.train_data.shape[0]\\n\\n        if seed:\\n            np.random.seed(seed)\\n        if shuffle:\\n            self.shuffle_train()\\n        self.shuffle = shuffle\\n        self.current_pos = 0\\n\\n    def next_batch(self):\\n        \"\"\"Either gets the next batch, or optionally shuffles and starts a\\n        new epoch.\"\"\"\\n        end_pos = self.current_pos + self.batch_size\\n        if end_pos < self.size:\\n            batch = (self.train_data[self.current_pos:end_pos],\\n                     self.train_labels[self.current_pos:end_pos])\\n            self.current_pos += self.batch_size\\n        else:\\n            # we return what\\'s left (-> possibly smaller batch!) and prepare\\n            # the start of a new epoch\\n            batch = (self.train_data[self.current_pos:self.size],\\n                     self.train_labels[self.current_pos:self.size])\\n            if self.shuffle:\\n                self.shuffle_train()\\n            self.current_pos = 0\\n            # print(\"Starting new epoch...\")\\n        return batch\\n\\n    def shuffle_train(self):\\n        shuffled_inds = np.arange(self.train_data.shape[0])\\n        np.random.shuffle(shuffled_inds)\\n        self.train_data = self.train_data[shuffled_inds]\\n        self.train_labels = self.train_labels[shuffled_inds]\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO6eemPNfoat",
        "colab_type": "code",
        "outputId": "686f1dbd-5a8a-49c8-ac3a-b9e4a05c5f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "  print(os.getcwd())\n",
        "  print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F420Gu-7f3x8",
        "colab_type": "code",
        "outputId": "7c88fb82-7c1e-401d-8449-041625be16d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676Zm3kMgNJP",
        "colab_type": "code",
        "outputId": "f40b2301-f26d-4059-cd7b-653c32467e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "path = '.'\n",
        " \n",
        "files = os.listdir(path)\n",
        "for name in files:\n",
        "    print(name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".config\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH1GhRfVTMU4",
        "colab_type": "text"
      },
      "source": [
        "# **Basic Hardcoded Densenet on CIFAR-10**\n",
        "\n",
        "**Summary**: Hardcoded Densenetwork with growth rate of 2 and no average pooling at the end. We observed **70% test accuracy** and **85% train accuracy** for batch size of 64 and epochs between **12 - 15**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c6qOipGYTksi",
        "outputId": "c54d3a28-52e7-44ad-d308-0a131f268683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mnist = tf.keras.datasets.cifar10\n",
        "(train_images_raw, train_labels_raw), (test_images_raw, test_labels_raw) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WXK1LI6rToUL",
        "outputId": "e99b3dea-8a30-491b-ac81-c6e6b0b782d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "print(train_images_raw.shape)\n",
        "print(train_labels_raw.shape)\n",
        "print(test_images_raw.shape)\n",
        "print(test_labels_raw.shape)\n",
        "print(train_labels_raw[0])\n",
        "plt.imshow(train_images_raw[9], cmap=\"Greys_r\")\n",
        "print(train_labels_raw[9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 1)\n",
            "[6]\n",
            "[3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAcIklEQVR4nO2dbWyk1XXH/2eembFnbK9fds3ifYGFDU1K04REFkoUGlGiRDSKRCK1KKlE+UCyURukRko/ICo1VGqkpGoS5UOValNQSJUm0CQoqKItlCahSVNYQ2FZ2LALu17wsut9sb322jMezzynH2ZoDbnn2Duel4X7/0mWx/f4Pvc+d54zz8z9zzlHVBWEkLc+mW5PgBDSGejshEQCnZ2QSKCzExIJdHZCIoHOTkgkZDfSWURuBPANAAmAv1fVL3v/XywWdGhoMGirrayY/dI0NcZPnMnZpp6enqZsFpVKxbSVFxdN2/Lysn1QZ/4Q25jJhF+/k4y9VknSpC1rXz5Wv0zmwvsAQCax70tinDMAZMSwOX2apWkR2+zoHNG4Bk68ehxzs7NBY9POLnVP+1sAHwYwBWCfiDyoqs9bfYaGBvHpT98StJ07ecIcq7xYDrZne/rsCTpP5u637TZtV+62bTC+k3B86hWzy/P79pm2ySNHTFvNuRYzOftp6ykUg+1DA5vMPpsGwy/Aa9mGR4ZN2+DgSLC92G/3GRiwxyr0h88LAHqLjq0QvkaSfMHskzqvtOHbTh1t9vWjFr6urJscYL/4/dEf/oHd58Jm9TquBfCiqh5R1QqA7wO4aQPHI4S0kY04+3YAq29pU402QshFSNs36ERkj4hMiMjE0tJSu4cjhBhsxNmPA9i56u8djbbXoap7VXVcVceLzmcrQkh72Yiz7wNwlYhcISJ5AJ8E8GBrpkUIaTVN78aralVEbgfwb6hLb/eo6nNenySbw/DotqBtdPNWs99lOy4Ptg+PbDH7VCRn2iSbN21eFGC5XAq2v/3SXWaf3e94l2k7cuiQaTs3O2Pa5mZs28vHjgbbX3k53A4AWUfmK+TtdaxV7I9luWxYRuvttXfjsz29pq13wFZeCgP9pm1o82i4fSR8HQLA4JA9x/5BW9UYcGyF/gHTlvSE3/F60mbWkCk9xXZDOruqPgTgoY0cgxDSGfgNOkIigc5OSCTQ2QmJBDo7IZFAZyckEja0G3+h9PYW8Btv/82g7fALh81+Z84tBNuLTuBET8GWjMrl86Ytn7dlubQSlt4Wl20JavSSMdP2/u27TNvxlydN29K5OfuYH7gu2H5i+te+7/R/5HN2pN+QIxkd2G8H+fzs0bBIUztlB/9kMrZwpE6kX9JjP2fW85mk9vFyzjWQdaIii312cM2gIy0PjOwItg8Ph4OJAGDz5s3B9qWFsK8AvLMTEg10dkIigc5OSCTQ2QmJBDo7IZHQ0d34JMlgeCC8u3vl264y+029cizYPjMzbfbZ5O3U99q7pvnEDoTpy4dfG0tlOwed1uxd32rVNGFw0A7GqCyHVQEAqNbCc9nppNsq9A6Ztv6ibduy8wrTtmQEFD38wH1mn6Rqr30+sdWVXGqvf1oK2zI1O+dh2VEFUkcVOO0krdIXbbUJiREI4+QNtHIlzp49bfbhnZ2QSKCzExIJdHZCIoHOTkgk0NkJiQQ6OyGR0FHprbxUwsFnnwnaNm2+xOxXyIZfk2bPnjL7lAzJBQAuudRJb5+pmaYVo+RHxZGMJLVtGceWc6q+DA/buc5+8YufBNsHCnYAx9W/da1pWzZkIQCo2EuFTaOXBttXsrbsOTs7a9qKWVvWKjqyXI+Rx02y9np4ZZycpwzqJIBTdWrJVMLBK14+xIWlsK1atSVF3tkJiQQ6OyGRQGcnJBLo7IREAp2dkEigsxMSCRuS3kRkEsACgBqAqqqOe/9fra1gZi4clXPg6cfNfrlqWLa49IpwWSgAqBh9AKDYb5cSKhbtnHFqvDY6Q2Gp5OQEs4OasFJZNm2/euZJ0/bUTx8Otvf12ec8Nmqf89adToSgIw/+9tXvDrZnb/kTs89xI7oRAM7NnTFtC/N2Oazz8+F8fYuLi2afUsmOKlxZsaUtdUQ7Efu+mjfkyHzOlhStIqnJtH1erdDZf1dV7WeCEHJRwLfxhETCRp1dATwsIk+KyJ5WTIgQ0h42+jb+OlU9LiKXAHhERH6lqo+t/ofGi8AeABgasrPHEELay4bu7Kp6vPH7FIAHAPzal6xVda+qjqvqeF+f/T1rQkh7adrZRaRPRAZeewzgIwAOtGpihJDWspG38VsBPCD1BHxZAP+oqv/qdUiSBJsGw2/ljy7ZJZnOnAwnliyltgwysMWOohMnaWCht9e0bR7dFmzPZm2JZLlkl4YqFOwyQ4cPHTRtv/z5f5q2TC0cijZ3xhZMXp16xbT1DITLDAFAvthv2oaMhJm/c/0NZh+v/FOpbEtKS0u2vLm4cC7YPj1ly3yTR4+atsMvvmjaPHlzx46dpm2zURqqULBlz5GRcGmoI1/6ktmnaWdX1SMAwmIqIeSig9IbIZFAZyckEujshEQCnZ2QSKCzExIJHU04CckARqK/oeGwlAAA00cmg+29jqw1P/Wyfbxpu0bck089ZdquNiK5in12AsjKctm0OUoT9j/1hGk7Z0RyAUC1Gpbe0podmudMw016uFKxpc/zGpbKjGAtAEBPzpaaCs4aDw7bMmtvPiyL5jO2XDp/zr6ubrjBrpm3dWtYQgOA/gF7/tne8KKkqf2c9RoScd6oAQfwzk5INNDZCYkEOjshkUBnJyQS6OyEREJHd+NVFWUjYVve2JEEgMQo4VNdsUs8adZO8HbyVbts1EtH7aCQX/7yv4PtGaf8UDaxl3h0ZMi0YcXexTeqYQEAFubDQSGbB+yglXyPHZAjGXuwWmrXf0qN2lC5nD3W4FA4eAbw1YRy2V6rQy+EA4p+8dP/MPtMTh4xbdu22aXDzsyeNW3qaB7Z3nAATdbJQVc1cuEtnLcDynhnJyQS6OyERAKdnZBIoLMTEgl0dkIigc5OSCR0VHpLsjkMGbnhpg/bOdeySVhGKzuBMMjbp5bLOjnoeux+55fCJZksGQQA0qwtNc07JY1qTs61wSFbsquk4cCV8rJdTuq8I9d40uH5sn3MTUbgR7piS2hWrkEAWFy088y94OTrm9gXLit25MgL9ljOehw99pJpyznlsFK1r7lMEr5GEuO6B4BqtRpsn5ubtccxLYSQtxR0dkIigc5OSCTQ2QmJBDo7IZFAZyckEtaU3kTkHgAfA3BKVd/ZaBsBcB+AXQAmAdysqvaef4N8Po+dO3cFbYf2/ZfZ7+y5cAmf0qwt/ezYdZlpyzjlnzJOlJfVTdWWk1INSyQAUDUiwwCgr2CXoZpfsGWohcXwmhSc8/Ly7k2eCq89AAwYJZ4AoK8YjuTKix3JdejQr0zb7Nxp0zY5edjpF45Eq6m99mrIlwDchH01o/RW/Zh2P03DB/Xy/1nX6YojA6/nzv5tADe+oe0OAI+q6lUAHm38TQi5iFnT2Rv11mfe0HwTgHsbj+8F8PEWz4sQ0mKa/cy+VVVPNB6fRL2iKyHkImbDG3Ra/2BhfrgQkT0iMiEiE3Nzdr5zQkh7adbZp0VkDAAav808T6q6V1XHVXV8yPlONyGkvTTr7A8CuLXx+FYAP27NdAgh7WI90tv3AFwPYIuITAH4IoAvA7hfRG4DcAzAzesZLCMZFJOwpDRmSHIAsFIIl7SpLtsyw3LFli3m5u0EhStOdFLOkMPESYZYcyLDqk4JIk3sMj7ZHifB5XJY/llW+3X9wGFbujr75NOmrVhwklgaSULVWd+SE8WYelKZo2slZjJQO6IMGfvaceUwJ0IQiaPZGcf0xrI0QPESWzpHe23ATxmmD63VlxBy8cBv0BESCXR2QiKBzk5IJNDZCYkEOjshkdDRhJNpLUV5ISyvbN+20+zXPzQSbC9Nl8w+M7N2tNaikTgSsBP5AQAyYVkjrTkJJ2v28Sr2Fw8xOz9v2vJ5W3oTY46lZbsu3vllW4pcXvHWypbDEuM+4ihvbl05L1IxTb2oQ+t4nqxlU3NkVp8LH8+T3swITGcc3tkJiQQ6OyGRQGcnJBLo7IREAp2dkEigsxMSCR2V3lRTLJfDcplXU2x4UzixYdU4Vn0w27RUsvvls3Y0VKkclqhSJ8lf1ol2ctQkZJzIq3LZjg7LiPH67QxWqdiynIcnDVlRauqdtCOh2SKfjzXH1JOoDPkSAMSbf5OY6+isbzPCIe/shEQCnZ2QSKCzExIJdHZCIoHOTkgkdDYQJq1haSlcJeqYU8Kn0JsPtg9tGjD7LHtlcJyM1qObw0E3gL1rXVqyd8crzjwqFWcX31EFksR+jV5ZCQfeeEErNWcX3N8RdnbjrUN6ASjOTrcfFOL0MyZiBQx1A+vc3B13Nz9dGN7ZCYkEOjshkUBnJyQS6OyERAKdnZBIoLMTEgnrKf90D4CPATilqu9stN0F4DMATjf+7U5VfWitYy0uLuCJfT8L2o6/fNTsl8uGZYbF87aGlu0tmLb+frts0Y6xMdN2biY83mzNlrUKRukqAJh1qto66dhQdfKglUqLwfYEYfkSQFMyzlqYapgXSNKk9ObR6jNzZT5PpmzxGjdzvPXc2b8N4MZA+9dV9ZrGz5qOTgjpLms6u6o+BmCmA3MhhLSRjXxmv11E9ovIPSISDjgnhFw0NOvs3wSwG8A1AE4A+Kr1jyKyR0QmRGRiaclJNkEIaStNObuqTqtqTeuFsb8F4Frnf/eq6riqjheL9qYZIaS9NOXsIrJ6y/oTAA60ZjqEkHaxHuntewCuB7BFRKYAfBHA9SJyDerKxiSAz65nsOVyCS+9EH5dmDlzxux35ZWXB9t7Cr1mn3LFKbtUscsd5bL2658YmdASR45ZcD66aMaObOtxpMPq4oJ9TEMGrKT2elglkuo0Fx1mHdKTrpq1vRlotfSW8bRZgzWdXVU/FWi++4JHIoR0FX6DjpBIoLMTEgl0dkIigc5OSCTQ2QmJhI4mnKxWVnBm6njQlta8skDhaRaKQ2aXU6enTFt/wY56WzgfTogJALl8eI5loywUAJScykqF4ibTdu6cPQ+t2okqi4W+YPt8yY7MS6tOKSRX8nIiwAzxzT1aJ0srOWQcSbSTkW2tliJ5ZyckEujshEQCnZ2QSKCzExIJdHZCIoHOTkgkdFR6q6Up5kthmaqYsyPY5o3EjFkn6q3o2HLOWS+Xl01bfzEsa5XLTmTbsi2Traity2nVsTkKT80wekkqPUFMxL4fXAxJFNsxVuJElKVOv5qTeLTVpF59PgPe2QmJBDo7IZFAZyckEujshEQCnZ2QSOjobnyqilIlvDudwM6RNnPm1WD76NZLzT7bt11i2np77FJIM2ftXHhnTp8Ntqc1JzAlY9vyTsDFJdvsczt55pxpm50/H2xvfje+ueAUq1+z5ZNajTdWzdnp9nK/eefm7dQ3k0+OgTCEEBM6OyGRQGcnJBLo7IREAp2dkEigsxMSCesp/7QTwHcAbEW9qs9eVf2GiIwAuA/ALtRLQN2sqnbiNACa1lAthWWj1HvdqYVtorZcl83a8smlY7asdcmWrabtX156KNi+bWyb2aeQM01YKtvBLosrtlRTdeo1WeuYyXi500yTS6tzpHnBHZ5U5o8V7uedsjePZmSytfpZtlbnu1vPzKsAvqCqVwN4H4DPicjVAO4A8KiqXgXg0cbfhJCLlDWdXVVPqOpTjccLAA4C2A7gJgD3Nv7tXgAfb9ckCSEb54Lek4jILgDvAfA4gK2qeqJhOon623xCyEXKup1dRPoB/BDA51V1frVN6x8ggh8iRGSPiEyIyETNrw1MCGkj63J2Ecmh7ujfVdUfNZqnRWSsYR8DcCrUV1X3quq4qo4nmTd3jW1C3sys6exS3+q8G8BBVf3aKtODAG5tPL4VwI9bPz1CSKtYT9TbBwDcAuBZEXm60XYngC8DuF9EbgNwDMDNax0on83gsi3FoG3zSLgdAIaGw9sBOad8Urlmy1qnzwTfhAAALt++27Tt3H5ZsH10i12GqupExL363EHTdmZuwbRVnAA2MWQcEe8jVOs/XjUjDfkSmifzuUc1WjsbBehJb0kSjn6sVm1puRnWdHZV/Tnss/9QS2dDCGkb/AYdIZFAZyckEujshEQCnZ2QSKCzExIJHU042ZPPYvfOLUFbcaDf7JfrC0tbx161k0OeXZg3bUuLjix32Yxpu3T7WLjP6ZNmnyOTr5i24ydPmzaInYxSPZvxLcVmJaNW40lyGedLV+rJg06Umnnaznqkakccqnr3R09udNa/maemiT68sxMSCXR2QiKBzk5IJNDZCYkEOjshkUBnJyQSOiq9JUkGfYN9QVumx44cWzISTqaJ/VqVFbueW6HHlq4WFu06aosrS8H2I5NHzT4zM7YE6CWOdCOvHJstbdlr1Wxiw6bkPCf6Tp3DZR1ZLnUkLzVkudSNbLPXaqVmR6LV1ElU6ZxbxnBD77yaiVTknZ2QSKCzExIJdHZCIoHOTkgk0NkJiYTO7sZncxjcEi699PIJO+fasRPhgJGasxtcKdm7puWSHQgzt1g2bZILL9eyU6rJ23DPZu3lT2vO7rMT+GGaxMu5ZtP8Tn24PesoKKmzm63OpSq5HrtfLXzMxAuEqTmlt2reejg7/E4AjUj43MR7zsSYo7vrTwiJAjo7IZFAZyckEujshEQCnZ2QSKCzExIJa0pvIrITwHdQL8msAPaq6jdE5C4AnwHwmi52p6o+5B0rBbBsKGJTr9olmaaMXG0VT9dK7dexasWW5Yp94UAdAMhWw1JIbcULxHByruWc4BRHdfGkN2s0cV7XvdJEHqlzbpayJV4AhyPl1Rw5LMnYgU1WOay8FxiUNBNotIYkakiAAJBWloPtGS+wJjFyDZo91qezVwF8QVWfEpEBAE+KyCMN29dV9W/WcQxCSJdZT623EwBONB4viMhBANvbPTFCSGu5oPdvIrILwHsAPN5oul1E9ovIPSIy3OK5EUJayLqdXUT6AfwQwOdVdR7ANwHsBnAN6nf+rxr99ojIhIhMLDlfUyWEtJd1ObuI5FB39O+q6o8AQFWnVbWmqimAbwG4NtRXVfeq6riqjhcLdvYYQkh7WdPZpZ576G4AB1X1a6vaV5dH+QSAA62fHiGkVaxnN/4DAG4B8KyIPN1ouxPAp0TkGtTVnkkAn13rQGktRWkxnMdtZWXF7JcxcoLVVryPBbZs4UVeJY60kjVMeUfwSHvsiKxK1ZaTfBHFk6+Mo3nRUF5+t+aC5cxjivO8JLDXI+Occ6ZmRyomxjwKTsRhNutIeU7prapzDVcd6Q2w+jlrZciDZ708fs4MAACq+nOErzxXUyeEXFzwG3SERAKdnZBIoLMTEgl0dkIigc5OSCR0NOGkpjWUz4cTS1ZLJbOfWEkDHTmm5pTp8eQTXQlHIAFOCSJH7tCeXtNWVXusStWev7qyXJiaF5HlJpW84KEa/cJz9MoueXeeYtaefzFnH3NTMSx9Fov285JJ7OvDSxLqRQ+qE8HWTHLOXD5sm56dNPvwzk5IJNDZCYkEOjshkUBnJyQS6OyERAKdnZBI6Kz0poq0Go5QGtmUM/tlDdnFSl4JAJrasfO5xB4rn3VsRmLDWmr3OedIaL1G7TgAqPY6dewqtoxTNZJfetFrnizn1nNzZLTESIiYz9qRbYN9thy2dWTQ7lew17E3H37OMlmv9pp3Xl60nH0deMeUTHitEkcCTAxZLp+fMvvwzk5IJNDZCYkEOjshkUBnJyQS6OyERAKdnZBI6Kj0JlCIkVxvdMSWykY3hyWNNPUSFNqJHpNMc6dt1fLyanxtWrKTYuZ67LpyXhLI5bJ93kbZsKblNc+WcWqs5Y06doW8nZSx34hQA4BioWjaLBkKABIjEi3j1HPzro9MxpbXvHuneklCzW5eLcDw8azkrP7RCCFvKejshEQCnZ2QSKCzExIJdHZCImHNbWkR6QXwGICexv//QFW/KCJXAPg+gM0AngRwi6quXabV2N3NOoEJli2XswMncom9s+sljfN2n2u18C54pWIHu3g7uwOb7B3m1FlKgb0LDsMmGXsHX8RLNOcEcDjBHRnD5t1dvBJVbiCJswNt9UucYKjEURm83XgRbxffC4QJ29RbLSPHn6eQrOfOvgzgBlV9N+rlmW8UkfcB+AqAr6vq2wDMArhtHccihHSJNZ1d65xv/Jlr/CiAGwD8oNF+L4CPt2WGhJCWsN767EmjguspAI8AeAnAnKq+9v51CsD29kyRENIK1uXsqlpT1WsA7ABwLYB3rHcAEdkjIhMiMlHysk0QQtrKBe3Gq+ocgJ8AeD+AIfn/HYkdAI4bffaq6riqjhd6OvrtXELIKtZ0dhEZFZGhxuMCgA8DOIi60/9+499uBfDjdk2SELJx1nOrHQNwr9RrJmUA3K+q/ywizwP4voj8FYD/AXD3egYUIzDBy7eVz4fljt5eJ2+dI614udO8oBZLelOnTzFXMG05JxijaowFAJKxx7NiQnzpx5GuvFJTXhUqQ83zykl50psrKbmanbUgnrzmjdVkP2eNE+s6UO95MQJ8nLVY09lVdT+A9wTaj6D++Z0Q8iaA36AjJBLo7IREAp2dkEigsxMSCXR2QiJBvCivlg8mchrAscafWwCc6djgNpzH6+E8Xs+bbR6Xq+poyNBRZ3/dwCITqjrelcE5D84jwnnwbTwhkUBnJyQSuunse7s49mo4j9fDebyet8w8uvaZnRDSWfg2npBI6Iqzi8iNIvKCiLwoInd0Yw6NeUyKyLMi8rSITHRw3HtE5JSIHFjVNiIij4jI4cbv4S7N4y4ROd5Yk6dF5KMdmMdOEfmJiDwvIs+JyJ822ju6Js48OromItIrIk+IyDONefxlo/0KEXm84Tf3iYhdMy2Eqnb0B/X0py8BuBJAHsAzAK7u9Dwac5kEsKUL434QwHsBHFjV9tcA7mg8vgPAV7o0j7sA/FmH12MMwHsbjwcAHAJwdafXxJlHR9cE9eDh/sbjHIDHAbwPwP0APtlo/zsAf3whx+3Gnf1aAC+q6hGtp57+PoCbujCPrqGqjwGYeUPzTagn7gQ6lMDTmEfHUdUTqvpU4/EC6slRtqPDa+LMo6NonZYnee2Gs28H8Mqqv7uZrFIBPCwiT4rIni7N4TW2quqJxuOTALZ2cS63i8j+xtv8tn+cWI2I7EI9f8Lj6OKavGEeQIfXpB1JXmPfoLtOVd8L4PcAfE5EPtjtCQH1V3aYuV7azjcB7Ea9RsAJAF/t1MAi0g/ghwA+r6rzq22dXJPAPDq+JrqBJK8W3XD24wB2rvrbTFbZblT1eOP3KQAPoLuZd6ZFZAwAGr9PdWMSqjrduNBSAN9Ch9ZERHKoO9h3VfVHjeaOr0loHt1ak8bYF5zk1aIbzr4PwFWNncU8gE8CeLDTkxCRPhEZeO0xgI8AOOD3aisPop64E+hiAs/XnKvBJ9CBNZF6jaa7ARxU1a+tMnV0Tax5dHpN2pbktVM7jG/Ybfwo6judLwH48y7N4UrUlYBnADzXyXkA+B7qbwdXUP/sdRvqNfMeBXAYwL8DGOnSPP4BwLMA9qPubGMdmMd1qL9F3w/g6cbPRzu9Js48OromAN6FehLX/ai/sPzFqmv2CQAvAvgnAD0Xclx+g46QSIh9g46QaKCzExIJdHZCIoHOTkgk0NkJiQQ6OyGRQGcnJBLo7IREwv8CCap0jnvfVBUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJGXCtzFToUP",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(( train_images_raw.astype(np.float32)/255.0, train_labels_raw.astype(np.int32) ))\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size = train_images_raw.shape[0]).batch(512).repeat(2)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(( test_images_raw.astype(np.float32)/255.0, test_labels_raw.astype(np.int32) ))\n",
        "\n",
        "test_data = test_data.batch(512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qu2UxD_UA2J",
        "colab_type": "code",
        "outputId": "cd86e407-2a5b-49d6-8892-b98f9213cb21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_shape = (train_images_raw.shape[1], train_images_raw.shape[2], train_images_raw.shape[3])\n",
        "print(input_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy0tCzJ-UDk1",
        "colab_type": "code",
        "outputId": "a50be47c-04b9-4fd5-e7b1-f8ce0df8e2c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "# Get the model.\n",
        "inputs = keras.Input(shape = input_shape, name='cifar')\n",
        "# x = layers.Conv2D(filters = 64, kernel_size=(1, 1), padding = \"same\", activation=tf.nn.relu, name = \"conv_preprocess\") (inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer = initializers.he_normal(seed=0), bias_initializer = initializers.glorot_uniform(seed=0), name = \"conv_1\" ) (inputs)\n",
        "# x = layers.AveragePooling2D(pool_size=(2,2)) (x)\n",
        "\n",
        "# first denseblock block r1\n",
        "x_0_saved = x\n",
        "\n",
        "x_1 = layers.Conv2D(filters=32, kernel_size=(3, 3), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_dens1a\") (x)\n",
        "x_1 = layers.BatchNormalization(name = 'r1a')(x_1)\n",
        "x_1_saved = x_1\n",
        "\n",
        "x_1 = tf.concat([x_0_saved, x_1], axis = -1, name = \"concat1\")\n",
        "\n",
        "x_2 = layers.Conv2D(filters=32, kernel_size=(3, 3), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_dens1b\") (x_1)\n",
        "x_2 = layers.BatchNormalization( name = 'r2a')(x_2)\n",
        "x_2_saved = x_2\n",
        "\n",
        "x_2 = tf.concat([x_0_saved, x_1_saved, x_2_saved], axis = -1, name = \"concat2\")\n",
        "\n",
        "# 1st transition layer\n",
        "transition_1 = layers.Conv2D(filters=24, kernel_size=(1, 1), strides=(2, 2), padding = \"valid\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_trans1\") (x_2)\n",
        "\n",
        "# 2nd dense block\n",
        "x_3 = layers.Conv2D(filters=16, kernel_size=(3, 3), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_dens2a\") (transition_1)\n",
        "x_3 = layers.BatchNormalization( name = 'r3a')(x_3)\n",
        "x_3_saved = x_3\n",
        "\n",
        "x_3 = tf.concat([x_3, transition_1], axis = -1, name = \"concat3\")\n",
        "\n",
        "x_4 = layers.Conv2D(filters=16, kernel_size=(3, 3), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_dens2b\") (x_3)\n",
        "x_4 = layers.BatchNormalization( name = 'r4a')(x_4)\n",
        "x_4_saved = x_4\n",
        "\n",
        "x_4 = tf.concat([transition_1, x_3_saved, x_4_saved], axis = -1, name = \"concat4\")\n",
        "\n",
        "# 2nd transition layer and dimensionality reducer\n",
        "transition_2 = layers.Conv2D(filters=16, kernel_size=(1, 1), strides=(2, 2), padding = \"valid\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_trans2\") (x_4)\n",
        "\n",
        "\n",
        "# 3rd dense block\n",
        "x_5 = layers.Conv2D(filters=16, kernel_size=(3, 3), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_dens3a\") (transition_2)\n",
        "x_5 = layers.BatchNormalization( name = 'r5a')(x_5)\n",
        "x_5_saved = x_5\n",
        "\n",
        "x_5 = tf.concat([x_5, transition_2], axis = -1, name = \"concat5\")\n",
        "\n",
        "x_6 = layers.Conv2D(filters=16, kernel_size=(3, 3), padding = \"same\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_dens3b\") (x_5)\n",
        "x_6 = layers.BatchNormalization( name = 'r6a')(x_6)\n",
        "x_6_saved = x_6\n",
        "\n",
        "x_6 = tf.concat([transition_2, x_5_saved, x_6_saved], axis = -1, name = \"concat6\")\n",
        "\n",
        "# 3rd transition layer and dimensionality reducer\n",
        "transition_3 = layers.Conv2D(filters=12, kernel_size=(1, 1), strides=(2, 2), padding = \"valid\", activation = tf.nn.relu, use_bias=True, kernel_initializer= initializers.he_normal(seed=0), bias_initializer= initializers.glorot_uniform(seed=0), name = \"conv_trans2\") (x_6)\n",
        "\n",
        "\n",
        "# flattening the tensor and connect it to a fully connected layer\n",
        "x_reduce_dim = layers.Flatten() (transition_2)\n",
        "x_dense_1 = layers.Dense(128, activation='relu', name='dense_1')(x_reduce_dim)\n",
        "outputs = layers.Dense(10, name='predictions', activation='softmax')(x_dense_1)\n",
        "\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 32, 32, 64)   1792        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_dens1a (Conv2D)            (None, 32, 32, 32)   18464       conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "r1a (BatchNormalization)        (None, 32, 32, 32)   128         conv_dens1a[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat1 (TensorFlow [(None, 32, 32, 96)] 0           conv_1[0][0]                     \n",
            "                                                                 r1a[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv_dens1b (Conv2D)            (None, 32, 32, 32)   27680       tf_op_layer_concat1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "r2a (BatchNormalization)        (None, 32, 32, 32)   128         conv_dens1b[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat2 (TensorFlow [(None, 32, 32, 128) 0           conv_1[0][0]                     \n",
            "                                                                 r1a[0][0]                        \n",
            "                                                                 r2a[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv_trans1 (Conv2D)            (None, 16, 16, 24)   3096        tf_op_layer_concat2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv_dens2a (Conv2D)            (None, 16, 16, 16)   3472        conv_trans1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "r3a (BatchNormalization)        (None, 16, 16, 16)   64          conv_dens2a[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat3 (TensorFlow [(None, 16, 16, 40)] 0           r3a[0][0]                        \n",
            "                                                                 conv_trans1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv_dens2b (Conv2D)            (None, 16, 16, 16)   5776        tf_op_layer_concat3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "r4a (BatchNormalization)        (None, 16, 16, 16)   64          conv_dens2b[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat4 (TensorFlow [(None, 16, 16, 56)] 0           conv_trans1[0][0]                \n",
            "                                                                 r3a[0][0]                        \n",
            "                                                                 r4a[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv_trans2 (Conv2D)            (None, 8, 8, 16)     912         tf_op_layer_concat4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1024)         0           conv_trans2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          131200      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           1290        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 194,066\n",
            "Trainable params: 193,874\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06fowdtFzzh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcf4-Ohaz3iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGOMBaKhz3qk",
        "colab_type": "code",
        "outputId": "fc824dbd-f79b-43f8-f417-b4e62f7fc7f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "train_features = train_images_raw.astype(np.float32)/255.0 \n",
        "test_features = test_images_raw.astype(np.float32)/255.0 \n",
        "\n",
        "train_labels = convert_to_one_hot(train_labels_raw, 10).T\n",
        "test_labels = convert_to_one_hot(test_labels_raw, 10).T\n",
        "print(np.shape(train_labels))\n",
        "print(np.shape(train_features))\n",
        "print(train_labels[0:3,:])\n",
        "print(train_features[0:1,:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 10)\n",
            "(50000, 32, 32, 3)\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "[[[[0.23137255 0.24313726 0.24705882]\n",
            "   [0.16862746 0.18039216 0.1764706 ]\n",
            "   [0.19607843 0.1882353  0.16862746]\n",
            "   ...\n",
            "   [0.61960787 0.5176471  0.42352942]\n",
            "   [0.59607846 0.49019608 0.4       ]\n",
            "   [0.5803922  0.4862745  0.40392157]]\n",
            "\n",
            "  [[0.0627451  0.07843138 0.07843138]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.07058824 0.03137255 0.        ]\n",
            "   ...\n",
            "   [0.48235294 0.34509805 0.21568628]\n",
            "   [0.46666667 0.3254902  0.19607843]\n",
            "   [0.47843137 0.34117648 0.22352941]]\n",
            "\n",
            "  [[0.09803922 0.09411765 0.08235294]\n",
            "   [0.0627451  0.02745098 0.        ]\n",
            "   [0.19215687 0.10588235 0.03137255]\n",
            "   ...\n",
            "   [0.4627451  0.32941177 0.19607843]\n",
            "   [0.47058824 0.32941177 0.19607843]\n",
            "   [0.42745098 0.28627452 0.16470589]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.8156863  0.6666667  0.3764706 ]\n",
            "   [0.7882353  0.6        0.13333334]\n",
            "   [0.7764706  0.6313726  0.10196079]\n",
            "   ...\n",
            "   [0.627451   0.52156866 0.27450982]\n",
            "   [0.21960784 0.12156863 0.02745098]\n",
            "   [0.20784314 0.13333334 0.07843138]]\n",
            "\n",
            "  [[0.7058824  0.54509807 0.3764706 ]\n",
            "   [0.6784314  0.48235294 0.16470589]\n",
            "   [0.7294118  0.5647059  0.11764706]\n",
            "   ...\n",
            "   [0.72156864 0.5803922  0.36862746]\n",
            "   [0.38039216 0.24313726 0.13333334]\n",
            "   [0.3254902  0.20784314 0.13333334]]\n",
            "\n",
            "  [[0.69411767 0.5647059  0.45490196]\n",
            "   [0.65882355 0.5058824  0.36862746]\n",
            "   [0.7019608  0.5568628  0.34117648]\n",
            "   ...\n",
            "   [0.84705883 0.72156864 0.54901963]\n",
            "   [0.5921569  0.4627451  0.32941177]\n",
            "   [0.48235294 0.36078432 0.28235295]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fq42IiLz3vp",
        "colab_type": "code",
        "outputId": "ab27a6b6-08eb-4cfb-868c-793983634f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(train_features, train_labels, epochs = 10, batch_size = 64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 1.4775 - accuracy: 0.4609\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 1.1021 - accuracy: 0.6058\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.9291 - accuracy: 0.6690\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.8081 - accuracy: 0.7118\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.7214 - accuracy: 0.7442\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.6440 - accuracy: 0.7716\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.5753 - accuracy: 0.7948\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.5112 - accuracy: 0.8181\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.4518 - accuracy: 0.8375\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.3982 - accuracy: 0.8592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3ea0607b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stVm__faz3oB",
        "colab_type": "code",
        "outputId": "dfca66fe-efa5-45b7-881c-940f44904802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "preds = model.evaluate(test_features, test_labels)\n",
        "print(\"Loss = \" + str(preds[0]))\n",
        "print(\"Test Accuracy = \" + str(preds[1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 1.0752 - accuracy: 0.6846\n",
            "Loss = 1.075183629989624\n",
            "Test Accuracy = 0.6845999956130981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFB6apwF5zmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfXPwgGOSgOv",
        "colab_type": "text"
      },
      "source": [
        "# **Flexible Dense Network using functions with hyperparameters such as growthrate, bottleneck and compression**\n",
        "\n",
        "**Config 1:** growth rate = 5, compression = 0.5, dense blocks = 3, filters used inside dense blocks = 16, epochs = 10, **bottleneck layer = False**\n",
        "\n",
        "- Test accuracy: 85.22%\n",
        "\n",
        "**Config 2:** growth rate = 5, compression = 0.5, dense blocks = 3, filters used inside dense blocks = 16, epochs = 10, **bottleneck layer = True**, **bottle neck layer filters = 32**\n",
        "\n",
        "- Test accuracy = 87%\n",
        "\n",
        "We conclude that using** bottleneck incrweases parameter efficiency** and also increases accuracy a bit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4saiaNknSmAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_block(growth_rate, filter_size, inputs, bottleneck = False, bottleneck_channels = 16, dense_block_num = 1, kernel_size = (3,3) ):\n",
        "  alfabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\"]\n",
        "  gr = [x for x in range(1, growth_rate+1)]\n",
        "  temp = inputs\n",
        "\n",
        "  if not bottleneck:\n",
        "    for a, i in list(zip(alfabets, gr)):\n",
        "      conv_layer_name = \"conv_\" + str(dense_block_num) + a\n",
        "      batchnorm_layer_name = \"bn_\" + str(dense_block_num) + a\n",
        "      concat_layer_name = \"concat_\" + str(dense_block_num) + a\n",
        "\n",
        "      batchnorm = layers.BatchNormalization( name = batchnorm_layer_name)(temp)\n",
        "      relu = layers.Activation('relu')(batchnorm)\n",
        "      x = layers.Conv2D(filters=filter_size , kernel_size=kernel_size, padding = \"same\", name = conv_layer_name) (relu)\n",
        "      #print(x.shape)\n",
        "      \n",
        "      concat = tf.concat([x, temp], axis = -1, name = concat_layer_name) \n",
        "      #print(concat.shape)\n",
        "      temp = concat\n",
        "      #print(conv_layer_name + \" : \" + batchnorm_layer_name + \" : \" + concat_layer_name)\n",
        "      #print()\n",
        "  elif bottleneck:\n",
        "    for a, i in list(zip(alfabets, gr)):\n",
        "      bottleneck_layer_name = \"bottleneck_\" + str(dense_block_num) + a\n",
        "      conv_layer_name = \"conv_\" + str(dense_block_num) + a\n",
        "      batchnorm_layer_name = \"bn_\" + str(dense_block_num) + a\n",
        "      concat_layer_name = \"concat_\" + str(dense_block_num) + a\n",
        "\n",
        "      batchnorm1 = layers.BatchNormalization( name = batchnorm_layer_name + \"_1\")(temp)\n",
        "      relu1 = layers.Activation('relu')(batchnorm1)\n",
        "      botneck = layers.Conv2D(filters = bottleneck_channels, kernel_size = (1, 1), padding = \"same\", name = bottleneck_layer_name) (relu1)\n",
        "\n",
        "      batchnorm2 = layers.BatchNormalization( name = batchnorm_layer_name + \"_2\")(botneck)\n",
        "      relu2 = layers.Activation('relu')(batchnorm2)\n",
        "      x = layers.Conv2D(filters=filter_size , kernel_size=kernel_size, padding = \"same\", name = conv_layer_name) (relu2)\n",
        "      #print(x.shape)\n",
        "      \n",
        "      concat = tf.concat([x, temp], axis = -1, name = concat_layer_name) \n",
        "      #print(concat.shape)\n",
        "      temp = concat\n",
        "      #print(conv_layer_name + \" : \" + batchnorm_layer_name + \" : \" + concat_layer_name)\n",
        "      #print()\n",
        "\n",
        "  return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNXizXdKTINa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transition_block(inputs, filter_size, transition_layer_num = 1, compression = 0.5):\n",
        "  transition_layer_name = str(transition_layer_num)\n",
        "\n",
        "  batchnorm = layers.BatchNormalization( name = \"bn_tr_\" + transition_layer_name)(inputs)  \n",
        "  relu = layers.Activation('relu')(batchnorm)\n",
        "  \n",
        "  conv_1_1 = layers.Conv2D(filters = filter_size * compression, kernel_size=(1, 1), padding = \"same\", name = \"conv_tr\" + transition_layer_name) (relu)\n",
        "  avg_pool = layers.AveragePooling2D(pool_size = (2, 2), strides = (2, 2)) (conv_1_1)\n",
        "\n",
        "  return avg_pool\n",
        "  ;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ln_5YiwhMNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_block(inputs, num_classes=10, dense_size = 32, pool_size = (2, 2)):\n",
        "  # flattening the tensor and connect it to a fully connected layer\n",
        "  BatchNorm = layers.BatchNormalization()(inputs)\n",
        "  relu = layers.Activation('relu')(BatchNorm)\n",
        "  avg_pool = layers.AveragePooling2D(pool_size = (2, 2)) (relu)\n",
        "  conv_2d_2_2 = layers.Conv2D(num_classes, kernel_size = (2,2))(avg_pool) \n",
        "  x_reduce_dim = layers.Flatten() (conv_2d_2_2)\n",
        "  x_dense_1 = layers.Dense(dense_size, activation='relu', name='dense_1')(x_reduce_dim)\n",
        "  outputs = layers.Dense(10, name='predictions', activation='softmax')(x_dense_1)\n",
        "  return outputs;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYB0uF5ITITp",
        "colab_type": "code",
        "outputId": "43db6a42-f8af-4f69-ec6b-8808279d6270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "inputs = keras.Input(shape = (32, 32, 3), name='cifar')\n",
        "model = dense_block(compression = 0.5, growth_rate = 4, filter_size = 12, inputs = inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-63dae6d74ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cifar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrowth_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: dense_block() got an unexpected keyword argument 'compression'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bel-nmy0ieFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def densenet_model(input_shape, compression, growth_rate, bottleneck = False, bottleneck_channels = 16):\n",
        "  inputs = keras.Input(shape = input_shape, name='cifar')\n",
        "  conv_preprocess = layers.Conv2D(filters=36, kernel_size=(3, 3), strides=(1, 1), padding = \"same\", use_bias=True, kernel_initializer = initializers.he_normal(seed=0), bias_initializer = initializers.glorot_uniform(seed=0), name = \"conv_preprocess\" ) (inputs)\n",
        "  \n",
        "  d1 = dense_block(growth_rate = growth_rate, filter_size = 16, inputs = conv_preprocess, dense_block_num = 1, kernel_size = (3,3), bottleneck = bottleneck, bottleneck_channels = bottleneck_channels) \n",
        "  t1_filter_size = d1.shape[3]\n",
        "  t1 = transition_block(inputs = d1, filter_size = t1_filter_size, transition_layer_num = 1, compression = compression)\n",
        "  \n",
        "  d2 = dense_block(growth_rate = growth_rate, filter_size = 16, inputs = t1, dense_block_num = 2, kernel_size = (3,3), bottleneck = bottleneck, bottleneck_channels = bottleneck_channels) \n",
        "  t2_filter_size = d2.shape[3]\n",
        "  t2 = transition_block(inputs = d2, filter_size = t2_filter_size, transition_layer_num = 2, compression = compression)\n",
        "\n",
        "  d3 = dense_block(growth_rate = growth_rate, filter_size = 16, inputs = t2, dense_block_num = 3, kernel_size = (3,3), bottleneck = bottleneck, bottleneck_channels = bottleneck_channels)\n",
        "  #t3_filter_size = d3.shape[3]\n",
        "  #t3 = transition_block(inputs = d3, filter_size = t3_filter_size, transition_layer_num = 3, compression = compression)\n",
        "  \n",
        "  output_tensor = output_block(inputs = d3, dense_size = 32, pool_size = (2, 2))\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=output_tensor)\n",
        "  model.summary()\n",
        "  return model;\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e9Ai7d1im5Y",
        "colab_type": "code",
        "outputId": "ea37c71c-4248-4640-8376-3aa11a873770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config1 = densenet_model(input_shape = (32, 32, 3), compression = 0.5, growth_rate = 5, bottleneck = False, bottleneck_channels = 16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a (BatchNormalization)      (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 36)   0           bn_1a[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   5200        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a (TensorFl [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b (BatchNormalization)      (None, 32, 32, 52)   208         tf_op_layer_concat_1a[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 52)   0           bn_1b[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   7504        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b (TensorFl [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c (BatchNormalization)      (None, 32, 32, 68)   272         tf_op_layer_concat_1b[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 68)   0           bn_1c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   9808        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c (TensorFl [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d (BatchNormalization)      (None, 32, 32, 84)   336         tf_op_layer_concat_1c[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 84)   0           bn_1d[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   12112       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d (TensorFl [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e (BatchNormalization)      (None, 32, 32, 100)  400         tf_op_layer_concat_1d[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 100)  0           bn_1e[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   14416       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e (TensorFl [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 116)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 58)   6786        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 58)   0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a (BatchNormalization)      (None, 16, 16, 58)   232         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 58)   0           bn_2a[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   8368        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a (TensorFl [(None, 16, 16, 74)] 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b (BatchNormalization)      (None, 16, 16, 74)   296         tf_op_layer_concat_2a[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 74)   0           bn_2b[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   10672       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b (TensorFl [(None, 16, 16, 90)] 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c (BatchNormalization)      (None, 16, 16, 90)   360         tf_op_layer_concat_2b[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 90)   0           bn_2c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   12976       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c (TensorFl [(None, 16, 16, 106) 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d (BatchNormalization)      (None, 16, 16, 106)  424         tf_op_layer_concat_2c[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 106)  0           bn_2d[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   15280       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d (TensorFl [(None, 16, 16, 122) 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e (BatchNormalization)      (None, 16, 16, 122)  488         tf_op_layer_concat_2d[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 122)  0           bn_2e[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   17584       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e (TensorFl [(None, 16, 16, 138) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 138)  552         tf_op_layer_concat_2e[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 138)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 69)   9591        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 69)     0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a (BatchNormalization)      (None, 8, 8, 69)     276         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 69)     0           bn_3a[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     9952        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a (TensorFl [(None, 8, 8, 85)]   0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b (BatchNormalization)      (None, 8, 8, 85)     340         tf_op_layer_concat_3a[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 85)     0           bn_3b[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     12256       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b (TensorFl [(None, 8, 8, 101)]  0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c (BatchNormalization)      (None, 8, 8, 101)    404         tf_op_layer_concat_3b[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 101)    0           bn_3c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     14560       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c (TensorFl [(None, 8, 8, 117)]  0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d (BatchNormalization)      (None, 8, 8, 117)    468         tf_op_layer_concat_3c[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 117)    0           bn_3d[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     16864       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d (TensorFl [(None, 8, 8, 133)]  0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e (BatchNormalization)      (None, 8, 8, 133)    532         tf_op_layer_concat_3d[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 133)    0           bn_3e[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     19168       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e (TensorFl [(None, 8, 8, 149)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 8, 8, 149)    596         tf_op_layer_concat_3e[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 149)    0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 149)    0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 3, 3, 10)     5970        average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 90)           0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 220,109\n",
            "Trainable params: 216,713\n",
            "Non-trainable params: 3,396\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fymd4Y4rnMCG",
        "colab_type": "code",
        "outputId": "e1bb3124-817c-490b-afe3-785f604f127b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p6nb9_xnb8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 250\n",
        "l = 5\n",
        "num_filter = 36\n",
        "compression = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQdhPDO_nOaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia8kzfdxnQh6",
        "colab_type": "code",
        "outputId": "6ce5b593-6e18-4843-b4d7-8bec75d09bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"training samples shape: \", x_train.shape)\n",
        "print(\"training labels shape: \", y_train.shape)\n",
        "print(\"test samples shape: \", x_test.shape)\n",
        "print(\"test labels shape: \", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training samples shape:  (50000, 32, 32, 3)\n",
            "training labels shape:  (50000, 10)\n",
            "test samples shape:  (10000, 32, 32, 3)\n",
            "test labels shape:  (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8yVFv2_mQri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rotation_range = 15, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, zoom_range = 0.2, shear_range = 15)\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phej7ebumQ8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_config1.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhFb6ts9mRGo",
        "colab_type": "code",
        "outputId": "e8480124-ec43-4a18-bb93-d76fe9d083ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model_config1.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 10 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1172/1171 [==============================] - 170s 145ms/step - loss: 1.2344 - accuracy: 0.5564 - val_loss: 0.9338 - val_accuracy: 0.6767\n",
            "Epoch 2/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.8160 - accuracy: 0.7121 - val_loss: 0.7852 - val_accuracy: 0.7360\n",
            "Epoch 3/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.6694 - accuracy: 0.7648 - val_loss: 0.6522 - val_accuracy: 0.7865\n",
            "Epoch 4/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.5856 - accuracy: 0.7956 - val_loss: 0.6601 - val_accuracy: 0.7833\n",
            "Epoch 5/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.5290 - accuracy: 0.8166 - val_loss: 0.6436 - val_accuracy: 0.7842\n",
            "Epoch 6/10\n",
            "1172/1171 [==============================] - 168s 144ms/step - loss: 0.4875 - accuracy: 0.8308 - val_loss: 0.5667 - val_accuracy: 0.8128\n",
            "Epoch 7/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.4553 - accuracy: 0.8412 - val_loss: 0.5998 - val_accuracy: 0.8142\n",
            "Epoch 8/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.4288 - accuracy: 0.8513 - val_loss: 0.6410 - val_accuracy: 0.8043\n",
            "Epoch 9/10\n",
            "1172/1171 [==============================] - 169s 144ms/step - loss: 0.4018 - accuracy: 0.8609 - val_loss: 0.5305 - val_accuracy: 0.8267\n",
            "Epoch 10/10\n",
            "1172/1171 [==============================] - 168s 144ms/step - loss: 0.3849 - accuracy: 0.8654 - val_loss: 0.4654 - val_accuracy: 0.8522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3bbcf55898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdY1qaaAxy-R",
        "colab_type": "text"
      },
      "source": [
        "Config 1 produced 86.5% train accuracy and 85.2% test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ9Yw0OkxyE8",
        "colab_type": "code",
        "outputId": "4baea6ba-2e5f-4967-86c0-e1b0228e6370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config2 = densenet_model(input_shape = (32, 32, 3), compression = 0.5, growth_rate = 5, bottleneck = True, bottleneck_channels = 32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a (BatchNormalization)      (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 36)   0           bn_1a[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_1 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b (BatchNormalization)      (None, 32, 32, 52)   208         tf_op_layer_concat_1a_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 52)   0           bn_1b[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_1 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c (BatchNormalization)      (None, 32, 32, 68)   272         tf_op_layer_concat_1b_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 68)   0           bn_1c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_1 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d (BatchNormalization)      (None, 32, 32, 84)   336         tf_op_layer_concat_1c_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 84)   0           bn_1d[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1d (Conv2D)          (None, 32, 32, 32)   2720        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   4624        bottleneck_1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d_1 (Tensor [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e (BatchNormalization)      (None, 32, 32, 100)  400         tf_op_layer_concat_1d_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 32, 32, 100)  0           bn_1e[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1e (Conv2D)          (None, 32, 32, 32)   3232        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   4624        bottleneck_1e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e_1 (Tensor [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 32, 32, 116)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 58)   6786        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 16, 16, 58)   0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a (BatchNormalization)      (None, 16, 16, 58)   232         average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 58)   0           bn_2a[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   1888        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_1 (Tensor [(None, 16, 16, 74)] 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b (BatchNormalization)      (None, 16, 16, 74)   296         tf_op_layer_concat_2a_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 74)   0           bn_2b[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   2400        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_1 (Tensor [(None, 16, 16, 90)] 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c (BatchNormalization)      (None, 16, 16, 90)   360         tf_op_layer_concat_2b_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 90)   0           bn_2c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   2912        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_1 (Tensor [(None, 16, 16, 106) 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d (BatchNormalization)      (None, 16, 16, 106)  424         tf_op_layer_concat_2c_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 106)  0           bn_2d[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2d (Conv2D)          (None, 16, 16, 32)   3424        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   4624        bottleneck_2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d_1 (Tensor [(None, 16, 16, 122) 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e (BatchNormalization)      (None, 16, 16, 122)  488         tf_op_layer_concat_2d_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 122)  0           bn_2e[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2e (Conv2D)          (None, 16, 16, 32)   3936        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   4624        bottleneck_2e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e_1 (Tensor [(None, 16, 16, 138) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 138)  552         tf_op_layer_concat_2e_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 138)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 69)   9591        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 8, 8, 69)     0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a (BatchNormalization)      (None, 8, 8, 69)     276         average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 69)     0           bn_3a[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     2240        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_1 (Tensor [(None, 8, 8, 85)]   0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b (BatchNormalization)      (None, 8, 8, 85)     340         tf_op_layer_concat_3a_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 85)     0           bn_3b[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     2752        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_1 (Tensor [(None, 8, 8, 101)]  0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c (BatchNormalization)      (None, 8, 8, 101)    404         tf_op_layer_concat_3b_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 101)    0           bn_3c[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     3264        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_1 (Tensor [(None, 8, 8, 117)]  0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d (BatchNormalization)      (None, 8, 8, 117)    468         tf_op_layer_concat_3c_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 117)    0           bn_3d[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3d (Conv2D)          (None, 8, 8, 32)     3776        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     4624        bottleneck_3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d_1 (Tensor [(None, 8, 8, 133)]  0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e (BatchNormalization)      (None, 8, 8, 133)    532         tf_op_layer_concat_3d_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 133)    0           bn_3e[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3e (Conv2D)          (None, 8, 8, 32)     4288        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     4624        bottleneck_3e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e_1 (Tensor [(None, 8, 8, 149)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 8, 8, 149)    596         tf_op_layer_concat_3e_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 149)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 4, 4, 149)    0           activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 3, 3, 10)     5970        average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 90)           0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 144,669\n",
            "Trainable params: 141,273\n",
            "Non-trainable params: 3,396\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfvge9ywxyIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_config2.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.0015), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fLGa-GyxyVO",
        "colab_type": "code",
        "outputId": "17cf419c-687c-4074-e3f4-9e1579c83204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model_config2.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 84s 71ms/step - loss: 1.2223 - accuracy: 0.5601 - val_loss: 1.0417 - val_accuracy: 0.6497\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 83s 71ms/step - loss: 0.8282 - accuracy: 0.7081 - val_loss: 0.9370 - val_accuracy: 0.6925\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.6884 - accuracy: 0.7594 - val_loss: 0.6775 - val_accuracy: 0.7661\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 83s 71ms/step - loss: 0.6089 - accuracy: 0.7887 - val_loss: 0.6380 - val_accuracy: 0.7904\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 83s 71ms/step - loss: 0.5545 - accuracy: 0.8067 - val_loss: 0.7158 - val_accuracy: 0.7721\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.5146 - accuracy: 0.8213 - val_loss: 0.7270 - val_accuracy: 0.7670\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 85s 72ms/step - loss: 0.4824 - accuracy: 0.8326 - val_loss: 0.6759 - val_accuracy: 0.7913\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.4556 - accuracy: 0.8418 - val_loss: 0.5205 - val_accuracy: 0.8302\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.4363 - accuracy: 0.8483 - val_loss: 0.6393 - val_accuracy: 0.8049\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.4164 - accuracy: 0.8556 - val_loss: 0.5599 - val_accuracy: 0.8186\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.3994 - accuracy: 0.8616 - val_loss: 0.5527 - val_accuracy: 0.8279\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.3848 - accuracy: 0.8674 - val_loss: 0.5209 - val_accuracy: 0.8317\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 84s 71ms/step - loss: 0.3733 - accuracy: 0.8696 - val_loss: 0.5538 - val_accuracy: 0.8261\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.3628 - accuracy: 0.8750 - val_loss: 0.4219 - val_accuracy: 0.8595\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 84s 72ms/step - loss: 0.3520 - accuracy: 0.8780 - val_loss: 0.4634 - val_accuracy: 0.8465\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 84s 71ms/step - loss: 0.3441 - accuracy: 0.8801 - val_loss: 0.4493 - val_accuracy: 0.8639\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 83s 71ms/step - loss: 0.3326 - accuracy: 0.8843 - val_loss: 0.5094 - val_accuracy: 0.8428\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 84s 71ms/step - loss: 0.3260 - accuracy: 0.8875 - val_loss: 0.4640 - val_accuracy: 0.8502\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 83s 71ms/step - loss: 0.3199 - accuracy: 0.8884 - val_loss: 0.4364 - val_accuracy: 0.8631\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 83s 71ms/step - loss: 0.3131 - accuracy: 0.8917 - val_loss: 0.4002 - val_accuracy: 0.8726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcac59474e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYeXXleB8xa8",
        "colab_type": "text"
      },
      "source": [
        "Config 2 : With bottleneck of 32 channels at bottleneck and growth rate of 5 clocked highest of 87% validation error for 20 epochs and 83% validation error for 10 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5S24GKeDTEc",
        "colab_type": "text"
      },
      "source": [
        "# **Playing with growth rate**\n",
        "**Summary**: Earlier we saw densenets with growth rate of fixed 5. Now we will change the growth rate to 3, 7, 9 and 12 and run for 10 - 20 epochs(epochs are kept a little less as gpu availability need to be considered).\n",
        "As expected , increase in growth rate leads to increase in accuracy as more and more featuremaps are considered inside the dense block\n",
        "\n",
        "**growth rate = 3 | config 4:** \n",
        "Params used = 80K, time taken to train per epoch = 80 seconds, \n",
        "Training accuracy = 86%, Test accuracy = 84%\n",
        "\n",
        "**growth rate = 7 | config 3:**\n",
        "Params used = 230K, time taken to train per epoch = 100 seconds, \n",
        "Training accuracy = 90%, Test accuracy = 88%\n",
        "\n",
        "**growth rate = 9 | config 5:**\n",
        "Params used = 330K, time taken to train per epoch = 120 seconds, \n",
        "Training accuracy = 93%, Test accuracy = 90%\n",
        "\n",
        "\n",
        "**growth rate = 12 | config 6:** \n",
        "Params used = 450K, time taken to train per epoch = 150 seconds, \n",
        "Training accuracy = 94%, Test accuracy = 90%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV4f6eNZxyZT",
        "colab_type": "code",
        "outputId": "b7b816cf-f6d1-4477-d9f0-c070e9d12af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model_config3 = densenet_model(input_shape = (32, 32, 3), compression = 0.5, growth_rate = 7, bottleneck = True, bottleneck_channels = 32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_1 (BatchNormalization)    (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 32, 32, 36)   0           bn_1a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 32, 32, 32)   0           bn_1a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_4 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_1 (BatchNormalization)    (None, 32, 32, 52)   208         tf_op_layer_concat_1a_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 32, 32, 52)   0           bn_1b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 32, 32, 32)   0           bn_1b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_4 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_1 (BatchNormalization)    (None, 32, 32, 68)   272         tf_op_layer_concat_1b_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 32, 32, 68)   0           bn_1c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 32, 32, 32)   0           bn_1c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_4 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_1 (BatchNormalization)    (None, 32, 32, 84)   336         tf_op_layer_concat_1c_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 32, 32, 84)   0           bn_1d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1d (Conv2D)          (None, 32, 32, 32)   2720        activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 32, 32, 32)   0           bn_1d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   4624        activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d_4 (Tensor [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_1 (BatchNormalization)    (None, 32, 32, 100)  400         tf_op_layer_concat_1d_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 32, 32, 100)  0           bn_1e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1e (Conv2D)          (None, 32, 32, 32)   3232        activation_134[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 32, 32, 32)   0           bn_1e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   4624        activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e_4 (Tensor [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1f_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 32, 32, 116)  0           bn_1f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1f (Conv2D)          (None, 32, 32, 32)   3744        activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1f_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 32, 32, 32)   0           bn_1f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1f (Conv2D)                (None, 32, 32, 16)   4624        activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1f_2 (Tensor [(None, 32, 32, 132) 0           conv_1f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1e_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1g_1 (BatchNormalization)    (None, 32, 32, 132)  528         tf_op_layer_concat_1f_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 32, 32, 132)  0           bn_1g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1g (Conv2D)          (None, 32, 32, 32)   4256        activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1g_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 32, 32, 32)   0           bn_1g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1g (Conv2D)                (None, 32, 32, 16)   4624        activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1g_2 (Tensor [(None, 32, 32, 148) 0           conv_1g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1f_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 148)  592         tf_op_layer_concat_1g_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 32, 32, 148)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 74)   11026       activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 16, 16, 74)   0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_1 (BatchNormalization)    (None, 16, 16, 74)   296         average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 16, 16, 74)   0           bn_2a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   2400        activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 16, 16, 32)   0           bn_2a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_4 (Tensor [(None, 16, 16, 90)] 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_1 (BatchNormalization)    (None, 16, 16, 90)   360         tf_op_layer_concat_2a_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 16, 16, 90)   0           bn_2b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   2912        activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 16, 16, 32)   0           bn_2b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        activation_144[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_4 (Tensor [(None, 16, 16, 106) 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_1 (BatchNormalization)    (None, 16, 16, 106)  424         tf_op_layer_concat_2b_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 16, 16, 106)  0           bn_2c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   3424        activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 16, 16, 32)   0           bn_2c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_4 (Tensor [(None, 16, 16, 122) 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_1 (BatchNormalization)    (None, 16, 16, 122)  488         tf_op_layer_concat_2c_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 16, 16, 122)  0           bn_2d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2d (Conv2D)          (None, 16, 16, 32)   3936        activation_147[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 16, 16, 32)   0           bn_2d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   4624        activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d_4 (Tensor [(None, 16, 16, 138) 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_1 (BatchNormalization)    (None, 16, 16, 138)  552         tf_op_layer_concat_2d_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 16, 16, 138)  0           bn_2e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2e (Conv2D)          (None, 16, 16, 32)   4448        activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 16, 16, 32)   0           bn_2e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   4624        activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e_4 (Tensor [(None, 16, 16, 154) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2f_1 (BatchNormalization)    (None, 16, 16, 154)  616         tf_op_layer_concat_2e_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 16, 16, 154)  0           bn_2f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2f (Conv2D)          (None, 16, 16, 32)   4960        activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2f_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 16, 16, 32)   0           bn_2f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2f (Conv2D)                (None, 16, 16, 16)   4624        activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2f_2 (Tensor [(None, 16, 16, 170) 0           conv_2f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2e_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2g_1 (BatchNormalization)    (None, 16, 16, 170)  680         tf_op_layer_concat_2f_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 16, 16, 170)  0           bn_2g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2g (Conv2D)          (None, 16, 16, 32)   5472        activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2g_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 16, 16, 32)   0           bn_2g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2g (Conv2D)                (None, 16, 16, 16)   4624        activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2g_2 (Tensor [(None, 16, 16, 186) 0           conv_2g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2f_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 186)  744         tf_op_layer_concat_2g_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 16, 16, 186)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 93)   17391       activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 8, 8, 93)     0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_1 (BatchNormalization)    (None, 8, 8, 93)     372         average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 8, 8, 93)     0           bn_3a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     3008        activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 8, 8, 32)     0           bn_3a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_4 (Tensor [(None, 8, 8, 109)]  0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_1 (BatchNormalization)    (None, 8, 8, 109)    436         tf_op_layer_concat_3a_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 8, 8, 109)    0           bn_3b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     3520        activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 8, 8, 32)     0           bn_3b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_4 (Tensor [(None, 8, 8, 125)]  0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_1 (BatchNormalization)    (None, 8, 8, 125)    500         tf_op_layer_concat_3b_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 8, 8, 125)    0           bn_3c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     4032        activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 8, 8, 32)     0           bn_3c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_4 (Tensor [(None, 8, 8, 141)]  0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_1 (BatchNormalization)    (None, 8, 8, 141)    564         tf_op_layer_concat_3c_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 8, 8, 141)    0           bn_3d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3d (Conv2D)          (None, 8, 8, 32)     4544        activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 8, 8, 32)     0           bn_3d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     4624        activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d_4 (Tensor [(None, 8, 8, 157)]  0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_1 (BatchNormalization)    (None, 8, 8, 157)    628         tf_op_layer_concat_3d_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 8, 8, 157)    0           bn_3e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3e (Conv2D)          (None, 8, 8, 32)     5056        activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 32)     0           bn_3e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     4624        activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e_4 (Tensor [(None, 8, 8, 173)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3f_1 (BatchNormalization)    (None, 8, 8, 173)    692         tf_op_layer_concat_3e_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 8, 8, 173)    0           bn_3f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3f (Conv2D)          (None, 8, 8, 32)     5568        activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3f_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 8, 8, 32)     0           bn_3f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3f (Conv2D)                (None, 8, 8, 16)     4624        activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3f_2 (Tensor [(None, 8, 8, 189)]  0           conv_3f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3e_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3g_1 (BatchNormalization)    (None, 8, 8, 189)    756         tf_op_layer_concat_3f_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 8, 8, 189)    0           bn_3g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3g (Conv2D)          (None, 8, 8, 32)     6080        activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3g_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 8, 8, 32)     0           bn_3g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3g (Conv2D)                (None, 8, 8, 16)     4624        activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3g_2 (Tensor [(None, 8, 8, 205)]  0           conv_3g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3f_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 8, 8, 205)    820         tf_op_layer_concat_3g_2[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 8, 8, 205)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 4, 4, 205)    0           activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 10)     8210        average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 90)           0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 230,941\n",
            "Trainable params: 223,661\n",
            "Non-trainable params: 7,280\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctJnjG_bxyda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_config3.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.0015), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0zEZdTyxykx",
        "colab_type": "code",
        "outputId": "b349deb3-1622-4e10-9948-84c3ade820a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model_config3.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 104s 89ms/step - loss: 1.2087 - accuracy: 0.5643 - val_loss: 1.2387 - val_accuracy: 0.5924\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 101s 86ms/step - loss: 0.7793 - accuracy: 0.7254 - val_loss: 0.8017 - val_accuracy: 0.7307\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 99s 84ms/step - loss: 0.6371 - accuracy: 0.7775 - val_loss: 0.6748 - val_accuracy: 0.7796\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.5478 - accuracy: 0.8092 - val_loss: 0.7885 - val_accuracy: 0.7537\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.4912 - accuracy: 0.8292 - val_loss: 0.5316 - val_accuracy: 0.8252\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.4466 - accuracy: 0.8449 - val_loss: 0.4959 - val_accuracy: 0.8380\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 98s 84ms/step - loss: 0.4157 - accuracy: 0.8566 - val_loss: 1.0015 - val_accuracy: 0.7336\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 99s 84ms/step - loss: 0.3856 - accuracy: 0.8657 - val_loss: 0.5963 - val_accuracy: 0.8117\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.3652 - accuracy: 0.8727 - val_loss: 0.5017 - val_accuracy: 0.8391\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.3447 - accuracy: 0.8805 - val_loss: 0.5592 - val_accuracy: 0.8303\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.3296 - accuracy: 0.8855 - val_loss: 0.4740 - val_accuracy: 0.8498\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 97s 82ms/step - loss: 0.3158 - accuracy: 0.8903 - val_loss: 0.5210 - val_accuracy: 0.8483\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 97s 82ms/step - loss: 0.3020 - accuracy: 0.8953 - val_loss: 0.3936 - val_accuracy: 0.8774\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 98s 84ms/step - loss: 0.2912 - accuracy: 0.8985 - val_loss: 0.4485 - val_accuracy: 0.8626\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2823 - accuracy: 0.9017 - val_loss: 0.5252 - val_accuracy: 0.8450\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 98s 84ms/step - loss: 0.2682 - accuracy: 0.9070 - val_loss: 0.5475 - val_accuracy: 0.8409\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 98s 83ms/step - loss: 0.2624 - accuracy: 0.9080 - val_loss: 0.4798 - val_accuracy: 0.8589\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2546 - accuracy: 0.9120 - val_loss: 0.3908 - val_accuracy: 0.8839\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2487 - accuracy: 0.9130 - val_loss: 0.4641 - val_accuracy: 0.8627\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 97s 83ms/step - loss: 0.2403 - accuracy: 0.9166 - val_loss: 0.4232 - val_accuracy: 0.8727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fca6e5af4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SU3Gob_xyjV",
        "colab_type": "code",
        "outputId": "cdd16928-2fe6-4833-cf53-ccc20c45ad23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config4 = densenet_model(input_shape = (32, 32, 3), compression = 0.5, growth_rate = 3, bottleneck = True, bottleneck_channels = 32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_1 (BatchNormalization)    (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 36)   0           bn_1a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 32, 32, 32)   0           bn_1a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_5 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_1 (BatchNormalization)    (None, 32, 32, 52)   208         tf_op_layer_concat_1a_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 32, 32, 52)   0           bn_1b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 32, 32, 32)   0           bn_1b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_5 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_1 (BatchNormalization)    (None, 32, 32, 68)   272         tf_op_layer_concat_1b_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 32, 32, 68)   0           bn_1c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 32, 32, 32)   0           bn_1c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_5 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 84)   336         tf_op_layer_concat_1c_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 32, 32, 84)   0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 42)   3570        activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 16, 16, 42)   0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_1 (BatchNormalization)    (None, 16, 16, 42)   168         average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 42)   0           bn_2a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   1376        activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 32)   0           bn_2a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_5 (Tensor [(None, 16, 16, 58)] 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_1 (BatchNormalization)    (None, 16, 16, 58)   232         tf_op_layer_concat_2a_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 58)   0           bn_2b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   1888        activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 32)   0           bn_2b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_5 (Tensor [(None, 16, 16, 74)] 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_1 (BatchNormalization)    (None, 16, 16, 74)   296         tf_op_layer_concat_2b_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 16, 16, 74)   0           bn_2c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   2400        activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 16, 16, 32)   0           bn_2c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_5 (Tensor [(None, 16, 16, 90)] 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 90)   360         tf_op_layer_concat_2c_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 16, 16, 90)   0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 45)   4095        activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_16 (AveragePo (None, 8, 8, 45)     0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_1 (BatchNormalization)    (None, 8, 8, 45)     180         average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 8, 8, 45)     0           bn_3a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     1472        activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 8, 8, 32)     0           bn_3a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_5 (Tensor [(None, 8, 8, 61)]   0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_1 (BatchNormalization)    (None, 8, 8, 61)     244         tf_op_layer_concat_3a_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 61)     0           bn_3b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     1984        activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 32)     0           bn_3b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_5 (Tensor [(None, 8, 8, 77)]   0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_1 (BatchNormalization)    (None, 8, 8, 77)     308         tf_op_layer_concat_3b_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 77)     0           bn_3c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     2496        activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 8, 8, 32)     0           bn_3c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_5 (Tensor [(None, 8, 8, 93)]   0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 8, 8, 93)     372         tf_op_layer_concat_3c_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 8, 8, 93)     0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_17 (AveragePo (None, 4, 4, 93)     0           activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 3, 3, 10)     3730        average_pooling2d_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 90)           0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 78,237\n",
            "Trainable params: 76,101\n",
            "Non-trainable params: 2,136\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYK03kHOG84L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_config4.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.0015), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzV7OJ2wG-pX",
        "colab_type": "code",
        "outputId": "dfe66690-0f27-4121-d66e-a213c102b4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model_config4.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 77s 65ms/step - loss: 1.2916 - accuracy: 0.5351 - val_loss: 1.0468 - val_accuracy: 0.6238\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 76s 65ms/step - loss: 0.9062 - accuracy: 0.6777 - val_loss: 1.0659 - val_accuracy: 0.6499\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.7792 - accuracy: 0.7260 - val_loss: 0.7902 - val_accuracy: 0.7310\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.7016 - accuracy: 0.7539 - val_loss: 0.8022 - val_accuracy: 0.7358\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.6510 - accuracy: 0.7728 - val_loss: 0.7260 - val_accuracy: 0.7649\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.6089 - accuracy: 0.7880 - val_loss: 0.7385 - val_accuracy: 0.7535\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.5750 - accuracy: 0.7997 - val_loss: 0.6859 - val_accuracy: 0.7779\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 76s 65ms/step - loss: 0.5510 - accuracy: 0.8088 - val_loss: 0.5544 - val_accuracy: 0.8177\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.5241 - accuracy: 0.8177 - val_loss: 0.7555 - val_accuracy: 0.7652\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.5078 - accuracy: 0.8236 - val_loss: 0.5918 - val_accuracy: 0.8056\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4935 - accuracy: 0.8288 - val_loss: 0.7777 - val_accuracy: 0.7575\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 78s 66ms/step - loss: 0.4763 - accuracy: 0.8345 - val_loss: 0.5245 - val_accuracy: 0.8288\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 78s 66ms/step - loss: 0.4642 - accuracy: 0.8387 - val_loss: 0.6029 - val_accuracy: 0.8090\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4538 - accuracy: 0.8426 - val_loss: 0.5790 - val_accuracy: 0.8126\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 78s 66ms/step - loss: 0.4422 - accuracy: 0.8463 - val_loss: 0.5165 - val_accuracy: 0.8317\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4313 - accuracy: 0.8503 - val_loss: 0.5412 - val_accuracy: 0.8273\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4235 - accuracy: 0.8534 - val_loss: 0.5221 - val_accuracy: 0.8285\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4162 - accuracy: 0.8547 - val_loss: 0.5529 - val_accuracy: 0.8192\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4088 - accuracy: 0.8581 - val_loss: 0.6649 - val_accuracy: 0.8077\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 77s 66ms/step - loss: 0.4023 - accuracy: 0.8602 - val_loss: 0.4814 - val_accuracy: 0.8411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fca6c4feb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p4YWIYfNHFs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2m3wOPHDIp",
        "colab_type": "code",
        "outputId": "00e6f1ec-8e8b-444a-ff2f-1d04efd9b302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config5 = densenet_model(input_shape = (32, 32, 3), compression = 0.5, growth_rate = 9, bottleneck = True, bottleneck_channels = 32)\n",
        "model_config5.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.002), metrics=['accuracy'])\n",
        "model_config5.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_1 (BatchNormalization)    (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 32, 32, 36)   0           bn_1a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 32, 32, 32)   0           bn_1a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_6 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_1 (BatchNormalization)    (None, 32, 32, 52)   208         tf_op_layer_concat_1a_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 32, 32, 52)   0           bn_1b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 32, 32, 32)   0           bn_1b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_6 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_1 (BatchNormalization)    (None, 32, 32, 68)   272         tf_op_layer_concat_1b_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 32, 32, 68)   0           bn_1c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 32, 32, 32)   0           bn_1c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_6 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_1 (BatchNormalization)    (None, 32, 32, 84)   336         tf_op_layer_concat_1c_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 32, 32, 84)   0           bn_1d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1d (Conv2D)          (None, 32, 32, 32)   2720        activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 32, 32, 32)   0           bn_1d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   4624        activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d_5 (Tensor [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_1 (BatchNormalization)    (None, 32, 32, 100)  400         tf_op_layer_concat_1d_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 32, 32, 100)  0           bn_1e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1e (Conv2D)          (None, 32, 32, 32)   3232        activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 32, 32, 32)   0           bn_1e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   4624        activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e_5 (Tensor [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1f_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 32, 32, 116)  0           bn_1f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1f (Conv2D)          (None, 32, 32, 32)   3744        activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1f_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 32, 32, 32)   0           bn_1f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1f (Conv2D)                (None, 32, 32, 16)   4624        activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1f_3 (Tensor [(None, 32, 32, 132) 0           conv_1f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1e_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1g_1 (BatchNormalization)    (None, 32, 32, 132)  528         tf_op_layer_concat_1f_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 32, 32, 132)  0           bn_1g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1g (Conv2D)          (None, 32, 32, 32)   4256        activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1g_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 32, 32, 32)   0           bn_1g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1g (Conv2D)                (None, 32, 32, 16)   4624        activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1g_3 (Tensor [(None, 32, 32, 148) 0           conv_1g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1f_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1h_1 (BatchNormalization)    (None, 32, 32, 148)  592         tf_op_layer_concat_1g_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 32, 32, 148)  0           bn_1h_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1h (Conv2D)          (None, 32, 32, 32)   4768        activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1h_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1h[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 32, 32, 32)   0           bn_1h_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1h (Conv2D)                (None, 32, 32, 16)   4624        activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1h (TensorFl [(None, 32, 32, 164) 0           conv_1h[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1g_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1i_1 (BatchNormalization)    (None, 32, 32, 164)  656         tf_op_layer_concat_1h[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 32, 32, 164)  0           bn_1i_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1i (Conv2D)          (None, 32, 32, 32)   5280        activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1i_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1i[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 32, 32, 32)   0           bn_1i_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1i (Conv2D)                (None, 32, 32, 16)   4624        activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1i (TensorFl [(None, 32, 32, 180) 0           conv_1i[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1h[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 180)  720         tf_op_layer_concat_1i[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 32, 32, 180)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 90)   16290       activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_18 (AveragePo (None, 16, 16, 90)   0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_1 (BatchNormalization)    (None, 16, 16, 90)   360         average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 16, 16, 90)   0           bn_2a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   2912        activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 16, 16, 32)   0           bn_2a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_6 (Tensor [(None, 16, 16, 106) 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_1 (BatchNormalization)    (None, 16, 16, 106)  424         tf_op_layer_concat_2a_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 16, 16, 106)  0           bn_2b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   3424        activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 16, 16, 32)   0           bn_2b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_6 (Tensor [(None, 16, 16, 122) 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_1 (BatchNormalization)    (None, 16, 16, 122)  488         tf_op_layer_concat_2b_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 16, 16, 122)  0           bn_2c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   3936        activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 16, 16, 32)   0           bn_2c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_6 (Tensor [(None, 16, 16, 138) 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_1 (BatchNormalization)    (None, 16, 16, 138)  552         tf_op_layer_concat_2c_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 16, 16, 138)  0           bn_2d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2d (Conv2D)          (None, 16, 16, 32)   4448        activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 16, 16, 32)   0           bn_2d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   4624        activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d_5 (Tensor [(None, 16, 16, 154) 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_1 (BatchNormalization)    (None, 16, 16, 154)  616         tf_op_layer_concat_2d_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 16, 16, 154)  0           bn_2e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2e (Conv2D)          (None, 16, 16, 32)   4960        activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 16, 16, 32)   0           bn_2e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   4624        activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e_5 (Tensor [(None, 16, 16, 170) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2f_1 (BatchNormalization)    (None, 16, 16, 170)  680         tf_op_layer_concat_2e_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 16, 16, 170)  0           bn_2f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2f (Conv2D)          (None, 16, 16, 32)   5472        activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2f_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 16, 16, 32)   0           bn_2f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2f (Conv2D)                (None, 16, 16, 16)   4624        activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2f_3 (Tensor [(None, 16, 16, 186) 0           conv_2f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2e_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2g_1 (BatchNormalization)    (None, 16, 16, 186)  744         tf_op_layer_concat_2f_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 16, 16, 186)  0           bn_2g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2g (Conv2D)          (None, 16, 16, 32)   5984        activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2g_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 16, 16, 32)   0           bn_2g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2g (Conv2D)                (None, 16, 16, 16)   4624        activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2g_3 (Tensor [(None, 16, 16, 202) 0           conv_2g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2f_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2h_1 (BatchNormalization)    (None, 16, 16, 202)  808         tf_op_layer_concat_2g_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 16, 16, 202)  0           bn_2h_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2h (Conv2D)          (None, 16, 16, 32)   6496        activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2h_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2h[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 16, 16, 32)   0           bn_2h_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2h (Conv2D)                (None, 16, 16, 16)   4624        activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2h (TensorFl [(None, 16, 16, 218) 0           conv_2h[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2g_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2i_1 (BatchNormalization)    (None, 16, 16, 218)  872         tf_op_layer_concat_2h[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 16, 16, 218)  0           bn_2i_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2i (Conv2D)          (None, 16, 16, 32)   7008        activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2i_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2i[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 16, 16, 32)   0           bn_2i_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2i (Conv2D)                (None, 16, 16, 16)   4624        activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2i (TensorFl [(None, 16, 16, 234) 0           conv_2i[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2h[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 234)  936         tf_op_layer_concat_2i[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 16, 16, 234)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 117)  27495       activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 8, 8, 117)    0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_1 (BatchNormalization)    (None, 8, 8, 117)    468         average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 8, 8, 117)    0           bn_3a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     3776        activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 8, 8, 32)     0           bn_3a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_6 (Tensor [(None, 8, 8, 133)]  0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_1 (BatchNormalization)    (None, 8, 8, 133)    532         tf_op_layer_concat_3a_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 8, 8, 133)    0           bn_3b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     4288        activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 8, 8, 32)     0           bn_3b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_6 (Tensor [(None, 8, 8, 149)]  0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_1 (BatchNormalization)    (None, 8, 8, 149)    596         tf_op_layer_concat_3b_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 8, 8, 149)    0           bn_3c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     4800        activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 8, 8, 32)     0           bn_3c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_6 (Tensor [(None, 8, 8, 165)]  0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_1 (BatchNormalization)    (None, 8, 8, 165)    660         tf_op_layer_concat_3c_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 8, 8, 165)    0           bn_3d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3d (Conv2D)          (None, 8, 8, 32)     5312        activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 8, 8, 32)     0           bn_3d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     4624        activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d_5 (Tensor [(None, 8, 8, 181)]  0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_1 (BatchNormalization)    (None, 8, 8, 181)    724         tf_op_layer_concat_3d_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 8, 8, 181)    0           bn_3e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3e (Conv2D)          (None, 8, 8, 32)     5824        activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 8, 8, 32)     0           bn_3e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     4624        activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e_5 (Tensor [(None, 8, 8, 197)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3f_1 (BatchNormalization)    (None, 8, 8, 197)    788         tf_op_layer_concat_3e_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 8, 8, 197)    0           bn_3f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3f (Conv2D)          (None, 8, 8, 32)     6336        activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3f_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 8, 8, 32)     0           bn_3f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3f (Conv2D)                (None, 8, 8, 16)     4624        activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3f_3 (Tensor [(None, 8, 8, 213)]  0           conv_3f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3e_5[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3g_1 (BatchNormalization)    (None, 8, 8, 213)    852         tf_op_layer_concat_3f_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 8, 8, 213)    0           bn_3g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3g (Conv2D)          (None, 8, 8, 32)     6848        activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3g_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 8, 8, 32)     0           bn_3g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3g (Conv2D)                (None, 8, 8, 16)     4624        activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3g_3 (Tensor [(None, 8, 8, 229)]  0           conv_3g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3f_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3h_1 (BatchNormalization)    (None, 8, 8, 229)    916         tf_op_layer_concat_3g_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 8, 8, 229)    0           bn_3h_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3h (Conv2D)          (None, 8, 8, 32)     7360        activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3h_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3h[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 8, 8, 32)     0           bn_3h_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3h (Conv2D)                (None, 8, 8, 16)     4624        activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3h (TensorFl [(None, 8, 8, 245)]  0           conv_3h[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3g_3[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3i_1 (BatchNormalization)    (None, 8, 8, 245)    980         tf_op_layer_concat_3h[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 8, 8, 245)    0           bn_3i_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3i (Conv2D)          (None, 8, 8, 32)     7872        activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3i_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3i[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 8, 8, 32)     0           bn_3i_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3i (Conv2D)                (None, 8, 8, 16)     4624        activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3i (TensorFl [(None, 8, 8, 261)]  0           conv_3i[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3h[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 261)    1044        tf_op_layer_concat_3i[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 8, 8, 261)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_20 (AveragePo (None, 4, 4, 261)    0           activation_248[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 3, 3, 10)     10450       average_pooling2d_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 90)           0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 331,293\n",
            "Trainable params: 320,385\n",
            "Non-trainable params: 10,908\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 1.1748 - accuracy: 0.5779 - val_loss: 1.1119 - val_accuracy: 0.6316\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.7409 - accuracy: 0.7409 - val_loss: 0.7241 - val_accuracy: 0.7579\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.5848 - accuracy: 0.7965 - val_loss: 0.5518 - val_accuracy: 0.8146\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 112s 95ms/step - loss: 0.5010 - accuracy: 0.8269 - val_loss: 0.7738 - val_accuracy: 0.7651\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 0.4404 - accuracy: 0.8475 - val_loss: 0.5495 - val_accuracy: 0.8222\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 0.3969 - accuracy: 0.8631 - val_loss: 0.4712 - val_accuracy: 0.8479\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 112s 95ms/step - loss: 0.3651 - accuracy: 0.8744 - val_loss: 0.4903 - val_accuracy: 0.8414\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.3382 - accuracy: 0.8830 - val_loss: 0.4918 - val_accuracy: 0.8559\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.3147 - accuracy: 0.8909 - val_loss: 0.5557 - val_accuracy: 0.8422\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.2989 - accuracy: 0.8974 - val_loss: 0.4278 - val_accuracy: 0.8634\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 112s 95ms/step - loss: 0.2825 - accuracy: 0.9027 - val_loss: 0.4041 - val_accuracy: 0.8791\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 0.2680 - accuracy: 0.9068 - val_loss: 0.4196 - val_accuracy: 0.8724\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 0.2576 - accuracy: 0.9110 - val_loss: 0.4177 - val_accuracy: 0.8737\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 0.2433 - accuracy: 0.9159 - val_loss: 0.4546 - val_accuracy: 0.8698\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 112s 96ms/step - loss: 0.2329 - accuracy: 0.9194 - val_loss: 0.3810 - val_accuracy: 0.8871\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 112s 95ms/step - loss: 0.2273 - accuracy: 0.9206 - val_loss: 0.4478 - val_accuracy: 0.8689\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.2174 - accuracy: 0.9245 - val_loss: 0.5452 - val_accuracy: 0.8518\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.2093 - accuracy: 0.9273 - val_loss: 0.4578 - val_accuracy: 0.8737\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.2023 - accuracy: 0.9294 - val_loss: 0.3586 - val_accuracy: 0.8959\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 111s 95ms/step - loss: 0.1960 - accuracy: 0.9320 - val_loss: 0.3375 - val_accuracy: 0.9008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fca2a157128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L72RUBmvV9sT",
        "colab_type": "code",
        "outputId": "c24cfae3-1fb1-4574-868b-db8a15c247c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config6 = densenet_model(input_shape = (32, 32, 3), compression = 0.5, growth_rate = 12, bottleneck = True, bottleneck_channels = 32)\n",
        "model_config6.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.0025), metrics=['accuracy'])\n",
        "model_config6.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_1 (BatchNormalization)    (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 32, 32, 36)   0           bn_1a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 32, 32, 32)   0           bn_1a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_7 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_1 (BatchNormalization)    (None, 32, 32, 52)   208         tf_op_layer_concat_1a_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 32, 32, 52)   0           bn_1b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_251[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 32, 32, 32)   0           bn_1b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_7 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_1 (BatchNormalization)    (None, 32, 32, 68)   272         tf_op_layer_concat_1b_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 32, 32, 68)   0           bn_1c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 32, 32, 32)   0           bn_1c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_7 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_1 (BatchNormalization)    (None, 32, 32, 84)   336         tf_op_layer_concat_1c_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 32, 32, 84)   0           bn_1d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1d (Conv2D)          (None, 32, 32, 32)   2720        activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 32, 32, 32)   0           bn_1d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   4624        activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d_6 (Tensor [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_1 (BatchNormalization)    (None, 32, 32, 100)  400         tf_op_layer_concat_1d_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 32, 32, 100)  0           bn_1e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1e (Conv2D)          (None, 32, 32, 32)   3232        activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 32, 32, 32)   0           bn_1e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   4624        activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e_6 (Tensor [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1f_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 32, 32, 116)  0           bn_1f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1f (Conv2D)          (None, 32, 32, 32)   3744        activation_259[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1f_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 32, 32, 32)   0           bn_1f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1f (Conv2D)                (None, 32, 32, 16)   4624        activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1f_4 (Tensor [(None, 32, 32, 132) 0           conv_1f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1e_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1g_1 (BatchNormalization)    (None, 32, 32, 132)  528         tf_op_layer_concat_1f_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 32, 32, 132)  0           bn_1g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1g (Conv2D)          (None, 32, 32, 32)   4256        activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1g_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 32, 32, 32)   0           bn_1g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1g (Conv2D)                (None, 32, 32, 16)   4624        activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1g_4 (Tensor [(None, 32, 32, 148) 0           conv_1g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1f_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1h_1 (BatchNormalization)    (None, 32, 32, 148)  592         tf_op_layer_concat_1g_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 32, 32, 148)  0           bn_1h_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1h (Conv2D)          (None, 32, 32, 32)   4768        activation_263[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1h_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1h[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 32, 32, 32)   0           bn_1h_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1h (Conv2D)                (None, 32, 32, 16)   4624        activation_264[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1h_1 (Tensor [(None, 32, 32, 164) 0           conv_1h[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1g_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1i_1 (BatchNormalization)    (None, 32, 32, 164)  656         tf_op_layer_concat_1h_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 32, 32, 164)  0           bn_1i_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1i (Conv2D)          (None, 32, 32, 32)   5280        activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1i_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1i[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 32, 32, 32)   0           bn_1i_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1i (Conv2D)                (None, 32, 32, 16)   4624        activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1i_1 (Tensor [(None, 32, 32, 180) 0           conv_1i[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1h_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1j_1 (BatchNormalization)    (None, 32, 32, 180)  720         tf_op_layer_concat_1i_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 32, 32, 180)  0           bn_1j_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1j (Conv2D)          (None, 32, 32, 32)   5792        activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1j_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1j[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 32, 32, 32)   0           bn_1j_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1j (Conv2D)                (None, 32, 32, 16)   4624        activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1j (TensorFl [(None, 32, 32, 196) 0           conv_1j[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1i_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1k_1 (BatchNormalization)    (None, 32, 32, 196)  784         tf_op_layer_concat_1j[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 32, 32, 196)  0           bn_1k_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1k (Conv2D)          (None, 32, 32, 32)   6304        activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1k_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1k[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 32, 32, 32)   0           bn_1k_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1k (Conv2D)                (None, 32, 32, 16)   4624        activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1k (TensorFl [(None, 32, 32, 212) 0           conv_1k[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1j[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 212)  848         tf_op_layer_concat_1k[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 32, 32, 212)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 106)  22578       activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_21 (AveragePo (None, 16, 16, 106)  0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_1 (BatchNormalization)    (None, 16, 16, 106)  424         average_pooling2d_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 16, 16, 106)  0           bn_2a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   3424        activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 16, 16, 32)   0           bn_2a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        activation_273[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_7 (Tensor [(None, 16, 16, 122) 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_1 (BatchNormalization)    (None, 16, 16, 122)  488         tf_op_layer_concat_2a_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 16, 16, 122)  0           bn_2b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   3936        activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 16, 16, 32)   0           bn_2b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        activation_275[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_7 (Tensor [(None, 16, 16, 138) 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_1 (BatchNormalization)    (None, 16, 16, 138)  552         tf_op_layer_concat_2b_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 16, 16, 138)  0           bn_2c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   4448        activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 16, 16, 32)   0           bn_2c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_7 (Tensor [(None, 16, 16, 154) 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_1 (BatchNormalization)    (None, 16, 16, 154)  616         tf_op_layer_concat_2c_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 16, 16, 154)  0           bn_2d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2d (Conv2D)          (None, 16, 16, 32)   4960        activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 16, 16, 32)   0           bn_2d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   4624        activation_279[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d_6 (Tensor [(None, 16, 16, 170) 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_1 (BatchNormalization)    (None, 16, 16, 170)  680         tf_op_layer_concat_2d_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 16, 16, 170)  0           bn_2e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2e (Conv2D)          (None, 16, 16, 32)   5472        activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 16, 16, 32)   0           bn_2e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   4624        activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e_6 (Tensor [(None, 16, 16, 186) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2f_1 (BatchNormalization)    (None, 16, 16, 186)  744         tf_op_layer_concat_2e_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_282 (Activation)     (None, 16, 16, 186)  0           bn_2f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2f (Conv2D)          (None, 16, 16, 32)   5984        activation_282[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2f_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_283 (Activation)     (None, 16, 16, 32)   0           bn_2f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2f (Conv2D)                (None, 16, 16, 16)   4624        activation_283[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2f_4 (Tensor [(None, 16, 16, 202) 0           conv_2f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2e_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2g_1 (BatchNormalization)    (None, 16, 16, 202)  808         tf_op_layer_concat_2f_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_284 (Activation)     (None, 16, 16, 202)  0           bn_2g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2g (Conv2D)          (None, 16, 16, 32)   6496        activation_284[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2g_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_285 (Activation)     (None, 16, 16, 32)   0           bn_2g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2g (Conv2D)                (None, 16, 16, 16)   4624        activation_285[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2g_4 (Tensor [(None, 16, 16, 218) 0           conv_2g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2f_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2h_1 (BatchNormalization)    (None, 16, 16, 218)  872         tf_op_layer_concat_2g_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_286 (Activation)     (None, 16, 16, 218)  0           bn_2h_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2h (Conv2D)          (None, 16, 16, 32)   7008        activation_286[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2h_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2h[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_287 (Activation)     (None, 16, 16, 32)   0           bn_2h_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2h (Conv2D)                (None, 16, 16, 16)   4624        activation_287[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2h_1 (Tensor [(None, 16, 16, 234) 0           conv_2h[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2g_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2i_1 (BatchNormalization)    (None, 16, 16, 234)  936         tf_op_layer_concat_2h_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_288 (Activation)     (None, 16, 16, 234)  0           bn_2i_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2i (Conv2D)          (None, 16, 16, 32)   7520        activation_288[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2i_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2i[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_289 (Activation)     (None, 16, 16, 32)   0           bn_2i_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2i (Conv2D)                (None, 16, 16, 16)   4624        activation_289[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2i_1 (Tensor [(None, 16, 16, 250) 0           conv_2i[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2h_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2j_1 (BatchNormalization)    (None, 16, 16, 250)  1000        tf_op_layer_concat_2i_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_290 (Activation)     (None, 16, 16, 250)  0           bn_2j_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2j (Conv2D)          (None, 16, 16, 32)   8032        activation_290[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2j_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2j[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_291 (Activation)     (None, 16, 16, 32)   0           bn_2j_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2j (Conv2D)                (None, 16, 16, 16)   4624        activation_291[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2j (TensorFl [(None, 16, 16, 266) 0           conv_2j[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2i_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2k_1 (BatchNormalization)    (None, 16, 16, 266)  1064        tf_op_layer_concat_2j[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_292 (Activation)     (None, 16, 16, 266)  0           bn_2k_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2k (Conv2D)          (None, 16, 16, 32)   8544        activation_292[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2k_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2k[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_293 (Activation)     (None, 16, 16, 32)   0           bn_2k_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2k (Conv2D)                (None, 16, 16, 16)   4624        activation_293[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2k (TensorFl [(None, 16, 16, 282) 0           conv_2k[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2j[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 282)  1128        tf_op_layer_concat_2k[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_294 (Activation)     (None, 16, 16, 282)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 141)  39903       activation_294[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_22 (AveragePo (None, 8, 8, 141)    0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_1 (BatchNormalization)    (None, 8, 8, 141)    564         average_pooling2d_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_295 (Activation)     (None, 8, 8, 141)    0           bn_3a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     4544        activation_295[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_296 (Activation)     (None, 8, 8, 32)     0           bn_3a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        activation_296[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_7 (Tensor [(None, 8, 8, 157)]  0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_1 (BatchNormalization)    (None, 8, 8, 157)    628         tf_op_layer_concat_3a_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_297 (Activation)     (None, 8, 8, 157)    0           bn_3b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     5056        activation_297[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_298 (Activation)     (None, 8, 8, 32)     0           bn_3b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        activation_298[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_7 (Tensor [(None, 8, 8, 173)]  0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_1 (BatchNormalization)    (None, 8, 8, 173)    692         tf_op_layer_concat_3b_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_299 (Activation)     (None, 8, 8, 173)    0           bn_3c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     5568        activation_299[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_300 (Activation)     (None, 8, 8, 32)     0           bn_3c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        activation_300[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_7 (Tensor [(None, 8, 8, 189)]  0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_1 (BatchNormalization)    (None, 8, 8, 189)    756         tf_op_layer_concat_3c_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_301 (Activation)     (None, 8, 8, 189)    0           bn_3d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3d (Conv2D)          (None, 8, 8, 32)     6080        activation_301[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_302 (Activation)     (None, 8, 8, 32)     0           bn_3d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     4624        activation_302[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d_6 (Tensor [(None, 8, 8, 205)]  0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_1 (BatchNormalization)    (None, 8, 8, 205)    820         tf_op_layer_concat_3d_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_303 (Activation)     (None, 8, 8, 205)    0           bn_3e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3e (Conv2D)          (None, 8, 8, 32)     6592        activation_303[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_304 (Activation)     (None, 8, 8, 32)     0           bn_3e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     4624        activation_304[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e_6 (Tensor [(None, 8, 8, 221)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3f_1 (BatchNormalization)    (None, 8, 8, 221)    884         tf_op_layer_concat_3e_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_305 (Activation)     (None, 8, 8, 221)    0           bn_3f_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3f (Conv2D)          (None, 8, 8, 32)     7104        activation_305[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3f_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3f[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_306 (Activation)     (None, 8, 8, 32)     0           bn_3f_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3f (Conv2D)                (None, 8, 8, 16)     4624        activation_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3f_4 (Tensor [(None, 8, 8, 237)]  0           conv_3f[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3e_6[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3g_1 (BatchNormalization)    (None, 8, 8, 237)    948         tf_op_layer_concat_3f_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_307 (Activation)     (None, 8, 8, 237)    0           bn_3g_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3g (Conv2D)          (None, 8, 8, 32)     7616        activation_307[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3g_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3g[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_308 (Activation)     (None, 8, 8, 32)     0           bn_3g_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3g (Conv2D)                (None, 8, 8, 16)     4624        activation_308[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3g_4 (Tensor [(None, 8, 8, 253)]  0           conv_3g[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3f_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3h_1 (BatchNormalization)    (None, 8, 8, 253)    1012        tf_op_layer_concat_3g_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_309 (Activation)     (None, 8, 8, 253)    0           bn_3h_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3h (Conv2D)          (None, 8, 8, 32)     8128        activation_309[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3h_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3h[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_310 (Activation)     (None, 8, 8, 32)     0           bn_3h_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3h (Conv2D)                (None, 8, 8, 16)     4624        activation_310[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3h_1 (Tensor [(None, 8, 8, 269)]  0           conv_3h[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3g_4[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3i_1 (BatchNormalization)    (None, 8, 8, 269)    1076        tf_op_layer_concat_3h_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_311 (Activation)     (None, 8, 8, 269)    0           bn_3i_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3i (Conv2D)          (None, 8, 8, 32)     8640        activation_311[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3i_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3i[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_312 (Activation)     (None, 8, 8, 32)     0           bn_3i_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3i (Conv2D)                (None, 8, 8, 16)     4624        activation_312[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3i_1 (Tensor [(None, 8, 8, 285)]  0           conv_3i[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3h_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3j_1 (BatchNormalization)    (None, 8, 8, 285)    1140        tf_op_layer_concat_3i_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_313 (Activation)     (None, 8, 8, 285)    0           bn_3j_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3j (Conv2D)          (None, 8, 8, 32)     9152        activation_313[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3j_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3j[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_314 (Activation)     (None, 8, 8, 32)     0           bn_3j_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3j (Conv2D)                (None, 8, 8, 16)     4624        activation_314[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3j (TensorFl [(None, 8, 8, 301)]  0           conv_3j[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3i_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3k_1 (BatchNormalization)    (None, 8, 8, 301)    1204        tf_op_layer_concat_3j[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_315 (Activation)     (None, 8, 8, 301)    0           bn_3k_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3k (Conv2D)          (None, 8, 8, 32)     9664        activation_315[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3k_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3k[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_316 (Activation)     (None, 8, 8, 32)     0           bn_3k_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3k (Conv2D)                (None, 8, 8, 16)     4624        activation_316[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3k (TensorFl [(None, 8, 8, 317)]  0           conv_3k[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3j[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 8, 8, 317)    1268        tf_op_layer_concat_3k[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_317 (Activation)     (None, 8, 8, 317)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_23 (AveragePo (None, 4, 4, 317)    0           activation_317[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 3, 3, 10)     12690       average_pooling2d_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 90)           0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 447,645\n",
            "Trainable params: 432,405\n",
            "Non-trainable params: 15,240\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 1.1966 - accuracy: 0.5705 - val_loss: 1.3844 - val_accuracy: 0.5599\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 145s 124ms/step - loss: 0.7392 - accuracy: 0.7405 - val_loss: 1.1250 - val_accuracy: 0.6642\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 145s 123ms/step - loss: 0.5736 - accuracy: 0.8013 - val_loss: 0.5642 - val_accuracy: 0.8183\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 145s 124ms/step - loss: 0.4770 - accuracy: 0.8358 - val_loss: 0.7248 - val_accuracy: 0.7892\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 145s 124ms/step - loss: 0.4157 - accuracy: 0.8569 - val_loss: 0.4380 - val_accuracy: 0.8590\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 145s 123ms/step - loss: 0.3746 - accuracy: 0.8718 - val_loss: 0.5656 - val_accuracy: 0.8256\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 145s 123ms/step - loss: 0.3420 - accuracy: 0.8829 - val_loss: 0.6373 - val_accuracy: 0.8108\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 145s 124ms/step - loss: 0.3128 - accuracy: 0.8927 - val_loss: 0.6011 - val_accuracy: 0.8216\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 145s 124ms/step - loss: 0.2894 - accuracy: 0.9002 - val_loss: 0.3962 - val_accuracy: 0.8782\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 0.2716 - accuracy: 0.9059 - val_loss: 0.4791 - val_accuracy: 0.8539\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 0.2564 - accuracy: 0.9112 - val_loss: 0.3467 - val_accuracy: 0.8898\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 0.2447 - accuracy: 0.9156 - val_loss: 0.3373 - val_accuracy: 0.8934\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 147s 125ms/step - loss: 0.2322 - accuracy: 0.9197 - val_loss: 0.4065 - val_accuracy: 0.8775\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 0.2206 - accuracy: 0.9240 - val_loss: 0.3592 - val_accuracy: 0.8886\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 146s 124ms/step - loss: 0.2109 - accuracy: 0.9269 - val_loss: 0.3470 - val_accuracy: 0.8997\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 0.2025 - accuracy: 0.9298 - val_loss: 0.4239 - val_accuracy: 0.8766\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 146s 125ms/step - loss: 0.1928 - accuracy: 0.9333 - val_loss: 0.4346 - val_accuracy: 0.8737\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 147s 126ms/step - loss: 0.1851 - accuracy: 0.9360 - val_loss: 0.4381 - val_accuracy: 0.8733\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 147s 126ms/step - loss: 0.1791 - accuracy: 0.9376 - val_loss: 0.3962 - val_accuracy: 0.8929\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 147s 126ms/step - loss: 0.1723 - accuracy: 0.9400 - val_loss: 0.4029 - val_accuracy: 0.8882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc9a746a898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlI20zF9cGo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOKDctxwcHmS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qUzBirycM_-",
        "colab_type": "text"
      },
      "source": [
        "# **Playing with compression**\n",
        "**Summary**: We have fixed growth rate to 5 and experiment with high(0.2), medium(0.5) and low compression(0.9). We recorded the accuracy , number of parameters and training time per epoch.  We ran experiments for 10-20 epochs\n",
        "\n",
        "As expected if we use more compression, the parameters would decrease and training time also decreased meagerly. But accuracy actually remained same for compression = 0.2 when compared to 0.5 which was a surprise, maybe there were more redundancy in featuremaps that could be removed without affecting overall outcome. As expected for low compression(0.9) we got 87% accuracy.\n",
        "\n",
        "**compression = 0.2 | config 7:** \n",
        "Params used = 120K, time taken to train per epoch = 83 seconds, \n",
        "Training accuracy = 88%, Test accuracy = 86.5%\n",
        "\n",
        "**compression = 0.5 | config 2:** \n",
        "Params used = 150K, time taken to train per epoch = 85 seconds, \n",
        "Training accuracy = 89%, Test accuracy = 87.2%\n",
        "\n",
        "**compression = 0.9 | config 8:** \n",
        "Params used = 200K, time taken to train per epoch = 90 seconds, \n",
        "Training accuracy = 91%, Test accuracy = 87%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6KD-xl2cSgi",
        "colab_type": "code",
        "outputId": "e9d59139-5703-487d-fb70-ef6e71728394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config7 = densenet_model(input_shape = (32, 32, 3), compression = 0.2, growth_rate = 5, bottleneck = True, bottleneck_channels = 32)\n",
        "model_config7.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.002), metrics=['accuracy'])\n",
        "model_config7.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_1 (BatchNormalization)    (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_318 (Activation)     (None, 32, 32, 36)   0           bn_1a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_318[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_319 (Activation)     (None, 32, 32, 32)   0           bn_1a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        activation_319[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_8 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_1 (BatchNormalization)    (None, 32, 32, 52)   208         tf_op_layer_concat_1a_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_320 (Activation)     (None, 32, 32, 52)   0           bn_1b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_320[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_321 (Activation)     (None, 32, 32, 32)   0           bn_1b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        activation_321[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_8 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_1 (BatchNormalization)    (None, 32, 32, 68)   272         tf_op_layer_concat_1b_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_322 (Activation)     (None, 32, 32, 68)   0           bn_1c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_322[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_323 (Activation)     (None, 32, 32, 32)   0           bn_1c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        activation_323[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_8 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_1 (BatchNormalization)    (None, 32, 32, 84)   336         tf_op_layer_concat_1c_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_324 (Activation)     (None, 32, 32, 84)   0           bn_1d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1d (Conv2D)          (None, 32, 32, 32)   2720        activation_324[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_325 (Activation)     (None, 32, 32, 32)   0           bn_1d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   4624        activation_325[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d_7 (Tensor [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_1 (BatchNormalization)    (None, 32, 32, 100)  400         tf_op_layer_concat_1d_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_326 (Activation)     (None, 32, 32, 100)  0           bn_1e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1e (Conv2D)          (None, 32, 32, 32)   3232        activation_326[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_327 (Activation)     (None, 32, 32, 32)   0           bn_1e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   4624        activation_327[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e_7 (Tensor [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_328 (Activation)     (None, 32, 32, 116)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 23)   2691        activation_328[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_24 (AveragePo (None, 16, 16, 23)   0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_1 (BatchNormalization)    (None, 16, 16, 23)   92          average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_329 (Activation)     (None, 16, 16, 23)   0           bn_2a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   768         activation_329[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_330 (Activation)     (None, 16, 16, 32)   0           bn_2a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        activation_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_8 (Tensor [(None, 16, 16, 39)] 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_1 (BatchNormalization)    (None, 16, 16, 39)   156         tf_op_layer_concat_2a_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_331 (Activation)     (None, 16, 16, 39)   0           bn_2b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   1280        activation_331[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_332 (Activation)     (None, 16, 16, 32)   0           bn_2b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        activation_332[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_8 (Tensor [(None, 16, 16, 55)] 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_1 (BatchNormalization)    (None, 16, 16, 55)   220         tf_op_layer_concat_2b_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_333 (Activation)     (None, 16, 16, 55)   0           bn_2c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   1792        activation_333[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_334 (Activation)     (None, 16, 16, 32)   0           bn_2c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        activation_334[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_8 (Tensor [(None, 16, 16, 71)] 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_1 (BatchNormalization)    (None, 16, 16, 71)   284         tf_op_layer_concat_2c_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_335 (Activation)     (None, 16, 16, 71)   0           bn_2d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2d (Conv2D)          (None, 16, 16, 32)   2304        activation_335[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_336 (Activation)     (None, 16, 16, 32)   0           bn_2d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   4624        activation_336[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d_7 (Tensor [(None, 16, 16, 87)] 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_1 (BatchNormalization)    (None, 16, 16, 87)   348         tf_op_layer_concat_2d_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_337 (Activation)     (None, 16, 16, 87)   0           bn_2e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2e (Conv2D)          (None, 16, 16, 32)   2816        activation_337[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_338 (Activation)     (None, 16, 16, 32)   0           bn_2e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   4624        activation_338[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e_7 (Tensor [(None, 16, 16, 103) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 103)  412         tf_op_layer_concat_2e_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_339 (Activation)     (None, 16, 16, 103)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 20)   2080        activation_339[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_25 (AveragePo (None, 8, 8, 20)     0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_1 (BatchNormalization)    (None, 8, 8, 20)     80          average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_340 (Activation)     (None, 8, 8, 20)     0           bn_3a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     672         activation_340[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_341 (Activation)     (None, 8, 8, 32)     0           bn_3a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        activation_341[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_8 (Tensor [(None, 8, 8, 36)]   0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_1 (BatchNormalization)    (None, 8, 8, 36)     144         tf_op_layer_concat_3a_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_342 (Activation)     (None, 8, 8, 36)     0           bn_3b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     1184        activation_342[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_343 (Activation)     (None, 8, 8, 32)     0           bn_3b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        activation_343[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_8 (Tensor [(None, 8, 8, 52)]   0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_1 (BatchNormalization)    (None, 8, 8, 52)     208         tf_op_layer_concat_3b_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_344 (Activation)     (None, 8, 8, 52)     0           bn_3c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     1696        activation_344[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_345 (Activation)     (None, 8, 8, 32)     0           bn_3c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        activation_345[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_8 (Tensor [(None, 8, 8, 68)]   0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_1 (BatchNormalization)    (None, 8, 8, 68)     272         tf_op_layer_concat_3c_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_346 (Activation)     (None, 8, 8, 68)     0           bn_3d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3d (Conv2D)          (None, 8, 8, 32)     2208        activation_346[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_347 (Activation)     (None, 8, 8, 32)     0           bn_3d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     4624        activation_347[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d_7 (Tensor [(None, 8, 8, 84)]   0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_1 (BatchNormalization)    (None, 8, 8, 84)     336         tf_op_layer_concat_3d_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_348 (Activation)     (None, 8, 8, 84)     0           bn_3e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3e (Conv2D)          (None, 8, 8, 32)     2720        activation_348[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_349 (Activation)     (None, 8, 8, 32)     0           bn_3e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     4624        activation_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e_7 (Tensor [(None, 8, 8, 100)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 8, 8, 100)    400         tf_op_layer_concat_3e_7[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_350 (Activation)     (None, 8, 8, 100)    0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_26 (AveragePo (None, 4, 4, 100)    0           activation_350[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 3, 3, 10)     4010        average_pooling2d_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 90)           0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 117,567\n",
            "Trainable params: 114,219\n",
            "Non-trainable params: 3,348\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 1.2971 - accuracy: 0.5281 - val_loss: 1.3648 - val_accuracy: 0.5502\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.8825 - accuracy: 0.6861 - val_loss: 0.8227 - val_accuracy: 0.7136\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.7362 - accuracy: 0.7406 - val_loss: 0.8624 - val_accuracy: 0.7241\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.6521 - accuracy: 0.7727 - val_loss: 0.6593 - val_accuracy: 0.7782\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.5930 - accuracy: 0.7934 - val_loss: 0.7971 - val_accuracy: 0.7488\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.5449 - accuracy: 0.8113 - val_loss: 0.6263 - val_accuracy: 0.7939\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.5147 - accuracy: 0.8211 - val_loss: 0.6168 - val_accuracy: 0.7933\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.4854 - accuracy: 0.8307 - val_loss: 0.7250 - val_accuracy: 0.7693\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.4625 - accuracy: 0.8388 - val_loss: 0.5537 - val_accuracy: 0.8159\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.4382 - accuracy: 0.8483 - val_loss: 0.8216 - val_accuracy: 0.7543\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.4245 - accuracy: 0.8524 - val_loss: 0.5003 - val_accuracy: 0.8396\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.4112 - accuracy: 0.8572 - val_loss: 0.4792 - val_accuracy: 0.8446\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3959 - accuracy: 0.8628 - val_loss: 0.5064 - val_accuracy: 0.8316\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 86s 73ms/step - loss: 0.3866 - accuracy: 0.8652 - val_loss: 0.5374 - val_accuracy: 0.8358\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3763 - accuracy: 0.8698 - val_loss: 0.5149 - val_accuracy: 0.8376\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3678 - accuracy: 0.8734 - val_loss: 0.5034 - val_accuracy: 0.8425\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3588 - accuracy: 0.8758 - val_loss: 0.5340 - val_accuracy: 0.8343\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3485 - accuracy: 0.8788 - val_loss: 0.4561 - val_accuracy: 0.8558\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3422 - accuracy: 0.8819 - val_loss: 0.4323 - val_accuracy: 0.8647\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 85s 73ms/step - loss: 0.3354 - accuracy: 0.8836 - val_loss: 0.4723 - val_accuracy: 0.8496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc9a4b6e9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZncXph7Ko5S4",
        "colab_type": "code",
        "outputId": "9bdabc8c-1cc8-4f26-ea09-f2223db57abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_config8 = densenet_model(input_shape = (32, 32, 3), compression = 0.9, growth_rate = 5, bottleneck = True, bottleneck_channels = 32)\n",
        "model_config8.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.002), metrics=['accuracy'])\n",
        "model_config8.fit(datagen.flow(x_train, y_train, batch_size), steps_per_epoch = 3*x_train.shape[0]/batch_size, \n",
        "                    epochs = 20 ,validation_data =(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cifar (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_preprocess (Conv2D)        (None, 32, 32, 36)   1008        cifar[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_1 (BatchNormalization)    (None, 32, 32, 36)   144         conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_351 (Activation)     (None, 32, 32, 36)   0           bn_1a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1a (Conv2D)          (None, 32, 32, 32)   1184        activation_351[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1a_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_352 (Activation)     (None, 32, 32, 32)   0           bn_1a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1a (Conv2D)                (None, 32, 32, 16)   4624        activation_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1a_9 (Tensor [(None, 32, 32, 52)] 0           conv_1a[0][0]                    \n",
            "                                                                 conv_preprocess[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_1 (BatchNormalization)    (None, 32, 32, 52)   208         tf_op_layer_concat_1a_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_353 (Activation)     (None, 32, 32, 52)   0           bn_1b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1b (Conv2D)          (None, 32, 32, 32)   1696        activation_353[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1b_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_354 (Activation)     (None, 32, 32, 32)   0           bn_1b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1b (Conv2D)                (None, 32, 32, 16)   4624        activation_354[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1b_9 (Tensor [(None, 32, 32, 68)] 0           conv_1b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1a_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_1 (BatchNormalization)    (None, 32, 32, 68)   272         tf_op_layer_concat_1b_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_355 (Activation)     (None, 32, 32, 68)   0           bn_1c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1c (Conv2D)          (None, 32, 32, 32)   2208        activation_355[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1c_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_356 (Activation)     (None, 32, 32, 32)   0           bn_1c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1c (Conv2D)                (None, 32, 32, 16)   4624        activation_356[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1c_9 (Tensor [(None, 32, 32, 84)] 0           conv_1c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1b_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_1 (BatchNormalization)    (None, 32, 32, 84)   336         tf_op_layer_concat_1c_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_357 (Activation)     (None, 32, 32, 84)   0           bn_1d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1d (Conv2D)          (None, 32, 32, 32)   2720        activation_357[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1d_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_358 (Activation)     (None, 32, 32, 32)   0           bn_1d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1d (Conv2D)                (None, 32, 32, 16)   4624        activation_358[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1d_8 (Tensor [(None, 32, 32, 100) 0           conv_1d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1c_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_1 (BatchNormalization)    (None, 32, 32, 100)  400         tf_op_layer_concat_1d_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_359 (Activation)     (None, 32, 32, 100)  0           bn_1e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_1e (Conv2D)          (None, 32, 32, 32)   3232        activation_359[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_1e_2 (BatchNormalization)    (None, 32, 32, 32)   128         bottleneck_1e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_360 (Activation)     (None, 32, 32, 32)   0           bn_1e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_1e (Conv2D)                (None, 32, 32, 16)   4624        activation_360[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1e_8 (Tensor [(None, 32, 32, 116) 0           conv_1e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_1d_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_1 (BatchNormalization)    (None, 32, 32, 116)  464         tf_op_layer_concat_1e_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_361 (Activation)     (None, 32, 32, 116)  0           bn_tr_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr1 (Conv2D)               (None, 32, 32, 104)  12168       activation_361[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_27 (AveragePo (None, 16, 16, 104)  0           conv_tr1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_1 (BatchNormalization)    (None, 16, 16, 104)  416         average_pooling2d_27[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_362 (Activation)     (None, 16, 16, 104)  0           bn_2a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2a (Conv2D)          (None, 16, 16, 32)   3360        activation_362[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2a_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_363 (Activation)     (None, 16, 16, 32)   0           bn_2a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2a (Conv2D)                (None, 16, 16, 16)   4624        activation_363[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2a_9 (Tensor [(None, 16, 16, 120) 0           conv_2a[0][0]                    \n",
            "                                                                 average_pooling2d_27[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_1 (BatchNormalization)    (None, 16, 16, 120)  480         tf_op_layer_concat_2a_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_364 (Activation)     (None, 16, 16, 120)  0           bn_2b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2b (Conv2D)          (None, 16, 16, 32)   3872        activation_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2b_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_365 (Activation)     (None, 16, 16, 32)   0           bn_2b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2b (Conv2D)                (None, 16, 16, 16)   4624        activation_365[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2b_9 (Tensor [(None, 16, 16, 136) 0           conv_2b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2a_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_1 (BatchNormalization)    (None, 16, 16, 136)  544         tf_op_layer_concat_2b_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_366 (Activation)     (None, 16, 16, 136)  0           bn_2c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2c (Conv2D)          (None, 16, 16, 32)   4384        activation_366[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2c_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_367 (Activation)     (None, 16, 16, 32)   0           bn_2c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2c (Conv2D)                (None, 16, 16, 16)   4624        activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2c_9 (Tensor [(None, 16, 16, 152) 0           conv_2c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2b_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_1 (BatchNormalization)    (None, 16, 16, 152)  608         tf_op_layer_concat_2c_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_368 (Activation)     (None, 16, 16, 152)  0           bn_2d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2d (Conv2D)          (None, 16, 16, 32)   4896        activation_368[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2d_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_369 (Activation)     (None, 16, 16, 32)   0           bn_2d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2d (Conv2D)                (None, 16, 16, 16)   4624        activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2d_8 (Tensor [(None, 16, 16, 168) 0           conv_2d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2c_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_1 (BatchNormalization)    (None, 16, 16, 168)  672         tf_op_layer_concat_2d_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_370 (Activation)     (None, 16, 16, 168)  0           bn_2e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_2e (Conv2D)          (None, 16, 16, 32)   5408        activation_370[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_2e_2 (BatchNormalization)    (None, 16, 16, 32)   128         bottleneck_2e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_371 (Activation)     (None, 16, 16, 32)   0           bn_2e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_2e (Conv2D)                (None, 16, 16, 16)   4624        activation_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2e_8 (Tensor [(None, 16, 16, 184) 0           conv_2e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_2d_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_tr_2 (BatchNormalization)    (None, 16, 16, 184)  736         tf_op_layer_concat_2e_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_372 (Activation)     (None, 16, 16, 184)  0           bn_tr_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_tr2 (Conv2D)               (None, 16, 16, 165)  30525       activation_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_28 (AveragePo (None, 8, 8, 165)    0           conv_tr2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_1 (BatchNormalization)    (None, 8, 8, 165)    660         average_pooling2d_28[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_373 (Activation)     (None, 8, 8, 165)    0           bn_3a_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3a (Conv2D)          (None, 8, 8, 32)     5312        activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3a_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3a[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_374 (Activation)     (None, 8, 8, 32)     0           bn_3a_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3a (Conv2D)                (None, 8, 8, 16)     4624        activation_374[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3a_9 (Tensor [(None, 8, 8, 181)]  0           conv_3a[0][0]                    \n",
            "                                                                 average_pooling2d_28[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_1 (BatchNormalization)    (None, 8, 8, 181)    724         tf_op_layer_concat_3a_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_375 (Activation)     (None, 8, 8, 181)    0           bn_3b_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3b (Conv2D)          (None, 8, 8, 32)     5824        activation_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3b_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3b[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 8, 8, 32)     0           bn_3b_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3b (Conv2D)                (None, 8, 8, 16)     4624        activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3b_9 (Tensor [(None, 8, 8, 197)]  0           conv_3b[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3a_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_1 (BatchNormalization)    (None, 8, 8, 197)    788         tf_op_layer_concat_3b_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_377 (Activation)     (None, 8, 8, 197)    0           bn_3c_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3c (Conv2D)          (None, 8, 8, 32)     6336        activation_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3c_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3c[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_378 (Activation)     (None, 8, 8, 32)     0           bn_3c_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3c (Conv2D)                (None, 8, 8, 16)     4624        activation_378[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3c_9 (Tensor [(None, 8, 8, 213)]  0           conv_3c[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3b_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_1 (BatchNormalization)    (None, 8, 8, 213)    852         tf_op_layer_concat_3c_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_379 (Activation)     (None, 8, 8, 213)    0           bn_3d_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3d (Conv2D)          (None, 8, 8, 32)     6848        activation_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3d_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_380 (Activation)     (None, 8, 8, 32)     0           bn_3d_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3d (Conv2D)                (None, 8, 8, 16)     4624        activation_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3d_8 (Tensor [(None, 8, 8, 229)]  0           conv_3d[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3c_9[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_1 (BatchNormalization)    (None, 8, 8, 229)    916         tf_op_layer_concat_3d_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_381 (Activation)     (None, 8, 8, 229)    0           bn_3e_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bottleneck_3e (Conv2D)          (None, 8, 8, 32)     7360        activation_381[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn_3e_2 (BatchNormalization)    (None, 8, 8, 32)     128         bottleneck_3e[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_382 (Activation)     (None, 8, 8, 32)     0           bn_3e_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_3e (Conv2D)                (None, 8, 8, 16)     4624        activation_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_3e_8 (Tensor [(None, 8, 8, 245)]  0           conv_3e[0][0]                    \n",
            "                                                                 tf_op_layer_concat_3d_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8, 8, 245)    980         tf_op_layer_concat_3e_8[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_383 (Activation)     (None, 8, 8, 245)    0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_29 (AveragePo (None, 4, 4, 245)    0           activation_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 3, 3, 10)     9810        average_pooling2d_29[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 90)           0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           2912        flatten_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           330         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 202,873\n",
            "Trainable params: 196,813\n",
            "Non-trainable params: 6,060\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 1.1961 - accuracy: 0.5701 - val_loss: 0.9399 - val_accuracy: 0.6751\n",
            "Epoch 2/20\n",
            "1172/1171 [==============================] - 88s 75ms/step - loss: 0.7787 - accuracy: 0.7270 - val_loss: 0.9745 - val_accuracy: 0.7029\n",
            "Epoch 3/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.6311 - accuracy: 0.7805 - val_loss: 0.6164 - val_accuracy: 0.7965\n",
            "Epoch 4/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.5463 - accuracy: 0.8113 - val_loss: 0.7139 - val_accuracy: 0.7735\n",
            "Epoch 5/20\n",
            "1172/1171 [==============================] - 90s 77ms/step - loss: 0.4905 - accuracy: 0.8307 - val_loss: 0.5937 - val_accuracy: 0.8054\n",
            "Epoch 6/20\n",
            "1172/1171 [==============================] - 91s 77ms/step - loss: 0.4490 - accuracy: 0.8444 - val_loss: 0.6415 - val_accuracy: 0.7933\n",
            "Epoch 7/20\n",
            "1172/1171 [==============================] - 91s 78ms/step - loss: 0.4187 - accuracy: 0.8555 - val_loss: 0.5244 - val_accuracy: 0.8318\n",
            "Epoch 8/20\n",
            "1172/1171 [==============================] - 88s 75ms/step - loss: 0.3908 - accuracy: 0.8648 - val_loss: 0.4425 - val_accuracy: 0.8531\n",
            "Epoch 9/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.3723 - accuracy: 0.8707 - val_loss: 0.4401 - val_accuracy: 0.8518\n",
            "Epoch 10/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.3531 - accuracy: 0.8765 - val_loss: 0.5368 - val_accuracy: 0.8348\n",
            "Epoch 11/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.3382 - accuracy: 0.8826 - val_loss: 0.4957 - val_accuracy: 0.8477\n",
            "Epoch 12/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.3252 - accuracy: 0.8872 - val_loss: 0.4202 - val_accuracy: 0.8660\n",
            "Epoch 13/20\n",
            "1172/1171 [==============================] - 90s 76ms/step - loss: 0.3117 - accuracy: 0.8919 - val_loss: 0.6491 - val_accuracy: 0.8167\n",
            "Epoch 14/20\n",
            "1172/1171 [==============================] - 90s 77ms/step - loss: 0.3003 - accuracy: 0.8953 - val_loss: 0.4539 - val_accuracy: 0.8591\n",
            "Epoch 15/20\n",
            "1172/1171 [==============================] - 90s 77ms/step - loss: 0.2900 - accuracy: 0.8994 - val_loss: 0.4273 - val_accuracy: 0.8675\n",
            "Epoch 16/20\n",
            "1172/1171 [==============================] - 90s 77ms/step - loss: 0.2835 - accuracy: 0.9010 - val_loss: 0.5307 - val_accuracy: 0.8479\n",
            "Epoch 17/20\n",
            "1172/1171 [==============================] - 91s 77ms/step - loss: 0.2736 - accuracy: 0.9044 - val_loss: 0.3649 - val_accuracy: 0.8824\n",
            "Epoch 18/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.2643 - accuracy: 0.9080 - val_loss: 0.5128 - val_accuracy: 0.8514\n",
            "Epoch 19/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.2592 - accuracy: 0.9102 - val_loss: 0.4545 - val_accuracy: 0.8669\n",
            "Epoch 20/20\n",
            "1172/1171 [==============================] - 89s 76ms/step - loss: 0.2504 - accuracy: 0.9131 - val_loss: 0.4380 - val_accuracy: 0.8686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc9a2b17cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi98oBclhc_7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh4GNQQ5hecv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdL2QFiXhfOW",
        "colab_type": "text"
      },
      "source": [
        "# **Using tf.function and recording run time**\n",
        "\n",
        "**Conclusion:** with tf.function computation was a bit faster than regular function for my other teammate. For me, with tf.function it took approximately 10.5 seconds per epoch where as with out tf.function it took approximately 10.5 seconds per epoch. Hence i did not see any particular decrease in runtime, which could be due to numerous factors like availability of compute resources on colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOr3u_3ViSjc",
        "colab_type": "text"
      },
      "source": [
        "Using tf.function on cifar-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmKHsuHthkA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "2b49064d-9d19-48f5-bd74-5601c3709c4f"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "print(train_labels[10])\n",
        "plt.imshow(train_images[10], cmap=plt.cm.binary)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7e966aa240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5VFtuN8hpXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(( train_images.astype(np.float32)/255.0, train_labels.astype(np.int32) ))\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size = train_images.shape[0]).batch(256)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(( test_images.astype(np.float32)/255.0, test_labels.astype(np.int32) ))\n",
        "\n",
        "test_data = test_data.batch(256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT6WVvoshru0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential([layers.Conv2D(64,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\",input_shape=(32,32,3)),\n",
        "                          layers.BatchNormalization(),\n",
        "                          layers.Conv2D(64,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(), \n",
        "                          layers.MaxPooling2D(),\n",
        "                          layers.Dropout(0.3),\n",
        "                           \n",
        "                          layers.Conv2D(128,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(),\n",
        "                          layers.Conv2D(128,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(), \n",
        "                          layers.MaxPooling2D(),\n",
        "                          layers.Dropout(0.4),\n",
        "\n",
        "                          layers.Conv2D(256,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(),\n",
        "                          layers.Conv2D(256,(3,3),activation='relu',padding='same',kernel_initializer= \"he_uniform\"),\n",
        "                          layers.BatchNormalization(), \n",
        "                          layers.MaxPooling2D(),\n",
        "                          layers.Dropout(0.5), \n",
        "                           \n",
        "                          layers.Flatten(), \n",
        "                          layers.Dense(64, activation='relu'), \n",
        "                          layers.Dense(10) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxXKL6x3hxBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=  5 * 1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c__BFcjXhxIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @tf.function\n",
        "def train_step(imgs, lbls):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(imgs)\n",
        "        xent = loss_fn(lbls, logits)\n",
        "\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(xent, varis)\n",
        "    optimizer.apply_gradients(zip(grads, varis))\n",
        "\n",
        "    return xent, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMAhpfyghxE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c03df8b0-513a-4f1b-92cb-a8514c74ee89"
      },
      "source": [
        "start = time.time()\n",
        "epochs=21\n",
        "for epoch in range(epochs):\n",
        "  tf.print(\"===== epoch number: {}\".format(epoch))\n",
        "\n",
        "  start_epoch = time.time()\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
        "    xent, logits = train_step(x_batch_train, y_batch_train)\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "  train_acc = train_acc_metric.result()\n",
        "  tf.print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  for x_batch_val, y_batch_val in test_data:\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  tf.print('Validation acc: %s' % (float(val_acc),))\n",
        "  end_epoch = time.time()\n",
        "  tf.print(\"time taken for this epoch(in seconds): \",(end_epoch - start_epoch))\n",
        "stop = time.time()\n",
        "tf.print(\"total time(in second): \", (stop - start))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== epoch number: 0\n",
            "Training acc over epoch: 0.29030001163482666\n",
            "Validation acc: 0.41440001130104065\n",
            "time taken for this epoch(in seconds):  17.0796959400177\n",
            "===== epoch number: 1\n",
            "Training acc over epoch: 0.4498000144958496\n",
            "Validation acc: 0.4661000072956085\n",
            "time taken for this epoch(in seconds):  10.78994369506836\n",
            "===== epoch number: 2\n",
            "Training acc over epoch: 0.5226399898529053\n",
            "Validation acc: 0.5393999814987183\n",
            "time taken for this epoch(in seconds):  10.54870319366455\n",
            "===== epoch number: 3\n",
            "Training acc over epoch: 0.5794000029563904\n",
            "Validation acc: 0.5791000127792358\n",
            "time taken for this epoch(in seconds):  10.579259395599365\n",
            "===== epoch number: 4\n",
            "Training acc over epoch: 0.6185399889945984\n",
            "Validation acc: 0.6139000058174133\n",
            "time taken for this epoch(in seconds):  10.570479393005371\n",
            "===== epoch number: 5\n",
            "Training acc over epoch: 0.654259979724884\n",
            "Validation acc: 0.6509000062942505\n",
            "time taken for this epoch(in seconds):  10.598912239074707\n",
            "===== epoch number: 6\n",
            "Training acc over epoch: 0.6848599910736084\n",
            "Validation acc: 0.6481999754905701\n",
            "time taken for this epoch(in seconds):  10.655783414840698\n",
            "===== epoch number: 7\n",
            "Training acc over epoch: 0.711679995059967\n",
            "Validation acc: 0.6834999918937683\n",
            "time taken for this epoch(in seconds):  10.637866973876953\n",
            "===== epoch number: 8\n",
            "Training acc over epoch: 0.7352200150489807\n",
            "Validation acc: 0.6766999959945679\n",
            "time taken for this epoch(in seconds):  10.60593843460083\n",
            "===== epoch number: 9\n",
            "Training acc over epoch: 0.7537800073623657\n",
            "Validation acc: 0.6802999973297119\n",
            "time taken for this epoch(in seconds):  10.57397198677063\n",
            "===== epoch number: 10\n",
            "Training acc over epoch: 0.7752599716186523\n",
            "Validation acc: 0.6887000203132629\n",
            "time taken for this epoch(in seconds):  10.75231146812439\n",
            "===== epoch number: 11\n",
            "Training acc over epoch: 0.7913399934768677\n",
            "Validation acc: 0.694100022315979\n",
            "time taken for this epoch(in seconds):  10.631386995315552\n",
            "===== epoch number: 12\n",
            "Training acc over epoch: 0.8105599880218506\n",
            "Validation acc: 0.694599986076355\n",
            "time taken for this epoch(in seconds):  10.646027326583862\n",
            "===== epoch number: 13\n",
            "Training acc over epoch: 0.8277000188827515\n",
            "Validation acc: 0.6948000192642212\n",
            "time taken for this epoch(in seconds):  10.636164903640747\n",
            "===== epoch number: 14\n",
            "Training acc over epoch: 0.8399400115013123\n",
            "Validation acc: 0.6843000054359436\n",
            "time taken for this epoch(in seconds):  10.600911140441895\n",
            "===== epoch number: 15\n",
            "Training acc over epoch: 0.8604999780654907\n",
            "Validation acc: 0.7057999968528748\n",
            "time taken for this epoch(in seconds):  10.561878442764282\n",
            "===== epoch number: 16\n",
            "Training acc over epoch: 0.8678399920463562\n",
            "Validation acc: 0.675599992275238\n",
            "time taken for this epoch(in seconds):  10.709770202636719\n",
            "===== epoch number: 17\n",
            "Training acc over epoch: 0.8830999732017517\n",
            "Validation acc: 0.6952999830245972\n",
            "time taken for this epoch(in seconds):  10.705875873565674\n",
            "===== epoch number: 18\n",
            "Training acc over epoch: 0.8984799981117249\n",
            "Validation acc: 0.6866999864578247\n",
            "time taken for this epoch(in seconds):  10.679130554199219\n",
            "===== epoch number: 19\n",
            "Training acc over epoch: 0.9053400158882141\n",
            "Validation acc: 0.6973000168800354\n",
            "time taken for this epoch(in seconds):  10.705833435058594\n",
            "===== epoch number: 20\n",
            "Training acc over epoch: 0.9174799919128418\n",
            "Validation acc: 0.6916000247001648\n",
            "time taken for this epoch(in seconds):  10.609690427780151\n",
            "total time(in second):  230.001690864563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx4eHsrpiLAD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14b8cda6-f958-42fc-f1d0-8015189ac318"
      },
      "source": [
        "for img_batch, lbl_batch in test_data:\n",
        "    val_acc_metric(lbl_batch, model(img_batch))\n",
        "\n",
        "val_acc_metric.result()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.0965>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ri6bUdiMcL",
        "colab_type": "text"
      },
      "source": [
        "**Without tf.function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9cP-OlWib7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "591911c7-4937-4fbd-b542-cbdc8f7fc222"
      },
      "source": [
        "start = time.time()\n",
        "epochs=11\n",
        "for epoch in range(epochs):\n",
        "  print(\"===== epoch number: {}\".format(epoch))\n",
        "\n",
        "  start_epoch = time.time()\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch_train)\n",
        "        xent = loss_fn(y_batch_train, logits)\n",
        "\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(xent, varis)\n",
        "    optimizer.apply_gradients(zip(grads, varis))\n",
        "    #xent, logits = train_step(x_batch_train, y_batch_train)\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "  train_acc = train_acc_metric.result()\n",
        "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  for x_batch_val, y_batch_val in test_data:\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print('Validation acc: %s' % (float(val_acc),))\n",
        "  end_epoch = time.time()\n",
        "  print(\"time taken for this epoch(in seconds): \",(end_epoch - start_epoch))\n",
        "stop = time.time()\n",
        "print(\"total time(in second): \", (stop - start))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== epoch number: 0\n",
            "Training acc over epoch: 0.10809999704360962\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.450707912445068\n",
            "===== epoch number: 1\n",
            "Training acc over epoch: 0.09749999642372131\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.398381233215332\n",
            "===== epoch number: 2\n",
            "Training acc over epoch: 0.09765999764204025\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.418264150619507\n",
            "===== epoch number: 3\n",
            "Training acc over epoch: 0.09976000338792801\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.35786771774292\n",
            "===== epoch number: 4\n",
            "Training acc over epoch: 0.09861999750137329\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.337143182754517\n",
            "===== epoch number: 5\n",
            "Training acc over epoch: 0.09905999898910522\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.325779914855957\n",
            "===== epoch number: 6\n",
            "Training acc over epoch: 0.09802000224590302\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.427091360092163\n",
            "===== epoch number: 7\n",
            "Training acc over epoch: 0.1006999984383583\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.43753457069397\n",
            "===== epoch number: 8\n",
            "Training acc over epoch: 0.09623999893665314\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.35387110710144\n",
            "===== epoch number: 9\n",
            "Training acc over epoch: 0.09722000360488892\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.380463600158691\n",
            "===== epoch number: 10\n",
            "Training acc over epoch: 0.09798000007867813\n",
            "Validation acc: 0.10000000149011612\n",
            "time taken for this epoch(in seconds):  10.341468811035156\n",
            "total time(in second):  114.23092246055603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlYhLwQbi2-1",
        "colab_type": "text"
      },
      "source": [
        "# **Using computation graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gGMs8Bji5XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU1gyCz8i7nF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test set\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    train_images = train_images.reshape(train_images.shape[0], 1, img_rows, img_cols)\n",
        "    test_images = test_images.reshape(test_images.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
        "    test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9mKe-cFi-cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices(( train_images.astype(np.float32)/255.0, train_labels.astype(np.int32) ))\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size = train_images.shape[0]).batch(32)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(( test_images.astype(np.float32)/255.0, test_labels.astype(np.int32) ))\n",
        "\n",
        "test_data = test_data.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GedfThz-jAa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([layers.Conv2D(64,(3,3),activation='relu',padding='same',input_shape=input_shape),\n",
        "                          layers.MaxPool2D(),\n",
        "                          layers.Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "                          layers.MaxPool2D(), \n",
        "                          layers.Flatten(), \n",
        "                          layers.Dense(64, activation='relu'), \n",
        "                          layers.Dense(10) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRuU2ObbjCnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=  5 * 1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM3Cy1BkjEWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(imgs, lbls):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(imgs)\n",
        "        xent = loss_fn(lbls, logits)\n",
        "\n",
        "    varis = model.trainable_variables\n",
        "    grads = tape.gradient(xent, varis)\n",
        "    optimizer.apply_gradients(zip(grads, varis))\n",
        "\n",
        "    return xent, logits\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = 'logs/func/%s' % stamp\n",
        "writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ORBoC63jGOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8106127c-aa2b-4d0c-e65c-e3822e1c33a1"
      },
      "source": [
        "epochs=5\n",
        "for epoch in range(epochs):\n",
        "  print(\"===== epoch number: {}\".format(epoch))\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n",
        "    tf.summary.trace_on(graph=True, profiler=True)\n",
        "    xent, logits = train_step(x_batch_train, y_batch_train)\n",
        "    with writer.as_default():\n",
        "      tf.summary.trace_export(\n",
        "      name=\"my_func_trace\",\n",
        "      step=0,\n",
        "      profiler_outdir=logdir)\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "  train_acc = train_acc_metric.result()\n",
        "  tf.print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  for x_batch_val, y_batch_val in test_data:\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  tf.print('Validation acc: %s' % (float(val_acc),))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== epoch number: 0\n",
            "Training acc over epoch: 0.9653000235557556\n",
            "Validation acc: 0.9843999743461609\n",
            "===== epoch number: 1\n",
            "Training acc over epoch: 0.9843666553497314\n",
            "Validation acc: 0.9843999743461609\n",
            "===== epoch number: 2\n",
            "Training acc over epoch: 0.9874666929244995\n",
            "Validation acc: 0.9854999780654907\n",
            "===== epoch number: 3\n",
            "Training acc over epoch: 0.9894999861717224\n",
            "Validation acc: 0.9868000149726868\n",
            "===== epoch number: 4\n",
            "Training acc over epoch: 0.9914500117301941\n",
            "Validation acc: 0.9829999804496765\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}