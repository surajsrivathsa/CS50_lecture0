{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 6: Realistic RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ukg40jDE7OWr",
        "l3gcdqdV23hs",
        "Vc201e0hQgKP",
        "wWQu9B9K3qDe",
        "Bn3id5U2GL2c",
        "NAvvvW93PcDi",
        "Szsy76yOP9B_",
        "O4S3KzuIx0OY",
        "WSd8a3bkNANy",
        "OWWwyMvIGdKi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajsrivathsa/ovgu_deeplearning/blob/master/Assignment_6_Realistic_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_9dD0WiFPt",
        "colab_type": "text"
      },
      "source": [
        "Deep Learning programming task\n",
        "\n",
        "**Assignment 6:** Realistic Language Modelling and RNN\n",
        "\n",
        "**Team members:**\n",
        "1. Sanjeeth Busnur Indushekar: 224133 : sanjeeth.busnur@st.ovgu.de\n",
        "2. Aditya Dey : 230580 : aditya.dey@st.ovgu.de\n",
        "3. Suraj Shashidhar: 230052 : suraj.shashidhar@st.ovgu.de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16GTsDxJd8Dt",
        "colab_type": "text"
      },
      "source": [
        "**Tasks to be done:**\n",
        "\n",
        "1) RNN that can be trained on variable-length sequences\n",
        "(You can use keras layers, but implement the masking of the loss and the loss aggregation yourself)\n",
        "\n",
        "\n",
        "2) Sampling variable length sequences from this RNN\n",
        "If those two parts did not work out at all, include the re-implementation of the last assignment with keras functionality\n",
        "\n",
        "\n",
        "3) Bonus: Language Model experiments\n",
        "\n",
        "\n",
        "**Major errors made**\n",
        "\n",
        "1) Did not save the model after training resulting in loss of compute time and resources. Later added code to save it and reload if necessary.\n",
        "\n",
        "2) Tried to run RNN with stateless(previous hidden activation is notcarried forward for the sequence) --> set stateful = True\n",
        "\n",
        "3) While language generation tried to generate entire sequence at once. Changed the code to generate character by character.\n",
        "\n",
        "4) Had added the maxchar length break condition and not break on stop_sequence tag character, changed the code. However we see that with additional max char break condition, generated output looks much better.\n",
        "\n",
        "\n",
        "5) Tried multiplying log probabilities instead of adding them, changed it to addition later\n",
        "\n",
        "\n",
        "**Summary**\n",
        "\n",
        "\n",
        "1) Language models related to both losses generate almost similar output. They hold structure in some places, but are filled with unnecessary spaces as probability of space is nearly 0.97, hence this keeps on repetaing.\n",
        "\n",
        "2) Without stopping for stop character and if we keep on sampling, the network doesn't stop providing outputs. This may result in too much text overloading compute resource. Hence here for demo purpose we added max len of 5000 as break after generating long sequence.\n",
        "\n",
        "3) Even though bible doesn't have word \"abrams\" but has word \"Abrams\" we see that probability of \"abrams\" > \"Abrams\" due to character level generation and probability and not word level\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUNuFKtiZ9n3",
        "colab_type": "text"
      },
      "source": [
        "**Sample King James Bible**\n",
        "\n",
        "1:1 In the beginning God created the heaven and the earth.\n",
        "\n",
        "1:2 And the earth was without form, and void; and darkness was upon\n",
        "the face of the deep. And the Spirit of God moved upon the face of the\n",
        "waters.\n",
        "\n",
        "1:3 And God said, Let there be light: and there was light.\n",
        "\n",
        "1:4 And God saw the light, that it was good: and God divided the light\n",
        "from the darkness.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adh3WsE1dmkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59zPOCe1KwKa",
        "colab_type": "code",
        "outputId": "6959770d-5567-438c-db54-351f16d4818b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import initializers\n",
        "import tensorboard\n",
        "import time\n",
        "from datetime import datetime\n",
        "from keras import backend as K\n",
        "from prepare_data2 import parse_seq\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0iZWwrneCIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 448 is the longest length sequence there are 31k sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO6eemPNfoat",
        "colab_type": "code",
        "outputId": "91077a08-5f75-46d2-c819-c5d249901137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "  print(os.getcwd())\n",
        "  print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F420Gu-7f3x8",
        "colab_type": "code",
        "outputId": "7c88fb82-7c1e-401d-8449-041625be16d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676Zm3kMgNJP",
        "colab_type": "code",
        "outputId": "04ff9b2e-233f-4b4a-9ed3-97ed3d2c7ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "\n",
        "path = '.'\n",
        " \n",
        "files = os.listdir(path)\n",
        "for name in files:\n",
        "    print(name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".config\n",
            "deeplearning.tfrecords\n",
            "prepare_data2.py\n",
            "the-king-james-bible.txt\n",
            "deeplearning_vocab\n",
            "__pycache__\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukg40jDE7OWr",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing - King James Bible**\n",
        "\n",
        "**Summary:** Data was already converted to sequences according to regex and serialized in local machine\n",
        "\n",
        "1) Read the serialized data and vocab.\n",
        "\n",
        "2) Parse the serialized data and convert it to category according to vocab dict\n",
        "\n",
        "3) Use batch padding and max sequence size of 448 which was displayed to be the sequence of max length during serialization.\n",
        "\n",
        "4) Convert the categorical data to onehot encoded data, so the dimensions must be **[batch size=128, max time steps = 448, vocab size = 78]**\n",
        "\n",
        "5) Print all three data one with no padding, one with padding and one hot encoded so that we can see the difference for first few tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsPlkWyc7ReY",
        "colab_type": "code",
        "outputId": "4231e206-43ca-457b-d681-6f33f14685ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"deeplearning.tfrecords\")\n",
        "\n",
        "\n",
        "#data.padded_batch\n",
        "#batched_data = data.padded_batch(batch_size = 128, drop_remainder=True)\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x))\n",
        "\n",
        "\n",
        "batched_categorical_data = data.padded_batch(batch_size=128, padded_shapes=448,padding_values=0, drop_remainder=True)\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"deeplearning_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "ch_to_ind = {v: k for k, v in ind_to_ch.items()}\n",
        "print(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 3, 'B': 4, 'r': 5, 'b': 6, 'p': 7, 'N': 8, '?': 9, 'C': 10, 'U': 11, '\\ufeff': 12, 'l': 13, 'J': 14, 'd': 15, 'j': 16, ',': 17, 'I': 18, 'v': 19, 'M': 20, 'n': 21, 'g': 22, '4': 23, 'y': 24, '6': 25, '0': 26, 'T': 27, 'O': 28, '*': 29, 'x': 30, ')': 31, 'i': 32, 's': 33, 'L': 34, 'f': 35, 'z': 36, 'e': 37, '3': 38, 't': 39, 'V': 40, 'w': 41, '.': 42, 'G': 43, '!': 44, 'R': 45, ' ': 46, 'Q': 47, 'H': 48, '\\n': 49, 'u': 50, '8': 51, 'E': 52, 'P': 53, '7': 54, 'Z': 55, 'o': 56, 'm': 57, ';': 58, '(': 59, 'h': 60, 'K': 61, 'D': 62, ':': 63, '1': 64, 'W': 65, 'q': 66, 'c': 67, '9': 68, \"'\": 69, 'S': 70, '-': 71, 'Y': 72, 'F': 73, '2': 74, 'A': 75, '5': 76, 'k': 77, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufRBb3phhk-",
        "colab_type": "code",
        "outputId": "2dd078b6-bf05-4be6-ab12-db7023de6bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "for num, elem in enumerate(data):\n",
        "  if(num > 1):\n",
        "    break;\n",
        "  print(elem)\n",
        "  print(\" ====== Creating Labels by taking slices ==========\")\n",
        "  print(elem[1:])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 1 12 27 60 37 46 73 32  5 33 39 46  4 56 56 77 46 56 35 46 20 56 33 37\n",
            " 33 63 46 46 10  3 13 13 37 15 46 43 37 21 37 33 32 33 49 49 49  2], shape=(46,), dtype=int32)\n",
            " ====== Creating Labels by taking slices ==========\n",
            "tf.Tensor(\n",
            "[12 27 60 37 46 73 32  5 33 39 46  4 56 56 77 46 56 35 46 20 56 33 37 33\n",
            " 63 46 46 10  3 13 13 37 15 46 43 37 21 37 33 32 33 49 49 49  2], shape=(45,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[ 1 46 18 21 46 39 60 37 46  6 37 22 32 21 21 32 21 22 46 43 56 15 46 67\n",
            "  5 37  3 39 37 15 46 39 60 37 46 60 37  3 19 37 21 46  3 21 15 46 39 60\n",
            " 37 46 37  3  5 39 60 42 49 49  2], shape=(59,), dtype=int32)\n",
            " ====== Creating Labels by taking slices ==========\n",
            "tf.Tensor(\n",
            "[46 18 21 46 39 60 37 46  6 37 22 32 21 21 32 21 22 46 43 56 15 46 67  5\n",
            " 37  3 39 37 15 46 39 60 37 46 60 37  3 19 37 21 46  3 21 15 46 39 60 37\n",
            " 46 37  3  5 39 60 42 49 49  2], shape=(58,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VmVp3W0m8Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_label(ds):\n",
        "  return ds[1:];\n",
        "\n",
        "all_label_data = data.map(create_label)\n",
        "batched_label_data = all_label_data.padded_batch(batch_size=128, padded_shapes=448,padding_values=0, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7vdyiFa97RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehotencode(ds):\n",
        "  \n",
        "  new_data = tf.one_hot(indices = ds, depth = vocab_size)\n",
        "  return new_data;\n",
        "\n",
        "onehot_encoded_batch_data = batched_categorical_data.map(onehotencode)\n",
        "onehot_encoded_label_data = batched_label_data.map(onehotencode)\n",
        "#new_data = data.map(onehotencode)\n",
        "#list(new_data.as_numpy_iterator())[1:5]\n",
        "#list(data.as_numpy_iterator())[1:2]\n",
        "#tf.one_hot(data, depth=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ4aDbJOLYgD",
        "colab_type": "code",
        "outputId": "ac253c6c-6747-4058-c3b6-f540e575ae8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Display varying sizes of each sequence in data vs padded sequence in batched categorical data vs onehot encoded added elements\n",
        "\n",
        "for batch_num, (original_element, padded_element, padded_label, onehotencoded_padded_element, onehotencoded_label) in enumerate(zip(data, batched_categorical_data, batched_label_data, onehot_encoded_batch_data, onehot_encoded_label_data)):\n",
        "  if(batch_num > 3):\n",
        "    break;\n",
        "  print(\"Batch number is : {}\".format(batch_num))\n",
        "  print(\" ===== ======= ======== \")\n",
        "  print(\"original unpadded sequence\")\n",
        "  print(original_element)\n",
        "  print(\" ===== ======= ======== \")\n",
        "  print(\"padded input sequence\")\n",
        "  print(padded_element)\n",
        "  print(\" ===== ======= ======== \")\n",
        "  print(\"padded label\")\n",
        "  print(padded_label)\n",
        "  print(\" ======= ======= ======\")\n",
        "  print(\"onehot padded input\")\n",
        "  print(onehotencoded_padded_element)\n",
        "  print(\" ===== ======= ======== \")\n",
        "  print(\"onehot padded label\")\n",
        "  print(onehotencoded_label)\n",
        "  print(\"====== ======= =====\")\n",
        "  print()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch number is : 0\n",
            " ===== ======= ======== \n",
            "original unpadded sequence\n",
            "tf.Tensor(\n",
            "[ 1 12 27 60 37 46 73 32  5 33 39 46  4 56 56 77 46 56 35 46 20 56 33 37\n",
            " 33 63 46 46 10  3 13 13 37 15 46 43 37 21 37 33 32 33 49 49 49  2], shape=(46,), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded input sequence\n",
            "tf.Tensor(\n",
            "[[ 1 12 27 ...  0  0  0]\n",
            " [ 1 46 18 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " ...\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded label\n",
            "tf.Tensor(\n",
            "[[12 27 60 ...  0  0  0]\n",
            " [46 18 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " ...\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ======= ======= ======\n",
            "onehot padded input\n",
            "tf.Tensor(\n",
            "[[[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            " ===== ======= ======== \n",
            "onehot padded label\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            "====== ======= =====\n",
            "\n",
            "Batch number is : 1\n",
            " ===== ======= ======== \n",
            "original unpadded sequence\n",
            "tf.Tensor(\n",
            "[ 1 46 18 21 46 39 60 37 46  6 37 22 32 21 21 32 21 22 46 43 56 15 46 67\n",
            "  5 37  3 39 37 15 46 39 60 37 46 60 37  3 19 37 21 46  3 21 15 46 39 60\n",
            " 37 46 37  3  5 39 60 42 49 49  2], shape=(59,), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded input sequence\n",
            "tf.Tensor(\n",
            "[[ 1 49 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " ...\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 27 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded label\n",
            "tf.Tensor(\n",
            "[[49 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " ...\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 27 60 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ======= ======= ======\n",
            "onehot padded input\n",
            "tf.Tensor(\n",
            "[[[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            " ===== ======= ======== \n",
            "onehot padded label\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            "====== ======= =====\n",
            "\n",
            "Batch number is : 2\n",
            " ===== ======= ======== \n",
            "original unpadded sequence\n",
            "tf.Tensor(\n",
            "[ 1 46 75 21 15 46 39 60 37 46 37  3  5 39 60 46 41  3 33 46 41 32 39 60\n",
            " 56 50 39 46 35 56  5 57 17 46  3 21 15 46 19 56 32 15 58 46  3 21 15 46\n",
            " 15  3  5 77 21 37 33 33 46 41  3 33 46 50  7 56 21 49 39 60 37 46 35  3\n",
            " 67 37 46 56 35 46 39 60 37 46 15 37 37  7 42 46 75 21 15 46 39 60 37 46\n",
            " 70  7 32  5 32 39 46 56 35 46 43 56 15 46 57 56 19 37 15 46 50  7 56 21\n",
            " 46 39 60 37 46 35  3 67 37 46 56 35 46 39 60 37 49 41  3 39 37  5 33 42\n",
            " 49 49  2], shape=(147,), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded input sequence\n",
            "tf.Tensor(\n",
            "[[ 1 46 11 ...  0  0  0]\n",
            " [ 1 46 27 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " ...\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46  8 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded label\n",
            "tf.Tensor(\n",
            "[[46 11 21 ...  0  0  0]\n",
            " [46 27 60 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " ...\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46  8 56 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ======= ======= ======\n",
            "onehot padded input\n",
            "tf.Tensor(\n",
            "[[[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            " ===== ======= ======== \n",
            "onehot padded label\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            "====== ======= =====\n",
            "\n",
            "Batch number is : 3\n",
            " ===== ======= ======== \n",
            "original unpadded sequence\n",
            "tf.Tensor(\n",
            "[ 1 46 75 21 15 46 43 56 15 46 33  3 32 15 17 46 34 37 39 46 39 60 37  5\n",
            " 37 46  6 37 46 13 32 22 60 39 63 46  3 21 15 46 39 60 37  5 37 46 41  3\n",
            " 33 46 13 32 22 60 39 42 49 49  2], shape=(59,), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded input sequence\n",
            "tf.Tensor(\n",
            "[[ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " ...\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]\n",
            " [ 1 46 75 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ===== ======= ======== \n",
            "padded label\n",
            "tf.Tensor(\n",
            "[[46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " ...\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]\n",
            " [46 75 21 ...  0  0  0]], shape=(128, 448), dtype=int32)\n",
            " ======= ======= ======\n",
            "onehot padded input\n",
            "tf.Tensor(\n",
            "[[[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            " ===== ======= ======== \n",
            "onehot padded label\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]]], shape=(128, 448, 78), dtype=float32)\n",
            "====== ======= =====\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7KXAhywZM-e",
        "colab_type": "text"
      },
      "source": [
        "# **Testing out Keras RNN based full layers (Fully Prebuilt RNNS) before applying to full king james data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au-E0L3xZpUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making RNNs stateful, that is it remembers previous activations or state and can use it for next  character\n",
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnBXHaFtZpbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=448,\n",
        "  batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTsFA7xtaGr_",
        "colab_type": "code",
        "outputId": "b71fc950-3750-4c88-efb9-2e6e95d5586b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating a sample tensor of the same shape as each batch and fitting the model and printing the summary\n",
        "# This also hekps print model summary as model is not initialized just by defining it, \n",
        "# it has to be first instantiated really by feeding the data\n",
        "\n",
        "example_input_tensor = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, 448, vocab_size]))\n",
        "example_prediction = model(example_input_tensor)\n",
        "#print(example_prediction)\n",
        "example_prediction.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 448, 78])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm_c2FGEZpX6",
        "colab_type": "code",
        "outputId": "19a2d039-5df2-4cd2-f5d5-5888528a1059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    multiple                  709632    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  35022     \n",
            "=================================================================\n",
            "Total params: 744,654\n",
            "Trainable params: 744,654\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDBPsYPdXz-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1yxpo7edD-8",
        "colab_type": "code",
        "outputId": "9f07dc84-448d-42df-d780-78d9b1fed43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Define custom loss\n",
        "\"\"\"\n",
        "def custom_loss(one_batch_data, max_len):\n",
        "\n",
        "    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
        "    def loss(labels,logits):\n",
        "      non_zero_counts = tf.math.count_nonzero(input=one_batch_data, axis = 1, dtype=tf.dtypes.float32)\n",
        "      non_zero_counts = non_zero_counts - 1\n",
        "      mask = tf.sequence_mask(non_zero_counts, max_len, dtype=tf.dtypes.float32 )\n",
        "\n",
        "      loss_tnsr = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "\n",
        "      masked_loss = tf.multiply(loss_tnsr, mask)\n",
        "\n",
        "      summed_loss_per_sequence_of_batch = tf.reduce_sum(masked_loss, axis = 1)\n",
        "\n",
        "      average_loss_per_sequence_of_batch = tf.divide(summed_loss_per_sequence_of_batch, non_zero_counts)\n",
        "\n",
        "      return average_loss_per_sequence_of_batch\n",
        "\n",
        "    return loss\n",
        "\"\"\"  \n",
        "    \n",
        "    \n",
        "\n",
        "def loss_fnc(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "\"\"\"\n",
        "example_batch_loss  = loss(target_example_batch, example_prediction)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nexample_batch_loss  = loss(target_example_batch, example_prediction)\\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhaWKATQeeMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = tf.keras.models.load_model('model.h5', custom_objects={'loss': custom_loss(one_batch_data, max_len=448)})\n",
        "\n",
        "model.compile(optimizer='adam', loss= loss_fnc, metrics=['accuracy'])\n",
        "epochs = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHGbdn6-dllS",
        "colab_type": "code",
        "outputId": "09d88b0e-67d6-49b3-e074-0a5f74d8f8f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "history = model.fit(zip(onehot_encoded_batch_data, batched_label_data),epochs=epochs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "242/242 [==============================] - 28s 115ms/step - loss: 0.9754 - accuracy: 0.7660\n",
            "Epoch 2/2\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 484 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-edfdf59eb700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_encoded_batch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_label_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_finalize_progbar\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    933\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZvN-sJbzp7N",
        "colab_type": "text"
      },
      "source": [
        "# **Testing out loss masking on toy dataset before applying to Final RNN model**\n",
        "\n",
        "**Summary:** We are testing two custom loss functions both are masking based.\n",
        "\n",
        "**custom_loss_fnc_1**: After masking don't do reduce sum and average it by non-zero counts. Instead calculate sum of loss for every sequence, if we have 128 as batch size then we will have 128 losses.\n",
        "Divide each loss element by its corresponding non zero count per sequence. Finally average this 128 dim tensor.\n",
        "\n",
        "**custom_loss_fnc_2**: Same as mentioned in assignment, once masked directly take reduce sum and average by non-zero count sum\n",
        "\n",
        "\n",
        "As expected loss_fnc 1 gives a bit higher loss than loss fnc 2 as first we take sequence wiae operation and then average out for entire batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f8wnv0RPw9M",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1DK7odnZeI6Hq8nbxU_Ce4uKGbm45O8oV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx_D1XhmADYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(labels, logits):\n",
        "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao1XvHOJc5oZ",
        "colab_type": "code",
        "outputId": "05b84a0d-df08-4e73-b910-0d4df83084f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "non_zero_counts = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, ]))\n",
        "print(non_zero_counts)\n",
        "non_zero_counts = non_zero_counts - 1\n",
        "print(non_zero_counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(128,) dtype=float32, numpy=\n",
            "array([-0.08848309,  0.06307742,  0.09187527,  0.08291286, -0.03411262,\n",
            "        0.11339895,  0.09436277, -0.09279253,  0.10694189,  0.01230136,\n",
            "       -0.11142575, -0.11904617,  0.07565774, -0.09266244, -0.04355676,\n",
            "        0.00621504, -0.0226154 , -0.08840972, -0.08646747, -0.10446479,\n",
            "       -0.10289109,  0.1522908 , -0.02661152, -0.09176905,  0.06746647,\n",
            "       -0.13089491, -0.06654619, -0.0944139 , -0.13866946,  0.05551672,\n",
            "       -0.10106552, -0.02755217, -0.14622976,  0.06731291, -0.0801073 ,\n",
            "        0.02602693,  0.01420946, -0.08935799, -0.1409924 ,  0.03435118,\n",
            "        0.12686832,  0.12743042, -0.06754315,  0.15068687,  0.1249377 ,\n",
            "        0.10851289,  0.11357336, -0.02233432, -0.03777158, -0.1305582 ,\n",
            "        0.03638826, -0.01808612,  0.08201273, -0.13927397,  0.00888909,\n",
            "       -0.14504269, -0.02896619, -0.02775975,  0.06411932,  0.11143847,\n",
            "       -0.08448561,  0.05828932,  0.12321164, -0.14291412,  0.07584174,\n",
            "        0.09134646, -0.10591309,  0.06095034, -0.03991966, -0.12115945,\n",
            "       -0.03705132,  0.06046696,  0.06175739, -0.03163304,  0.01868433,\n",
            "       -0.03464942, -0.13426387,  0.05848116, -0.00963497,  0.11097519,\n",
            "        0.10651542,  0.0870579 , -0.10803298,  0.03093414,  0.05828249,\n",
            "       -0.01355308, -0.10356273,  0.11493643,  0.02642679, -0.09258819,\n",
            "       -0.04087429, -0.10003231,  0.02322076,  0.00155188, -0.07807186,\n",
            "        0.13091545,  0.11316238, -0.01260412,  0.05130489,  0.08558694,\n",
            "        0.02293755,  0.01314414, -0.06958063,  0.07124427,  0.00635554,\n",
            "       -0.1114727 , -0.05282877, -0.00132515, -0.04134565, -0.07124928,\n",
            "       -0.04116837,  0.07131837, -0.00753923, -0.10586603, -0.00714989,\n",
            "       -0.094009  ,  0.04266645, -0.08243054, -0.10077823, -0.0162129 ,\n",
            "        0.08074054,  0.08547649, -0.15107276,  0.14860375,  0.080322  ,\n",
            "       -0.1354693 ,  0.1264904 ,  0.14488147], dtype=float32)>\n",
            "tf.Tensor(\n",
            "[-1.0884831  -0.93692255 -0.90812474 -0.91708714 -1.0341126  -0.88660103\n",
            " -0.90563726 -1.0927925  -0.8930581  -0.9876987  -1.1114258  -1.1190462\n",
            " -0.9243423  -1.0926625  -1.0435568  -0.99378496 -1.0226154  -1.0884097\n",
            " -1.0864675  -1.1044648  -1.1028911  -0.8477092  -1.0266116  -1.0917691\n",
            " -0.9325335  -1.1308949  -1.0665462  -1.0944139  -1.1386695  -0.9444833\n",
            " -1.1010655  -1.0275521  -1.1462297  -0.9326871  -1.0801073  -0.97397304\n",
            " -0.98579055 -1.089358   -1.1409924  -0.96564883 -0.8731317  -0.87256956\n",
            " -1.0675431  -0.84931314 -0.8750623  -0.8914871  -0.8864266  -1.0223343\n",
            " -1.0377716  -1.1305583  -0.9636117  -1.0180861  -0.9179873  -1.139274\n",
            " -0.9911109  -1.1450427  -1.0289662  -1.0277598  -0.93588066 -0.88856155\n",
            " -1.0844857  -0.9417107  -0.8767884  -1.142914   -0.9241583  -0.90865356\n",
            " -1.105913   -0.93904966 -1.0399196  -1.1211594  -1.0370513  -0.93953305\n",
            " -0.9382426  -1.031633   -0.9813157  -1.0346494  -1.1342639  -0.94151884\n",
            " -1.009635   -0.8890248  -0.8934846  -0.9129421  -1.108033   -0.96906585\n",
            " -0.9417175  -1.0135531  -1.1035627  -0.8850636  -0.9735732  -1.0925882\n",
            " -1.0408742  -1.1000323  -0.9767792  -0.99844813 -1.0780718  -0.86908454\n",
            " -0.8868376  -1.0126041  -0.9486951  -0.9144131  -0.97706246 -0.98685586\n",
            " -1.0695807  -0.92875576 -0.9936445  -1.1114727  -1.0528288  -1.0013251\n",
            " -1.0413456  -1.0712492  -1.0411683  -0.9286816  -1.0075393  -1.1058661\n",
            " -1.0071499  -1.094009   -0.95733356 -1.0824306  -1.1007782  -1.0162129\n",
            " -0.9192594  -0.9145235  -1.1510727  -0.85139626 -0.919678   -1.1354693\n",
            " -0.8735096  -0.8551185 ], shape=(128,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg3iTQl8zuI2",
        "colab_type": "code",
        "outputId": "c261cc75-c8f5-457b-ccb2-d6b70a76321f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# No difference between loss functions of keras and tensorflow\n",
        "# tf.nn.sparse_softmax_cross_entropy_with_logits vs tf.keras.sparse_softmax_cross_entropy_with_logits\n",
        "\n",
        "for x, y in zip(onehot_encoded_batch_data.take(2), batched_label_data.take(2)):\n",
        "  non_zero_counts = tf.math.count_nonzero(input=y,axis = 1, dtype=tf.dtypes.float32)\n",
        "  mask = tf.sequence_mask(non_zero_counts, 448, dtype=tf.dtypes.float32 )\n",
        "  example_prediction = model(x)\n",
        "  #loss_tnsr_keras = loss_fnc(y, example_prediction)\n",
        "  #print(loss_tnsr_keras)\n",
        "  print(non_zero_counts)\n",
        "  print(\"====== ========\")\n",
        "  print()\n",
        "  loss_tnsr_flow = loss_function(y, example_prediction)\n",
        "  print(loss_tnsr_flow)\n",
        "  print()\n",
        "  print(\"========== summed_loss_per_sequence_of_batch without masking ==============\")\n",
        "  print()\n",
        "  summed_loss_per_sequence_of_batch_without_masking = tf.reduce_sum(loss_tnsr_flow, axis = 1)\n",
        "  print(summed_loss_per_sequence_of_batch_without_masking)\n",
        "  print()\n",
        "  print(\" ==== ====== ==\")\n",
        "  print(\" Masked loss\")\n",
        "  masked_loss = tf.multiply(loss_tnsr_flow, mask)\n",
        "  print(masked_loss)\n",
        "  print()\n",
        "  print(\"========== summed_loss_per_sequence_of_batch ==============\")\n",
        "  print()\n",
        "  summed_loss_per_sequence_of_batch = tf.reduce_sum(masked_loss, axis = 1)\n",
        "  print(summed_loss_per_sequence_of_batch)\n",
        "  print()\n",
        "  print(\"========= average_loss_per_sequence_of_batch ========= \")\n",
        "  average_loss_per_sequence_of_batch = tf.divide(summed_loss_per_sequence_of_batch, non_zero_counts)\n",
        "  print(average_loss_per_sequence_of_batch)\n",
        "  print()\n",
        "  \n",
        "  #print(mask)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 45.  58. 146.  58.  89. 119. 116. 148.  93. 130. 125. 180. 178.  55.\n",
            " 170. 106. 133.  80. 121.  56. 162. 201. 129.  55. 161. 175. 255. 109.\n",
            " 246. 204. 193. 128.  75. 134. 134. 145. 207.  86. 139. 100. 206. 112.\n",
            " 110.  76. 105. 134.  98.  98. 143. 109. 234. 154. 142. 102. 132. 119.\n",
            "  73. 186.  91. 148.  64. 133. 241. 142. 194.  73. 106. 134. 103. 135.\n",
            " 221. 146. 194. 262.  99. 167.  82.  90. 188. 111. 168. 110. 109. 115.\n",
            " 139. 108.  87. 167. 150. 110. 102. 116. 141.  74. 234. 181. 103. 152.\n",
            " 116. 103.  97.  97. 132. 178.  75. 162. 129. 116. 117. 130. 109.  85.\n",
            "  60.  98.  77.  49. 102.  75.  55. 106.  76.  62. 106.  89.  70.  90.\n",
            "  81.  62.], shape=(128,), dtype=float32)\n",
            "====== ========\n",
            "\n",
            "tf.Tensor(\n",
            "[[1.6106581e+01 3.3956265e+00 3.7805769e-01 ... 1.7343017e-03\n",
            "  1.7344207e-03 1.7345398e-03]\n",
            " [5.0245047e-01 2.6580746e+00 2.8654106e+00 ... 1.7312076e-03\n",
            "  1.7312076e-03 1.7312076e-03]\n",
            " [5.0337994e-01 2.0717266e+00 4.8572069e-01 ... 1.7175222e-03\n",
            "  1.7176411e-03 1.7177602e-03]\n",
            " ...\n",
            " [5.0717580e-01 2.0687680e+00 4.8645079e-01 ... 1.7679789e-03\n",
            "  1.7679789e-03 1.7679789e-03]\n",
            " [5.0156116e-01 2.0730476e+00 4.8564339e-01 ... 1.7260904e-03\n",
            "  1.7262094e-03 1.7262094e-03]\n",
            " [5.0535131e-01 2.0697093e+00 4.8640591e-01 ... 1.7607199e-03\n",
            "  1.7607199e-03 1.7608389e-03]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch without masking ==============\n",
            "\n",
            "tf.Tensor(\n",
            "[144.3438  109.84833 305.8499  112.54414 191.13544 244.71114 237.20364\n",
            " 312.61053 186.85072 265.09195 250.76758 400.17853 391.1314   92.18201\n",
            " 347.83914 218.5642  294.63245 163.36728 248.02396  97.14761 352.9077\n",
            " 442.20398 279.22043  98.67483 333.03064 355.90186 520.9502  242.12251\n",
            " 507.43835 440.59808 398.18094 263.37433 132.02164 272.46405 290.78595\n",
            " 274.30707 429.77808 166.56592 280.6752  213.30586 424.06787 238.49411\n",
            " 242.69463 163.4205  223.75711 300.46814 202.23167 214.07663 292.27362\n",
            " 228.67885 482.47888 325.3456  332.42908 205.53313 302.33533 234.01398\n",
            " 136.80124 397.24738 178.61136 302.0932  134.25995 283.07602 484.37735\n",
            " 285.74353 380.85278 152.69565 233.6543  282.2918  193.5636  271.00653\n",
            " 477.42618 300.1603  425.3067  570.3406  203.67926 355.13217 186.76593\n",
            " 203.08247 400.52258 236.02446 367.83252 244.90392 255.66682 231.96674\n",
            " 318.70755 239.04555 195.59067 363.7477  331.3894  269.40305 224.18903\n",
            " 257.07175 319.22375 168.4657  503.31393 419.16614 207.24167 332.15436\n",
            " 320.7257  225.07849 220.76851 205.43416 327.95407 434.63232 180.42795\n",
            " 378.30676 274.14386 256.22964 241.16757 303.83408 266.69427 181.0961\n",
            " 135.56778 235.26985 163.04837 119.30323 251.4357  154.70964 138.03513\n",
            " 263.33636 159.31955 151.59521 263.4195  199.26637 172.69168 220.70209\n",
            " 177.93504 154.63905], shape=(128,), dtype=float32)\n",
            "\n",
            " ==== ====== ==\n",
            " Masked loss\n",
            "tf.Tensor(\n",
            "[[16.10658     3.3956265   0.3780577  ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.50245047  2.6580746   2.8654106  ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.50337994  2.0717266   0.4857207  ...  0.          0.\n",
            "   0.        ]\n",
            " ...\n",
            " [ 0.5071758   2.068768    0.4864508  ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.50156116  2.0730476   0.4856434  ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.5053513   2.0697093   0.4864059  ...  0.          0.\n",
            "   0.        ]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch ==============\n",
            "\n",
            "tf.Tensor(\n",
            "[142.88872  109.12874  305.28876  111.82982  190.47816  244.09435\n",
            " 236.58922  312.01834  186.1888   264.4729   250.17079  399.6424\n",
            " 390.62256   91.452484 346.5669   217.9177   294.04086  160.09726\n",
            " 247.41777   96.42119  352.37292  441.73492  278.62503   97.94812\n",
            " 332.46115  355.38885  520.57294  241.49258  507.04303  440.13388\n",
            " 397.6701   262.7721   131.33142  271.87775  290.1955   271.563\n",
            " 429.32492  165.90862  280.09595  212.66971  423.5902   237.87764\n",
            " 241.23087  162.73529  223.12485  299.89706  201.547    212.49612\n",
            " 291.6816   228.03992  482.0749   324.78305  331.25336  204.87698\n",
            " 301.7047   233.40561  136.11658  396.59177  177.78427  301.5021\n",
            " 132.34705  282.4709   483.9654   285.16516  380.3802   151.821\n",
            " 233.02518  281.6286   192.92053  270.4174   476.13867  299.59082\n",
            " 424.8203   568.9409   202.3797   354.59546  186.09381  202.423\n",
            " 398.91388  235.4008   367.30408  244.30627  255.05203  231.37589\n",
            " 317.5102   238.3988   194.75586  363.20636  330.8237   268.50217\n",
            " 223.55586  255.61298  318.64978  167.76715  502.874    418.65344\n",
            " 206.59357  331.57788  320.09543  224.4426   220.12238  204.7861\n",
            " 327.37473  434.10602  179.74913  377.77664  273.5799   254.49373\n",
            " 240.5653   302.05328  265.47943  180.42325  134.022    234.0262\n",
            " 162.36398  117.926186 250.19695  154.022    136.84363  262.1041\n",
            " 158.63492  150.06274  262.19028  198.60175  170.80469  219.44833\n",
            " 177.25706  153.45181 ], shape=(128,), dtype=float32)\n",
            "\n",
            "========= average_loss_per_sequence_of_batch ========= \n",
            "tf.Tensor(\n",
            "[3.175305  1.8815299 2.091019  1.9281003 2.1402042 2.051213  2.0395622\n",
            " 2.108232  2.0020301 2.034407  2.0013664 2.2202356 2.1945088 1.6627724\n",
            " 2.0386288 2.0558274 2.2108335 2.0012157 2.044775  1.7218069 2.1751416\n",
            " 2.1976862 2.159884  1.780875  2.064976  2.0307934 2.0414624 2.2155282\n",
            " 2.0611506 2.157519  2.0604668 2.052907  1.7510856 2.0289383 2.165638\n",
            " 1.8728482 2.0740335 1.9291699 2.0150788 2.126697  2.0562632 2.1239076\n",
            " 2.193008  2.1412537 2.1249986 2.2380378 2.056602  2.1683278 2.0397315\n",
            " 2.0921094 2.0601492 2.108981  2.33277   2.0085979 2.2856417 1.9613917\n",
            " 1.8646107 2.1322138 1.9536734 2.0371764 2.0679226 2.1238413 2.008155\n",
            " 2.0082054 1.9607226 2.0797398 2.1983507 2.101706  1.8730149 2.0030918\n",
            " 2.1544735 2.051992  2.1897955 2.1715302 2.0442393 2.123326  2.2694368\n",
            " 2.2491446 2.1218824 2.120728  2.186334  2.220966  2.339927  2.0119643\n",
            " 2.284246  2.2073963 2.238573  2.1748884 2.2054913 2.4409287 2.191724\n",
            " 2.20356   2.2599275 2.2671237 2.1490343 2.3130023 2.0057628 2.1814334\n",
            " 2.7594433 2.1790543 2.2693028 2.111197  2.4801116 2.438798  2.396655\n",
            " 2.3319545 2.1207743 2.1939116 2.0561137 2.3234868 2.4355912 2.1226265\n",
            " 2.2337    2.3880224 2.1086233 2.4066567 2.4529111 2.0536268 2.488066\n",
            " 2.47268   2.0873015 2.4203668 2.473493  2.2314804 2.440067  2.438315\n",
            " 2.1883588 2.4750292], shape=(128,), dtype=float32)\n",
            "\n",
            "tf.Tensor(\n",
            "[105.  69.  64.  77. 111.  87.  69. 160. 105.  87.  79. 111. 120. 144.\n",
            " 219. 149.  96. 203.  49. 116.  54.  82. 115. 168. 120. 177. 201. 190.\n",
            " 146. 153. 175. 137.  70. 135. 150. 115. 186.  64.  83. 129. 116. 105.\n",
            "  91. 204.  64. 160. 205. 101. 116. 131. 118. 128.  81. 164.  88. 268.\n",
            "  70. 169. 114. 131. 111. 151. 106. 110. 104. 231.  87. 160. 108. 252.\n",
            "  94.  35.  89. 255.  81. 140. 142. 262. 129. 110. 224. 109.  82. 192.\n",
            " 100. 101.  61.  81. 184. 178. 153. 101. 107. 170. 183. 136. 119.  81.\n",
            "  65.  84. 102. 215.  82.  87.  82. 104.  65.  76. 124.  99.  63.  70.\n",
            " 135.  65. 125.  68. 115. 104.  93.  66.  67.  77.  51.  56.  50. 123.\n",
            " 169. 115.], shape=(128,), dtype=float32)\n",
            "====== ========\n",
            "\n",
            "tf.Tensor(\n",
            "[[6.9503603e+00 2.5604696e+00 7.1381122e-01 ... 1.7686929e-03\n",
            "  1.7686929e-03 1.7688118e-03]\n",
            " [5.2111208e-01 2.0728171e+00 4.8876262e-01 ... 1.7569120e-03\n",
            "  1.7570310e-03 1.7570310e-03]\n",
            " [5.2456540e-01 2.0678778e+00 4.8972479e-01 ... 1.7388238e-03\n",
            "  1.7389428e-03 1.7391808e-03]\n",
            " ...\n",
            " [5.0648284e-01 2.0602951e+00 4.9050686e-01 ... 1.7260904e-03\n",
            "  1.7262094e-03 1.7262094e-03]\n",
            " [5.2256995e-01 2.0713832e+00 4.8915249e-01 ... 1.7201402e-03\n",
            "  1.7202593e-03 1.7202593e-03]\n",
            " [5.0903469e-01 2.5329921e+00 1.8804719e-01 ... 1.7350157e-03\n",
            "  1.7350157e-03 1.7350157e-03]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch without masking ==============\n",
            "\n",
            "tf.Tensor(\n",
            "[262.92923  159.93253  139.54474  182.23105  275.06204  197.95892\n",
            " 165.33598  344.42422  251.11798  180.76608  194.70363  242.01773\n",
            " 229.73877  328.89783  450.9696   324.3133   198.0153   401.35925\n",
            "  99.81267  282.03583  122.28761  181.90857  262.18915  366.85352\n",
            " 268.04587  393.27832  426.93     422.84888  290.41595  313.2079\n",
            " 380.48825  244.12306  162.63052  290.2128   322.53156  246.09314\n",
            " 414.70227  136.91467  192.85294  263.6696   250.272    230.28601\n",
            " 183.9938   444.3024   132.69421  346.05298  435.6236   219.35367\n",
            " 229.76202  275.00824  241.5788   263.50842  186.22818  340.63116\n",
            " 187.0322   549.68115  154.84323  350.28497  245.82526  286.21378\n",
            " 224.55725  313.93323  228.96198  240.93494  230.0962   455.78973\n",
            " 184.71976  361.22687  233.64674  535.03784  184.73228   85.113205\n",
            " 171.27655  539.1879   174.30008  312.65344  318.5306   596.4455\n",
            " 260.46234  242.70227  445.7843   237.49721  163.76904  410.92273\n",
            " 223.4884   234.00482  133.31833  192.26212  364.99014  399.13416\n",
            " 340.6795   213.12047  224.77391  388.3661   395.0304   287.26144\n",
            " 241.98038  161.4699   160.61818  164.39001  215.62569  476.5626\n",
            " 184.31363  195.67564  171.85504  226.19327  146.53076  166.39847\n",
            " 265.76282  230.85269  151.62161  170.26114  303.9284   147.98567\n",
            " 315.8495   163.92398  272.44977  232.44525  233.5753   167.40134\n",
            " 181.75323  197.21608  130.72766  120.5804   103.77939  270.83752\n",
            " 413.9074   245.26584 ], shape=(128,), dtype=float32)\n",
            "\n",
            " ==== ====== ==\n",
            " Masked loss\n",
            "tf.Tensor(\n",
            "[[6.9503603  2.5604696  0.7138112  ... 0.         0.         0.        ]\n",
            " [0.5211121  2.072817   0.48876262 ... 0.         0.         0.        ]\n",
            " [0.5245654  2.0678778  0.48972479 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.50648284 2.060295   0.49050686 ... 0.         0.         0.        ]\n",
            " [0.52256995 2.0713832  0.4891525  ... 0.         0.         0.        ]\n",
            " [0.5090347  2.5329921  0.18804719 ... 0.         0.         0.        ]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch ==============\n",
            "\n",
            "tf.Tensor(\n",
            "[261.6977   158.58176  138.82489  181.52362  273.8416   197.29114\n",
            " 163.56798  343.8968   249.88895  180.09927  194.02621  238.9957\n",
            " 229.13028  328.32373  450.5396   323.7627   197.36551  400.8944\n",
            "  99.11576  281.41895  121.56706  181.22914  261.56897  366.32092\n",
            " 267.40787  392.7649   426.4231   422.34155  289.8414   312.6582\n",
            " 379.9702   243.54276  161.91357  289.64108  321.97632  245.47751\n",
            " 414.20868  136.20813  192.17645  263.08572  247.07655  229.63068\n",
            " 183.33292  443.84674  131.99374  344.08228  435.15002  218.7049\n",
            " 229.12196  274.4179   240.97118  262.9167   185.55493  338.98965\n",
            " 186.36517  549.30554  154.14621  349.2063   244.6498   285.62958\n",
            " 223.93326  313.37018  227.55505  240.30621  228.88515  455.34857\n",
            " 182.67429  360.68457  233.00098  534.6317   184.07672   82.07355\n",
            " 170.60587  538.8089   172.71072  312.05588  317.95392  596.07983\n",
            " 259.86783  242.07083  445.35883  236.87936  163.08348  410.4237\n",
            " 222.82811  233.36082  130.30132  190.52644  364.49057  398.62326\n",
            " 339.7083   212.47778  223.60199  387.8377   394.5284   286.6787\n",
            " 241.38177  160.79434  159.8148   163.70883  214.99237  476.12256\n",
            " 183.62695  195.01312  171.18924  225.56151  145.82718  165.708\n",
            " 265.16992  230.20059  150.91766  169.57     303.35355  147.28836\n",
            " 315.24615  163.22684  271.8608   231.81403  229.99571  166.6957\n",
            " 180.54074  196.54294  126.636444 117.68387  100.84584  270.23904\n",
            " 413.38287  244.65746 ], shape=(128,), dtype=float32)\n",
            "\n",
            "========= average_loss_per_sequence_of_batch ========= \n",
            "tf.Tensor(\n",
            "[2.492359  2.2982864 2.169139  2.3574495 2.4670415 2.2677143 2.3705504\n",
            " 2.149355  2.3798947 2.0701065 2.456028  2.1531143 1.909419  2.280026\n",
            " 2.0572586 2.172904  2.0558908 1.9748493 2.0227706 2.4260254 2.251242\n",
            " 2.2101114 2.2745128 2.1804817 2.2283988 2.2190106 2.121508  2.2228503\n",
            " 1.9852151 2.0435176 2.1712584 1.7776843 2.313051  2.1454895 2.1465087\n",
            " 2.134587  2.2269285 2.128252  2.315379  2.0394242 2.1299703 2.1869588\n",
            " 2.0146475 2.1757193 2.0624022 2.1505141 2.122683  2.165395  1.9751893\n",
            " 2.094793  2.0421286 2.0540366 2.2908015 2.0670102 2.1177862 2.0496476\n",
            " 2.2020886 2.0663095 2.146051  2.1803784 2.0174167 2.0752993 2.1467457\n",
            " 2.184602  2.2008188 1.971206  2.0997045 2.2542787 2.1574163 2.1215544\n",
            " 1.958263  2.3449585 1.9169198 2.112976  2.1322312 2.2289705 2.2391121\n",
            " 2.2751138 2.0144794 2.200644  1.988209  2.1732051 1.9888229 2.1376235\n",
            " 2.228281  2.3105032 2.1360872 2.3521783 1.980927  2.2394564 2.2203157\n",
            " 2.1037405 2.0897381 2.2813983 2.1558928 2.1079316 2.0284183 1.9851153\n",
            " 2.4586892 1.9489146 2.1077683 2.2145236 2.2393532 2.2415302 2.0876737\n",
            " 2.1688607 2.243495  2.1803684 2.138467  2.3252585 2.3955185 2.4224286\n",
            " 2.2470634 2.2659748 2.5219693 2.4003947 2.364007  2.228981  2.4730723\n",
            " 2.5256922 2.694638  2.5525057 2.4830675 2.1014977 2.0169168 2.1970654\n",
            " 2.4460526 2.1274562], shape=(128,), dtype=float32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COQ2-w5sXS8A",
        "colab_type": "text"
      },
      "source": [
        "**Loss function - 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdxuqjlwX12r",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1DMUmJpvKnAqho3uDWSgUDeAMH7x08lbo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Lp6OqOzuQ0",
        "colab_type": "code",
        "outputId": "f175810e-9bbd-47a0-d8d1-f600826c5c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for x, y in zip(onehot_encoded_batch_data.take(2), batched_label_data.take(2)):\n",
        "  non_zero_counts = tf.math.count_nonzero(input=y,axis = 1, dtype=tf.dtypes.float32)\n",
        "  mask = tf.sequence_mask(non_zero_counts, 448, dtype=tf.dtypes.float32 )\n",
        "  example_prediction = model(x)\n",
        "  #loss_tnsr_keras = loss_fnc(y, example_prediction)\n",
        "  #print(loss_tnsr_keras)\n",
        "  print(non_zero_counts)\n",
        "  sum_non_zero_counts_per_batch = tf.reduce_sum(non_zero_counts, axis = None)\n",
        "  print(\"====== ========\")\n",
        "  print()\n",
        "  loss_tnsr_flow = loss_function(y, example_prediction)\n",
        "  print(loss_tnsr_flow)\n",
        "  print()\n",
        "  print(\"========== summed_loss_per_sequence_of_batch without masking ==============\")\n",
        "  print()\n",
        "  summed_loss_per_sequence_of_batch_without_masking = tf.reduce_sum(loss_tnsr_flow, axis = 1)\n",
        "  print(summed_loss_per_sequence_of_batch_without_masking)\n",
        "  print()\n",
        "  print(\" ==== ====== ==\")\n",
        "  print(\" Masked loss\")\n",
        "  masked_loss = tf.multiply(loss_tnsr_flow, mask)\n",
        "  print(masked_loss)\n",
        "  print()\n",
        "  print(\"========== summed_loss_per_sequence_of_batch ==============\")\n",
        "  print()\n",
        "  summed_loss_per_sequence_of_batch = tf.reduce_sum(masked_loss, axis = None)\n",
        "  print(summed_loss_per_sequence_of_batch)\n",
        "  print()\n",
        "  print(\"========= average_loss_per_sequence_of_batch ========= \")\n",
        "  average_loss_per_sequence_of_batch = tf.divide(summed_loss_per_sequence_of_batch, sum_non_zero_counts_per_batch)\n",
        "  print(average_loss_per_sequence_of_batch)\n",
        "  print()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 45.  58. 146.  58.  89. 119. 116. 148.  93. 130. 125. 180. 178.  55.\n",
            " 170. 106. 133.  80. 121.  56. 162. 201. 129.  55. 161. 175. 255. 109.\n",
            " 246. 204. 193. 128.  75. 134. 134. 145. 207.  86. 139. 100. 206. 112.\n",
            " 110.  76. 105. 134.  98.  98. 143. 109. 234. 154. 142. 102. 132. 119.\n",
            "  73. 186.  91. 148.  64. 133. 241. 142. 194.  73. 106. 134. 103. 135.\n",
            " 221. 146. 194. 262.  99. 167.  82.  90. 188. 111. 168. 110. 109. 115.\n",
            " 139. 108.  87. 167. 150. 110. 102. 116. 141.  74. 234. 181. 103. 152.\n",
            " 116. 103.  97.  97. 132. 178.  75. 162. 129. 116. 117. 130. 109.  85.\n",
            "  60.  98.  77.  49. 102.  75.  55. 106.  76.  62. 106.  89.  70.  90.\n",
            "  81.  62.], shape=(128,), dtype=float32)\n",
            "====== ========\n",
            "\n",
            "tf.Tensor(\n",
            "[[1.6111551e+01 3.3855002e+00 3.6975500e-01 ... 1.7357297e-03\n",
            "  1.7358487e-03 1.7358487e-03]\n",
            " [5.0986779e-01 2.6823816e+00 2.8770697e+00 ... 1.7297795e-03\n",
            "  1.7300176e-03 1.7300176e-03]\n",
            " [5.1856112e-01 2.0734143e+00 4.8905951e-01 ... 1.7176411e-03\n",
            "  1.7176411e-03 1.7177602e-03]\n",
            " ...\n",
            " [5.2241099e-01 2.0716062e+00 4.8932359e-01 ... 1.7682168e-03\n",
            "  1.7683359e-03 1.7683359e-03]\n",
            " [5.2418792e-01 2.0710292e+00 4.8991236e-01 ... 1.7253765e-03\n",
            "  1.7254954e-03 1.7256144e-03]\n",
            " [5.1966238e-01 2.0734785e+00 4.8902479e-01 ... 1.7617909e-03\n",
            "  1.7617909e-03 1.7619099e-03]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch without masking ==============\n",
            "\n",
            "tf.Tensor(\n",
            "[144.43283 109.86379 305.86102 112.58009 191.18274 244.72653 237.22849\n",
            " 312.65564 186.90108 265.1079  250.77191 400.19934 391.1468   92.21413\n",
            " 347.86816 218.60368 294.65457 163.37012 248.13194  97.18147 352.9307\n",
            " 442.2228  279.24417  98.70526 333.04248 355.92178 520.96545 242.14471\n",
            " 507.4762  440.64175 398.21216 263.37512 132.05273 272.5187  290.815\n",
            " 274.35403 429.8166  166.62088 280.6931  213.35065 424.08987 238.5205\n",
            " 242.70866 163.44716 223.76889 300.49542 202.23767 214.09995 292.3686\n",
            " 228.68404 482.4981  325.35632 332.44788 205.52441 302.35034 234.06387\n",
            " 136.82182 397.2679  178.62111 302.123   134.31282 283.08478 484.40204\n",
            " 285.77045 380.86768 152.69977 233.6436  282.30112 193.58493 271.0127\n",
            " 477.4615  300.2547  425.33725 570.3589  203.66956 355.1625  186.75589\n",
            " 203.08301 400.56427 236.05435 367.87225 244.90479 255.72871 232.01018\n",
            " 318.69272 239.06023 195.6007  363.7804  331.44336 269.42264 224.20691\n",
            " 257.0658  319.23492 168.49532 503.3408  419.18835 207.25597 332.18008\n",
            " 320.73074 225.09737 220.801   205.47955 327.9955  434.66504 180.46771\n",
            " 378.31195 274.1727  256.25705 241.1973  303.83197 266.71954 181.11435\n",
            " 135.60449 235.23645 163.10696 119.30652 251.39822 154.72585 138.06854\n",
            " 263.3659  159.3265  151.60231 263.44238 199.26923 172.68866 220.72534\n",
            " 177.9619  154.68532], shape=(128,), dtype=float32)\n",
            "\n",
            " ==== ====== ==\n",
            " Masked loss\n",
            "tf.Tensor(\n",
            "[[16.111551    3.3855002   0.369755   ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.5098678   2.6823816   2.8770697  ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.5185611   2.0734143   0.4890595  ...  0.          0.\n",
            "   0.        ]\n",
            " ...\n",
            " [ 0.522411    2.0716062   0.4893236  ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.5241879   2.0710292   0.48991236 ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.5196624   2.0734785   0.4890248  ...  0.          0.\n",
            "   0.        ]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch ==============\n",
            "\n",
            "tf.Tensor(34320.9, shape=(), dtype=float32)\n",
            "\n",
            "========= average_loss_per_sequence_of_batch ========= \n",
            "tf.Tensor(2.148951, shape=(), dtype=float32)\n",
            "\n",
            "tf.Tensor(\n",
            "[105.  69.  64.  77. 111.  87.  69. 160. 105.  87.  79. 111. 120. 144.\n",
            " 219. 149.  96. 203.  49. 116.  54.  82. 115. 168. 120. 177. 201. 190.\n",
            " 146. 153. 175. 137.  70. 135. 150. 115. 186.  64.  83. 129. 116. 105.\n",
            "  91. 204.  64. 160. 205. 101. 116. 131. 118. 128.  81. 164.  88. 268.\n",
            "  70. 169. 114. 131. 111. 151. 106. 110. 104. 231.  87. 160. 108. 252.\n",
            "  94.  35.  89. 255.  81. 140. 142. 262. 129. 110. 224. 109.  82. 192.\n",
            " 100. 101.  61.  81. 184. 178. 153. 101. 107. 170. 183. 136. 119.  81.\n",
            "  65.  84. 102. 215.  82.  87.  82. 104.  65.  76. 124.  99.  63.  70.\n",
            " 135.  65. 125.  68. 115. 104.  93.  66.  67.  77.  51.  56.  50. 123.\n",
            " 169. 115.], shape=(128,), dtype=float32)\n",
            "====== ========\n",
            "\n",
            "tf.Tensor(\n",
            "[[6.9498739e+00 2.5608447e+00 7.1325421e-01 ... 1.7688118e-03\n",
            "  1.7688118e-03 1.7688118e-03]\n",
            " [5.2156329e-01 2.0724180e+00 4.8878294e-01 ... 1.7567930e-03\n",
            "  1.7569120e-03 1.7569120e-03]\n",
            " [5.2456594e-01 2.0678682e+00 4.8972690e-01 ... 1.7388238e-03\n",
            "  1.7388238e-03 1.7391808e-03]\n",
            " ...\n",
            " [5.0638980e-01 2.0603294e+00 4.9051934e-01 ... 1.7260904e-03\n",
            "  1.7262094e-03 1.7262094e-03]\n",
            " [5.2279574e-01 2.0710020e+00 4.8920286e-01 ... 1.7200212e-03\n",
            "  1.7202593e-03 1.7202593e-03]\n",
            " [5.0868666e-01 2.5326836e+00 1.8813568e-01 ... 1.7350157e-03\n",
            "  1.7350157e-03 1.7351347e-03]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch without masking ==============\n",
            "\n",
            "tf.Tensor(\n",
            "[262.93182 159.93039 139.54483 182.23767 275.06348 197.959   165.33643\n",
            " 344.42377 251.11832 180.76605 194.70349 242.01776 229.73878 328.89917\n",
            " 450.96967 324.31372 198.01532 401.35797  99.81204 282.03497 122.28761\n",
            " 181.90848 262.18915 366.85535 268.04593 393.27826 426.93    422.84888\n",
            " 290.41595 313.2079  380.48828 244.1235  162.63284 290.21283 322.5316\n",
            " 246.0932  414.70227 136.91193 192.85294 263.66974 250.27197 230.28625\n",
            " 183.99419 444.3015  132.69421 346.0529  435.62372 219.35373 229.76253\n",
            " 275.0083  241.5788  263.50842 186.22815 340.63104 187.03221 549.6809\n",
            " 154.84283 350.28497 245.82428 286.21378 224.5586  313.93326 228.96198\n",
            " 240.93506 230.09622 455.79028 184.71968 361.22702 233.64673 535.03796\n",
            " 184.7323   85.11328 171.27658 539.1879  174.30081 312.65332 318.53052\n",
            " 596.44556 260.46234 242.70226 445.78433 237.49724 163.76831 410.92267\n",
            " 223.48837 234.0049  133.31839 192.26227 364.99    399.13416 340.6807\n",
            " 213.12024 224.77383 388.36646 395.0304  287.2614  241.98041 161.46982\n",
            " 160.61801 164.38995 215.62602 476.5633  184.31364 195.67564 171.85056\n",
            " 226.19324 146.53084 166.39853 265.76294 230.85309 151.62213 170.26163\n",
            " 303.92822 147.98593 315.84967 163.92133 272.44647 232.44305 233.57706\n",
            " 167.40146 181.7537  197.21838 130.72858 120.58056 103.77577 270.83704\n",
            " 413.9078  245.2627 ], shape=(128,), dtype=float32)\n",
            "\n",
            " ==== ====== ==\n",
            " Masked loss\n",
            "tf.Tensor(\n",
            "[[6.949874   2.5608447  0.7132542  ... 0.         0.         0.        ]\n",
            " [0.5215633  2.072418   0.48878294 ... 0.         0.         0.        ]\n",
            " [0.52456594 2.0678682  0.4897269  ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.5063898  2.0603294  0.49051934 ... 0.         0.         0.        ]\n",
            " [0.52279574 2.071002   0.48920286 ... 0.         0.         0.        ]\n",
            " [0.50868666 2.5326836  0.18813568 ... 0.         0.         0.        ]], shape=(128, 448), dtype=float32)\n",
            "\n",
            "========== summed_loss_per_sequence_of_batch ==============\n",
            "\n",
            "tf.Tensor(33435.215, shape=(), dtype=float32)\n",
            "\n",
            "========= average_loss_per_sequence_of_batch ========= \n",
            "tf.Tensor(2.169709, shape=(), dtype=float32)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVnRisPTbYMl",
        "colab_type": "text"
      },
      "source": [
        "# **Complete RNN-GRU based model for the assignment**\n",
        "\n",
        "**summary:** Both models were run for 20 epochs one after the other. Model took batch size of 128, max time step size of 448, onehot size of 78\n",
        "\n",
        "**Model with custom_loss_fnc_1** : Loss = 1.05, time per epoch = 28 sec, hidden units = 450\n",
        "\n",
        "**Model with custom_loss_fnc_2** : Loss = 1.43, time per epoch = 28 sec, hidden units = 450\n",
        "\n",
        "**Model with custom_loss_fnc_2** : epochs = 33, Loss = 1.43, time per epoch = 25 sec, hidden units = 512, learning rate = 0.0012"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJFy3VwQbi3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_Xy4-pJ3qp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_training_model_512 = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=512,\n",
        "  batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f2MSV6B3qvM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cb27476-b87d-4149-cdf1-8cbe2b72966d"
      },
      "source": [
        "example_input_tensor = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, 448, vocab_size]))\n",
        "example_prediction = gru_training_model_512(example_input_tensor)\n",
        "#print(example_prediction)\n",
        "example_prediction.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 448, 78])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU89DCsU3q7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "44ccb87f-2b5e-4490-be96-2495b84cc535"
      },
      "source": [
        "gru_training_model_512.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    multiple                  909312    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 949,326\n",
            "Trainable params: 949,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJyY17yM3q5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir3 = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix3 = os.path.join(checkpoint_dir3, \"ckpt_512act_{epoch}\")\n",
        "\n",
        "checkpoint_callback3=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix3,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9sPtqUX4FKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_masked_loss2(one_batch_data, max_len, logits, labels):\n",
        "\n",
        "    non_zero_counts = tf.math.count_nonzero(input=one_batch_data, axis = 1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    #Do -1 as the last element of each sequence isn’t used as input.\n",
        "    non_zero_counts = non_zero_counts - 1\n",
        "\n",
        "    #Creating the mask\n",
        "    mask = tf.sequence_mask(non_zero_counts, max_len, dtype=tf.dtypes.float32 )\n",
        "\n",
        "    #Calculate loss for each element of the batch and timestep\n",
        "    loss_tnsr = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "\n",
        "    #Create a mask based on non-zero sequence\n",
        "    masked_loss = tf.multiply(loss_tnsr, mask)\n",
        "\n",
        "    summed_loss_per_sequence_of_batch = tf.reduce_sum(masked_loss, axis = None)\n",
        "    summed_non_zero_counts_of_batch = tf.reduce_sum(non_zero_counts, axis = None)\n",
        "    average_masked_loss_per_batch = tf.divide(summed_loss_per_sequence_of_batch, summed_non_zero_counts_of_batch)\n",
        "\n",
        "    return average_masked_loss_per_batch;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unpBxn584FRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target, one_batch_data, max_len):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = gru_training_model_512(inp)\n",
        "    loss = custom_masked_loss2(one_batch_data, max_len, logits = predictions, labels=target)\n",
        "  grads = tape.gradient(loss, gru_training_model_512.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, gru_training_model_512.trainable_variables))\n",
        "\n",
        "  return loss;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4vr0DE94FZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0012)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jiv_Tq23q3I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15d692df-a30b-4699-84c3-32d6d5ea0435"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 33\n",
        "max_len = 448\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = gru_training_model_512.reset_states()\n",
        "\n",
        "  for (batch_n, (onehot_inp, categ_inp, categ_target)) in enumerate(zip(onehot_encoded_batch_data, batched_categorical_data, batched_label_data)):\n",
        "    loss = train_step(inp = onehot_inp, target = categ_target, one_batch_data = categ_inp, max_len = max_len)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    gru_training_model_512.save_weights(checkpoint_prefix3.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "gru_training_model_512.save_weights(checkpoint_prefix3.format(epoch=epoch))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.357568740844727\n",
            "Epoch 1 Batch 100 Loss 2.378333330154419\n",
            "Epoch 1 Batch 200 Loss 2.0631933212280273\n",
            "Epoch 1 Loss 1.9581\n",
            "Time taken for 1 epoch 25.494893312454224 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.03416109085083\n",
            "Epoch 2 Batch 100 Loss 1.9421584606170654\n",
            "Epoch 2 Batch 200 Loss 1.8093279600143433\n",
            "Epoch 2 Loss 1.7557\n",
            "Time taken for 1 epoch 24.220762968063354 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7994821071624756\n",
            "Epoch 3 Batch 100 Loss 1.7473204135894775\n",
            "Epoch 3 Batch 200 Loss 1.5871973037719727\n",
            "Epoch 3 Loss 1.5589\n",
            "Time taken for 1 epoch 24.2340407371521 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6117326021194458\n",
            "Epoch 4 Batch 100 Loss 1.5695502758026123\n",
            "Epoch 4 Batch 200 Loss 1.4285595417022705\n",
            "Epoch 4 Loss 1.4173\n",
            "Time taken for 1 epoch 24.21273708343506 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4783350229263306\n",
            "Epoch 5 Batch 100 Loss 1.445587158203125\n",
            "Epoch 5 Batch 200 Loss 1.320628046989441\n",
            "Epoch 5 Loss 1.3097\n",
            "Time taken for 1 epoch 24.18663001060486 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3788082599639893\n",
            "Epoch 6 Batch 100 Loss 1.3532609939575195\n",
            "Epoch 6 Batch 200 Loss 1.2502127885818481\n",
            "Epoch 6 Loss 1.2346\n",
            "Time taken for 1 epoch 24.223658084869385 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3240338563919067\n",
            "Epoch 7 Batch 100 Loss 1.2884935140609741\n",
            "Epoch 7 Batch 200 Loss 1.2106616497039795\n",
            "Epoch 7 Loss 1.1842\n",
            "Time taken for 1 epoch 24.19918656349182 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2677119970321655\n",
            "Epoch 8 Batch 100 Loss 1.234553337097168\n",
            "Epoch 8 Batch 200 Loss 1.1552670001983643\n",
            "Epoch 8 Loss 1.1429\n",
            "Time taken for 1 epoch 24.215816974639893 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2219632863998413\n",
            "Epoch 9 Batch 100 Loss 1.1971567869186401\n",
            "Epoch 9 Batch 200 Loss 1.124967336654663\n",
            "Epoch 9 Loss 1.1111\n",
            "Time taken for 1 epoch 24.17533540725708 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1900407075881958\n",
            "Epoch 10 Batch 100 Loss 1.1549818515777588\n",
            "Epoch 10 Batch 200 Loss 1.0973739624023438\n",
            "Epoch 10 Loss 1.0858\n",
            "Time taken for 1 epoch 24.20395517349243 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.1654245853424072\n",
            "Epoch 11 Batch 100 Loss 1.1253635883331299\n",
            "Epoch 11 Batch 200 Loss 1.074331521987915\n",
            "Epoch 11 Loss 1.0653\n",
            "Time taken for 1 epoch 24.222594738006592 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.1455057859420776\n",
            "Epoch 12 Batch 100 Loss 1.1034531593322754\n",
            "Epoch 12 Batch 200 Loss 1.0529075860977173\n",
            "Epoch 12 Loss 1.0486\n",
            "Time taken for 1 epoch 24.22786784172058 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.119397521018982\n",
            "Epoch 13 Batch 100 Loss 1.0869001150131226\n",
            "Epoch 13 Batch 200 Loss 1.034777045249939\n",
            "Epoch 13 Loss 1.0354\n",
            "Time taken for 1 epoch 24.277276039123535 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.1012756824493408\n",
            "Epoch 14 Batch 100 Loss 1.0648272037506104\n",
            "Epoch 14 Batch 200 Loss 1.0187190771102905\n",
            "Epoch 14 Loss 1.0194\n",
            "Time taken for 1 epoch 24.219336986541748 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.084820032119751\n",
            "Epoch 15 Batch 100 Loss 1.044390082359314\n",
            "Epoch 15 Batch 200 Loss 1.0036911964416504\n",
            "Epoch 15 Loss 1.0104\n",
            "Time taken for 1 epoch 24.242456674575806 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.0693141222000122\n",
            "Epoch 16 Batch 100 Loss 1.0340216159820557\n",
            "Epoch 16 Batch 200 Loss 0.9884423017501831\n",
            "Epoch 16 Loss 1.0001\n",
            "Time taken for 1 epoch 24.229968786239624 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.0522230863571167\n",
            "Epoch 17 Batch 100 Loss 1.0184730291366577\n",
            "Epoch 17 Batch 200 Loss 0.9751741290092468\n",
            "Epoch 17 Loss 0.9924\n",
            "Time taken for 1 epoch 24.17243766784668 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.036780834197998\n",
            "Epoch 18 Batch 100 Loss 1.010076880455017\n",
            "Epoch 18 Batch 200 Loss 0.9618951082229614\n",
            "Epoch 18 Loss 0.9803\n",
            "Time taken for 1 epoch 24.222934007644653 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.0216662883758545\n",
            "Epoch 19 Batch 100 Loss 0.9971919655799866\n",
            "Epoch 19 Batch 200 Loss 0.9501793384552002\n",
            "Epoch 19 Loss 0.9720\n",
            "Time taken for 1 epoch 24.246959686279297 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0060298442840576\n",
            "Epoch 20 Batch 100 Loss 0.9882599115371704\n",
            "Epoch 20 Batch 200 Loss 0.9391727447509766\n",
            "Epoch 20 Loss 0.9659\n",
            "Time taken for 1 epoch 24.222196102142334 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.000444769859314\n",
            "Epoch 21 Batch 100 Loss 0.977132260799408\n",
            "Epoch 21 Batch 200 Loss 0.9262720942497253\n",
            "Epoch 21 Loss 0.9577\n",
            "Time taken for 1 epoch 24.239203214645386 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.9896541237831116\n",
            "Epoch 22 Batch 100 Loss 0.9678628444671631\n",
            "Epoch 22 Batch 200 Loss 0.9154757857322693\n",
            "Epoch 22 Loss 0.9515\n",
            "Time taken for 1 epoch 24.261539220809937 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.9722464680671692\n",
            "Epoch 23 Batch 100 Loss 0.9597634673118591\n",
            "Epoch 23 Batch 200 Loss 0.9093746542930603\n",
            "Epoch 23 Loss 0.9470\n",
            "Time taken for 1 epoch 24.220786094665527 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.9621958136558533\n",
            "Epoch 24 Batch 100 Loss 0.9558173418045044\n",
            "Epoch 24 Batch 200 Loss 0.8988587856292725\n",
            "Epoch 24 Loss 0.9479\n",
            "Time taken for 1 epoch 24.276538848876953 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.9518331289291382\n",
            "Epoch 25 Batch 100 Loss 0.9474956393241882\n",
            "Epoch 25 Batch 200 Loss 0.888966977596283\n",
            "Epoch 25 Loss 0.9402\n",
            "Time taken for 1 epoch 24.23515510559082 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.9493638873100281\n",
            "Epoch 26 Batch 100 Loss 0.942984938621521\n",
            "Epoch 26 Batch 200 Loss 0.8859096765518188\n",
            "Epoch 26 Loss 0.9413\n",
            "Time taken for 1 epoch 24.195306539535522 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.9507715106010437\n",
            "Epoch 27 Batch 100 Loss 0.9436439871788025\n",
            "Epoch 27 Batch 200 Loss 0.8814640045166016\n",
            "Epoch 27 Loss 0.9329\n",
            "Time taken for 1 epoch 24.279151916503906 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.9355942010879517\n",
            "Epoch 28 Batch 100 Loss 0.9442564249038696\n",
            "Epoch 28 Batch 200 Loss 0.8706061244010925\n",
            "Epoch 28 Loss 0.9339\n",
            "Time taken for 1 epoch 24.22495746612549 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.925282895565033\n",
            "Epoch 29 Batch 100 Loss 0.9471185803413391\n",
            "Epoch 29 Batch 200 Loss 0.8703040480613708\n",
            "Epoch 29 Loss 0.9287\n",
            "Time taken for 1 epoch 24.258601427078247 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.9322119355201721\n",
            "Epoch 30 Batch 100 Loss 0.947991132736206\n",
            "Epoch 30 Batch 200 Loss 0.8652757406234741\n",
            "Epoch 30 Loss 0.9284\n",
            "Time taken for 1 epoch 24.241424560546875 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.9220128059387207\n",
            "Epoch 31 Batch 100 Loss 0.9497614502906799\n",
            "Epoch 31 Batch 200 Loss 0.8578720688819885\n",
            "Epoch 31 Loss 0.9324\n",
            "Time taken for 1 epoch 24.246453046798706 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.9187986254692078\n",
            "Epoch 32 Batch 100 Loss 0.9474779367446899\n",
            "Epoch 32 Batch 200 Loss 0.8530184030532837\n",
            "Epoch 32 Loss 0.9326\n",
            "Time taken for 1 epoch 24.235254764556885 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.915989100933075\n",
            "Epoch 33 Batch 100 Loss 0.9430614709854126\n",
            "Epoch 33 Batch 200 Loss 0.8476676940917969\n",
            "Epoch 33 Loss 0.9316\n",
            "Time taken for 1 epoch 24.227739810943604 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFfZOQ-b5Ozy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c513efc2-d58d-411c-b672-6c4d1b607c7a"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_512act_32'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qebbl41E3q1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "4e0f0071-c908-4fa3-85bd-7f5b480334a4"
      },
      "source": [
        "!mkdir -p saved_model_3\n",
        "\n",
        "gru_training_model_512.save('saved_model_3/my_model1') \n",
        "\n",
        "!zip -r /content/saved_model_3.zip /content/saved_model_3\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: saved_model_3/my_model1/assets\n",
            "  adding: content/saved_model_3/ (stored 0%)\n",
            "  adding: content/saved_model_3/my_model1/ (stored 0%)\n",
            "  adding: content/saved_model_3/my_model1/variables/ (stored 0%)\n",
            "  adding: content/saved_model_3/my_model1/variables/variables.index (deflated 42%)\n",
            "  adding: content/saved_model_3/my_model1/variables/variables.data-00000-of-00002 (deflated 71%)\n",
            "  adding: content/saved_model_3/my_model1/variables/variables.data-00001-of-00002 (deflated 7%)\n",
            "  adding: content/saved_model_3/my_model1/saved_model.pb (deflated 90%)\n",
            "  adding: content/saved_model_3/my_model1/assets/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7e0Eur8bi99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_training_model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=448,\n",
        "  batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoJmBjZ2bi6z",
        "colab_type": "code",
        "outputId": "702916d3-3ae7-4af7-9265-6d155f1a675c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_tensor = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, 448, vocab_size]))\n",
        "example_prediction = gru_training_model(example_input_tensor)\n",
        "#print(example_prediction)\n",
        "example_prediction.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 448, 78])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc133e7XcFWq",
        "colab_type": "code",
        "outputId": "50ecedaa-315b-4a97-9584-5eeccc7273a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "gru_training_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_1 (GRU)                  multiple                  709632    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  35022     \n",
            "=================================================================\n",
            "Total params: 744,654\n",
            "Trainable params: 744,654\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIJt8SkBcJpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27TR0OrTcRIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_masked_loss1(one_batch_data, max_len, logits, labels):\n",
        "\n",
        "    non_zero_counts = tf.math.count_nonzero(input=one_batch_data, axis = 1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    #Do -1 as the last element of each sequence isn’t used as input.\n",
        "    non_zero_counts = non_zero_counts - 1\n",
        "\n",
        "    #Creating the mask\n",
        "    mask = tf.sequence_mask(non_zero_counts, max_len, dtype=tf.dtypes.float32 )\n",
        "\n",
        "    #Calculate loss for each element of the batch and timestep\n",
        "    loss_tnsr = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "\n",
        "    #Create a mask based on non-zero sequence\n",
        "    masked_loss = tf.multiply(loss_tnsr, mask)\n",
        "\n",
        "    summed_loss_per_sequence_of_batch = tf.reduce_sum(masked_loss, axis = 1)\n",
        "\n",
        "    average_loss_per_sequence_of_batch = tf.divide(summed_loss_per_sequence_of_batch, non_zero_counts)\n",
        "\n",
        "    #Since the loss is already masked and averaged per sequence\n",
        "    average_masked_loss_per_batch = tf.reduce_mean(average_loss_per_sequence_of_batch, axis = None)\n",
        "\n",
        "    return average_masked_loss_per_batch;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUVjpwSdbHuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_masked_loss2(one_batch_data, max_len, logits, labels):\n",
        "\n",
        "    non_zero_counts = tf.math.count_nonzero(input=one_batch_data, axis = 1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    #Do -1 as the last element of each sequence isn’t used as input.\n",
        "    non_zero_counts = non_zero_counts - 1\n",
        "\n",
        "    #Creating the mask\n",
        "    mask = tf.sequence_mask(non_zero_counts, max_len, dtype=tf.dtypes.float32 )\n",
        "\n",
        "    #Calculate loss for each element of the batch and timestep\n",
        "    loss_tnsr = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "\n",
        "    #Create a mask based on non-zero sequence\n",
        "    masked_loss = tf.multiply(loss_tnsr, mask)\n",
        "\n",
        "    summed_loss_per_sequence_of_batch = tf.reduce_sum(masked_loss, axis = None)\n",
        "    summed_non_zero_counts_of_batch = tf.reduce_sum(non_zero_counts, axis = None)\n",
        "    average_masked_loss_per_batch = tf.divide(summed_loss_per_sequence_of_batch, summed_non_zero_counts_of_batch)\n",
        "\n",
        "    return average_masked_loss_per_batch;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMeowFeiblmN",
        "colab_type": "text"
      },
      "source": [
        "**Training using custom loss function 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lPHnfQkcRQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target, one_batch_data, max_len):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = gru_training_model(inp)\n",
        "    loss = custom_masked_loss1(one_batch_data, max_len, logits = predictions, labels=target)\n",
        "  grads = tape.gradient(loss, gru_training_model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, gru_training_model.trainable_variables))\n",
        "\n",
        "  return loss;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ltTQDmcRZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWLD7NMocRV3",
        "colab_type": "code",
        "outputId": "e0fb2da9-88c5-4a43-b100-b10c8a6e2d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 20\n",
        "max_len = 448\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = gru_training_model.reset_states()\n",
        "\n",
        "  for (batch_n, (onehot_inp, categ_inp, categ_target)) in enumerate(zip(onehot_encoded_batch_data, batched_categorical_data, batched_label_data)):\n",
        "    loss = train_step(inp = onehot_inp, target = categ_target, one_batch_data = categ_inp, max_len = max_len)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    gru_training_model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "gru_training_model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.357480049133301\n",
            "Epoch 1 Batch 100 Loss 2.440483570098877\n",
            "Epoch 1 Batch 200 Loss 2.0773138999938965\n",
            "Epoch 1 Loss 1.9816\n",
            "Time taken for 1 epoch 27.535837411880493 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.0602526664733887\n",
            "Epoch 2 Batch 100 Loss 1.9782841205596924\n",
            "Epoch 2 Batch 200 Loss 1.8430869579315186\n",
            "Epoch 2 Loss 1.8067\n",
            "Time taken for 1 epoch 26.861476182937622 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.8456518650054932\n",
            "Epoch 3 Batch 100 Loss 1.8182569742202759\n",
            "Epoch 3 Batch 200 Loss 1.6516454219818115\n",
            "Epoch 3 Loss 1.6413\n",
            "Time taken for 1 epoch 27.246646642684937 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.680924892425537\n",
            "Epoch 4 Batch 100 Loss 1.665950059890747\n",
            "Epoch 4 Batch 200 Loss 1.5056744813919067\n",
            "Epoch 4 Loss 1.5073\n",
            "Time taken for 1 epoch 27.61521577835083 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5570275783538818\n",
            "Epoch 5 Batch 100 Loss 1.5552847385406494\n",
            "Epoch 5 Batch 200 Loss 1.4008651971817017\n",
            "Epoch 5 Loss 1.4060\n",
            "Time taken for 1 epoch 27.77303457260132 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4630892276763916\n",
            "Epoch 6 Batch 100 Loss 1.4726283550262451\n",
            "Epoch 6 Batch 200 Loss 1.3267936706542969\n",
            "Epoch 6 Loss 1.3315\n",
            "Time taken for 1 epoch 27.667627334594727 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3917174339294434\n",
            "Epoch 7 Batch 100 Loss 1.401740312576294\n",
            "Epoch 7 Batch 200 Loss 1.268568754196167\n",
            "Epoch 7 Loss 1.2709\n",
            "Time taken for 1 epoch 27.668094396591187 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3325358629226685\n",
            "Epoch 8 Batch 100 Loss 1.3362950086593628\n",
            "Epoch 8 Batch 200 Loss 1.218503713607788\n",
            "Epoch 8 Loss 1.2174\n",
            "Time taken for 1 epoch 27.70577359199524 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2890050411224365\n",
            "Epoch 9 Batch 100 Loss 1.2865978479385376\n",
            "Epoch 9 Batch 200 Loss 1.1784589290618896\n",
            "Epoch 9 Loss 1.1793\n",
            "Time taken for 1 epoch 27.737211227416992 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.252218246459961\n",
            "Epoch 10 Batch 100 Loss 1.2513459920883179\n",
            "Epoch 10 Batch 200 Loss 1.1488306522369385\n",
            "Epoch 10 Loss 1.1508\n",
            "Time taken for 1 epoch 27.771015644073486 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.2245875597000122\n",
            "Epoch 11 Batch 100 Loss 1.2209280729293823\n",
            "Epoch 11 Batch 200 Loss 1.122618556022644\n",
            "Epoch 11 Loss 1.1271\n",
            "Time taken for 1 epoch 27.738527536392212 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.1954982280731201\n",
            "Epoch 12 Batch 100 Loss 1.1990036964416504\n",
            "Epoch 12 Batch 200 Loss 1.1027638912200928\n",
            "Epoch 12 Loss 1.1074\n",
            "Time taken for 1 epoch 27.716339826583862 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.1672954559326172\n",
            "Epoch 13 Batch 100 Loss 1.1740036010742188\n",
            "Epoch 13 Batch 200 Loss 1.0831292867660522\n",
            "Epoch 13 Loss 1.0923\n",
            "Time taken for 1 epoch 27.74877381324768 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.1440984010696411\n",
            "Epoch 14 Batch 100 Loss 1.1535606384277344\n",
            "Epoch 14 Batch 200 Loss 1.0683245658874512\n",
            "Epoch 14 Loss 1.0766\n",
            "Time taken for 1 epoch 27.747778177261353 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.1255478858947754\n",
            "Epoch 15 Batch 100 Loss 1.135481834411621\n",
            "Epoch 15 Batch 200 Loss 1.0526225566864014\n",
            "Epoch 15 Loss 1.0650\n",
            "Time taken for 1 epoch 27.732396364212036 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.1084840297698975\n",
            "Epoch 16 Batch 100 Loss 1.1214015483856201\n",
            "Epoch 16 Batch 200 Loss 1.0389087200164795\n",
            "Epoch 16 Loss 1.0515\n",
            "Time taken for 1 epoch 27.737642765045166 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.092888593673706\n",
            "Epoch 17 Batch 100 Loss 1.1025720834732056\n",
            "Epoch 17 Batch 200 Loss 1.024897575378418\n",
            "Epoch 17 Loss 1.0408\n",
            "Time taken for 1 epoch 27.75464367866516 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.0780692100524902\n",
            "Epoch 18 Batch 100 Loss 1.0903639793395996\n",
            "Epoch 18 Batch 200 Loss 1.0139051675796509\n",
            "Epoch 18 Loss 1.0307\n",
            "Time taken for 1 epoch 27.731720447540283 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.0658107995986938\n",
            "Epoch 19 Batch 100 Loss 1.0750298500061035\n",
            "Epoch 19 Batch 200 Loss 1.0010977983474731\n",
            "Epoch 19 Loss 1.0222\n",
            "Time taken for 1 epoch 27.762583255767822 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0510358810424805\n",
            "Epoch 20 Batch 100 Loss 1.0630455017089844\n",
            "Epoch 20 Batch 200 Loss 0.9904446005821228\n",
            "Epoch 20 Loss 1.0131\n",
            "Time taken for 1 epoch 27.746737718582153 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0E3zjkYf7z9",
        "colab_type": "code",
        "outputId": "8ab2cb04-b270-4ffe-8203-d22169044dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_19'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNl-8LLvb6Gw",
        "colab_type": "code",
        "outputId": "8cb51ce8-f30a-458b-dbd3-81d95b2ffcdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "gru_training_model2 = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  rnn_units=448,\n",
        "  batch_size=128)\n",
        "\n",
        "example_input_tensor = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, 448, vocab_size]))\n",
        "example_prediction = gru_training_model2(example_input_tensor)\n",
        "#print(example_prediction)\n",
        "example_prediction.shape\n",
        "\n",
        "gru_training_model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_2 (GRU)                  multiple                  709632    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  35022     \n",
            "=================================================================\n",
            "Total params: 744,654\n",
            "Trainable params: 744,654\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3J_j9Q7b6No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir2 = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix2 = os.path.join(checkpoint_dir2, \"ckpt_model2_{epoch}\")\n",
        "\n",
        "checkpoint_callback2=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix2,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jf8Z_77b6Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step2(inp, target, one_batch_data, max_len):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = gru_training_model2(inp)\n",
        "    loss = custom_masked_loss2(one_batch_data, max_len, logits = predictions, labels=target)\n",
        "  grads = tape.gradient(loss, gru_training_model2.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, gru_training_model2.trainable_variables))\n",
        "\n",
        "  return loss;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urHyyhJMcsuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer2 = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V93BkO6tcsyq",
        "colab_type": "code",
        "outputId": "ad59b7f1-f45e-4945-aa24-3c66129f37b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 20\n",
        "max_len = 448\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = gru_training_model2.reset_states()\n",
        "\n",
        "  for (batch_n, (onehot_inp, categ_inp, categ_target)) in enumerate(zip(onehot_encoded_batch_data, batched_categorical_data, batched_label_data)):\n",
        "    loss = train_step2(inp = onehot_inp, target = categ_target, one_batch_data = categ_inp, max_len = max_len)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    gru_training_model2.save_weights(checkpoint_prefix2.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "gru_training_model2.save_weights(checkpoint_prefix2.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.358142375946045\n",
            "Epoch 1 Batch 100 Loss 2.385197162628174\n",
            "Epoch 1 Batch 200 Loss 2.2783901691436768\n",
            "Epoch 1 Loss 2.1744\n",
            "Time taken for 1 epoch 27.627805471420288 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.5182697772979736\n",
            "Epoch 2 Batch 100 Loss 2.1150431632995605\n",
            "Epoch 2 Batch 200 Loss 2.0190112590789795\n",
            "Epoch 2 Loss 1.9390\n",
            "Time taken for 1 epoch 26.934789896011353 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.00642466545105\n",
            "Epoch 3 Batch 100 Loss 1.9639248847961426\n",
            "Epoch 3 Batch 200 Loss 1.8999260663986206\n",
            "Epoch 3 Loss 1.8389\n",
            "Time taken for 1 epoch 27.448675870895386 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.8690279722213745\n",
            "Epoch 4 Batch 100 Loss 1.8850500583648682\n",
            "Epoch 4 Batch 200 Loss 1.820862889289856\n",
            "Epoch 4 Loss 1.7749\n",
            "Time taken for 1 epoch 27.508951663970947 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.7887191772460938\n",
            "Epoch 5 Batch 100 Loss 1.8267598152160645\n",
            "Epoch 5 Batch 200 Loss 1.7662551403045654\n",
            "Epoch 5 Loss 1.7269\n",
            "Time taken for 1 epoch 27.330742597579956 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.7258942127227783\n",
            "Epoch 6 Batch 100 Loss 1.782846450805664\n",
            "Epoch 6 Batch 200 Loss 1.7243610620498657\n",
            "Epoch 6 Loss 1.6903\n",
            "Time taken for 1 epoch 27.40380024909973 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.681548833847046\n",
            "Epoch 7 Batch 100 Loss 1.7503291368484497\n",
            "Epoch 7 Batch 200 Loss 1.6889832019805908\n",
            "Epoch 7 Loss 1.6612\n",
            "Time taken for 1 epoch 27.484817266464233 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.648929476737976\n",
            "Epoch 8 Batch 100 Loss 1.7193548679351807\n",
            "Epoch 8 Batch 200 Loss 1.6608420610427856\n",
            "Epoch 8 Loss 1.6351\n",
            "Time taken for 1 epoch 27.426196575164795 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.6174612045288086\n",
            "Epoch 9 Batch 100 Loss 1.6955682039260864\n",
            "Epoch 9 Batch 200 Loss 1.63467276096344\n",
            "Epoch 9 Loss 1.6074\n",
            "Time taken for 1 epoch 27.357609033584595 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.5913029909133911\n",
            "Epoch 10 Batch 100 Loss 1.6773277521133423\n",
            "Epoch 10 Batch 200 Loss 1.6126649379730225\n",
            "Epoch 10 Loss 1.5831\n",
            "Time taken for 1 epoch 27.425506114959717 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.5750973224639893\n",
            "Epoch 11 Batch 100 Loss 1.6595361232757568\n",
            "Epoch 11 Batch 200 Loss 1.592136263847351\n",
            "Epoch 11 Loss 1.5649\n",
            "Time taken for 1 epoch 27.457117319107056 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.5583572387695312\n",
            "Epoch 12 Batch 100 Loss 1.6413981914520264\n",
            "Epoch 12 Batch 200 Loss 1.57435142993927\n",
            "Epoch 12 Loss 1.5485\n",
            "Time taken for 1 epoch 27.405118227005005 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.5451968908309937\n",
            "Epoch 13 Batch 100 Loss 1.6239689588546753\n",
            "Epoch 13 Batch 200 Loss 1.5574098825454712\n",
            "Epoch 13 Loss 1.5345\n",
            "Time taken for 1 epoch 27.42347550392151 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.5311377048492432\n",
            "Epoch 14 Batch 100 Loss 1.6131304502487183\n",
            "Epoch 14 Batch 200 Loss 1.54076087474823\n",
            "Epoch 14 Loss 1.5204\n",
            "Time taken for 1 epoch 27.369622230529785 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.517227053642273\n",
            "Epoch 15 Batch 100 Loss 1.5994738340377808\n",
            "Epoch 15 Batch 200 Loss 1.5247337818145752\n",
            "Epoch 15 Loss 1.5061\n",
            "Time taken for 1 epoch 27.36984896659851 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.5031895637512207\n",
            "Epoch 16 Batch 100 Loss 1.5873363018035889\n",
            "Epoch 16 Batch 200 Loss 1.5113751888275146\n",
            "Epoch 16 Loss 1.4923\n",
            "Time taken for 1 epoch 27.367607593536377 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.4868313074111938\n",
            "Epoch 17 Batch 100 Loss 1.5734494924545288\n",
            "Epoch 17 Batch 200 Loss 1.4981815814971924\n",
            "Epoch 17 Loss 1.4804\n",
            "Time taken for 1 epoch 27.393270254135132 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.4777318239212036\n",
            "Epoch 18 Batch 100 Loss 1.5619354248046875\n",
            "Epoch 18 Batch 200 Loss 1.4854155778884888\n",
            "Epoch 18 Loss 1.4683\n",
            "Time taken for 1 epoch 27.403273344039917 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4651927947998047\n",
            "Epoch 19 Batch 100 Loss 1.5489567518234253\n",
            "Epoch 19 Batch 200 Loss 1.4695405960083008\n",
            "Epoch 19 Loss 1.4528\n",
            "Time taken for 1 epoch 27.407066345214844 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.4605571031570435\n",
            "Epoch 20 Batch 100 Loss 1.5345072746276855\n",
            "Epoch 20 Batch 200 Loss 1.453513264656067\n",
            "Epoch 20 Loss 1.4367\n",
            "Time taken for 1 epoch 27.403342962265015 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxFpVfa5hChU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p saved_model_1\n",
        "!mkdir -p saved_model_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP-SVro_hCq0",
        "colab_type": "code",
        "outputId": "7fc7a56f-a457-4937-a666-a2225be20e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "gru_training_model.save('saved_model_1/my_model1') \n",
        "gru_training_model2.save('saved_model_2/my_model2') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: saved_model_1/my_model1/assets\n",
            "INFO:tensorflow:Assets written to: saved_model_2/my_model2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEg_4weIhwsl",
        "colab_type": "code",
        "outputId": "e66614d6-99e4-48b4-8067-28fa77c93859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "!zip -r /content/saved_model_1.zip /content/saved_model_1\n",
        "!zip -r /content/saved_model_2.zip /content/saved_model_2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/saved_model_1/ (stored 0%)\n",
            "  adding: content/saved_model_1/my_model1/ (stored 0%)\n",
            "  adding: content/saved_model_1/my_model1/saved_model.pb (deflated 90%)\n",
            "  adding: content/saved_model_1/my_model1/assets/ (stored 0%)\n",
            "  adding: content/saved_model_1/my_model1/variables/ (stored 0%)\n",
            "  adding: content/saved_model_1/my_model1/variables/variables.index (deflated 41%)\n",
            "  adding: content/saved_model_1/my_model1/variables/variables.data-00000-of-00002 (deflated 71%)\n",
            "  adding: content/saved_model_1/my_model1/variables/variables.data-00001-of-00002 (deflated 8%)\n",
            "  adding: content/saved_model_2/ (stored 0%)\n",
            "  adding: content/saved_model_2/my_model2/ (stored 0%)\n",
            "  adding: content/saved_model_2/my_model2/saved_model.pb (deflated 91%)\n",
            "  adding: content/saved_model_2/my_model2/assets/ (stored 0%)\n",
            "  adding: content/saved_model_2/my_model2/variables/ (stored 0%)\n",
            "  adding: content/saved_model_2/my_model2/variables/variables.index (deflated 41%)\n",
            "  adding: content/saved_model_2/my_model2/variables/variables.data-00000-of-00002 (deflated 71%)\n",
            "  adding: content/saved_model_2/my_model2/variables/variables.data-00001-of-00002 (deflated 10%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j6PSqhzh4vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/saved_model_1.zip\")\n",
        "files.download(\"/content/saved_model_2.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "073xGADSl5cv",
        "colab_type": "text"
      },
      "source": [
        "# **Language Model**\n",
        "\n",
        "**Summary:** Language is generated by both models, we also tested generating language by remving stop condition on the stop_character.\n",
        "\n",
        "Both the languages generated by models 1 and 2 were almost the same and followed some structure. Some most repeating words like Jesus, LORD and GOD appeared in the output\n",
        "\n",
        "Without the stop consition, the language model did not stop and started to fill the text cell. Hence we added a max char length of 5000 to avoid crashing of our notebook. If left to its own then the model doesn't seem to stop and would eventually result in filling up of memory of the system, hence breaks it.\n",
        "\n",
        "Also more activations could improve the performance of the languae model as it could remember more parts of the data for a bit more time\n",
        "\n",
        "**Model with 512 hidden units and loss function 2:** Provided a better output with no unnecessary spaces, line breaks and conformed to the structure mentioned in the bible\n",
        "\n",
        "**Model with 450 hidden units and loss function 1:** Provided worse results than the 512 units one but same output as that of  loss function 2 450 units. It had unecessary spaces , line breaks and some special characters appearing where it shouldn't\n",
        "\n",
        "**Model with 450 hidden units and loss function 2:** Provided the same output as 450 units loss function 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IyQmnJk7YFs",
        "colab_type": "text"
      },
      "source": [
        "**Language model for model with 512 hidden activations and loss function 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcZ7CCOY66Lj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "7a496591-70cb-4b42-b0a1-32355097e603"
      },
      "source": [
        "language_model_512 = build_model(vocab_size=len(vocab),  rnn_units=512, batch_size=1)\n",
        "\n",
        "language_model_512.load_weights(tf.train.latest_checkpoint(checkpoint_dir3))\n",
        "\n",
        "language_model_512.build(tf.TensorShape([1, None, vocab_size]))\n",
        "\n",
        "language_model_512.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_1 (GRU)                  multiple                  909312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  40014     \n",
            "=================================================================\n",
            "Total params: 949,326\n",
            "Trainable params: 949,326\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxZ0hVdI66T5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e89b5b0b-56d5-41ed-8f3b-3b9f6fa92907"
      },
      "source": [
        "line_size = 10\n",
        "max_char_per_line = 500\n",
        "stop_char = \"</S>\"\n",
        "start_char = \"<S>\"\n",
        "start_ind = ch_to_ind[\"<S>\"]\n",
        "stop_ind = ch_to_ind[\"</S>\"]\n",
        "start_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = start_ind, depth = vocab_size), 0), 0)\n",
        "stop_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = stop_ind, depth = vocab_size), 0), 0)\n",
        "print(start_sequence_onehot_encode.shape)\n",
        "string = \"\"\n",
        "tmp = None\n",
        "index_list = list(range(vocab_size))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWhITCCQ66Rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "55b92aa5-b907-4438-8873-d43a9adefd03"
      },
      "source": [
        "print(tf.expand_dims(start_sequence_onehot_encode, 0))\n",
        "out_one_time_step = tf.nn.softmax(axis=-1,logits=tf.expand_dims(start_sequence_onehot_encode, 0))\n",
        "out_one_time_step"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 78), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1, 78), dtype=float32, numpy=\n",
              "array([[[[0.01254417, 0.0340986 , 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMmJau17J7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_output(logits):\n",
        "  return tf.nn.softmax(axis = -1, logits = logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofQmW4Am7KC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dccf25f-bc24-48bf-81bc-b7c654c56298"
      },
      "source": [
        "for line in range(15):\n",
        "  string = \"\"\n",
        "  ind = 0\n",
        "  out_ind = None\n",
        "  language_model_512.reset_states()\n",
        "  while(out_ind != ch_to_ind[\"</S>\"]):\n",
        "    if(ind == 0):\n",
        "      predicted_char_logits = language_model_512(start_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output(predicted_char_logits)\n",
        "      #print(predicted_char_softmax)\n",
        "    else:\n",
        "      next_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ind, depth = vocab_size), 0), 0)\n",
        "      predicted_char_logits = language_model_512(next_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output(predicted_char_logits)\n",
        "\n",
        "    np_array = predicted_char_softmax.numpy()\n",
        "    \n",
        "    ind = np.random.choice( index_list, p=np_array.flatten())\n",
        "    out_ind = ind\n",
        "    if(ind != ch_to_ind[\"</S>\"]):\n",
        "      #string = string + \"\\n\";\n",
        "      #break;    \n",
        "      string = string + ind_to_ch[ind]\n",
        "    \n",
        "    \n",
        "  \n",
        "  print(string)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Parain among you.\n",
            "\n",
            "\n",
            "\n",
            "My herd, is from things yourselves of, saying will I say?  \n",
            " And he cast fords\n",
            "to greater that they may all their own sight he will know, many that\n",
            "secreth some mother, let him doest shall purso rule over me as I out\n",
            "of thestung?  \n",
            " And Moses beseech you to years of gold, and dragouse not the\n",
            "forms of the fruits we found shall take the maits, and how\n",
            "charged in the altar that stood by saints; \n",
            " There are partbedmoness of their own corruptions, anded the country\n",
            "of nine anger, saying, What they seen to bring\n",
            "noneness to pray, and often many at tearitl.\n",
            "\n",
            "\n",
            " Let her closhing for nought with the holy\n",
            "aposklessed, and my God had the bond of Christ.\n",
            "\n",
            "\n",
            " Every sign chareth all things from deather.\n",
            "\n",
            "\n",
            " And he shall out of them that hath an exatines was law wait at your\n",
            "old and unwisers, the world would cause them to the parpase of one\n",
            "of these feightiness.\n",
            "\n",
            "\n",
            " We do well in the way, I will be desolate,\n",
            "by one man's people might stand and forsaken.\n",
            "\n",
            "\n",
            " So kind send common taken with God at the bonds, in glory\n",
            "that are the house should be rooter, and transgressed.\n",
            "\n",
            "\n",
            "\n",
            "There was a grief, but beleest, and let expecting awoy flick, even as the\n",
            "hands on another.\n",
            "\n",
            "\n",
            " Though that which is ceaseth in the Lord, which is eatena, as casting\n",
            "them to build us Christ.\n",
            "\n",
            "\n",
            " Have we not mecked, worke, whether I come apparet until this piece, who\n",
            "forgiveth sported, the families of the life of ourchest eaten there\n",
            "be your bonds, loven will lay things without the law.\n",
            "\n",
            "\n",
            " Jesus answered, He first from faithful man shall fill the world?  \n",
            " For this halt thought and women, being instrumped himself with our\n",
            "idols; lest I speak prayers on his hand upon every\n",
            "tribule.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed8sLTfL7Q8h",
        "colab_type": "text"
      },
      "source": [
        "**Language model for model with 450 hidden activations and loss function 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTN5Qhaal7Ts",
        "colab_type": "code",
        "outputId": "6c0319c3-c770-4059-d3e6-1be59204dae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "language_model = build_model(vocab_size=len(vocab),  rnn_units=448, batch_size=1)\n",
        "\n",
        "language_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fac36b4ff60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqn962WgmzJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language_model.build(tf.TensorShape([1, None, vocab_size]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IvJj7Cxl7rj",
        "colab_type": "code",
        "outputId": "3e28df54-b801-432a-b21e-bb657ee5eedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "language_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_4 (GRU)                  multiple                  709632    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  35022     \n",
            "=================================================================\n",
            "Total params: 744,654\n",
            "Trainable params: 744,654\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQG34i3rl7zh",
        "colab_type": "code",
        "outputId": "954539b5-6141-4c43-ac1d-368856dfff6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "line_size = 10\n",
        "max_char_per_line = 500\n",
        "stop_char = \"</S>\"\n",
        "start_char = \"<S>\"\n",
        "start_ind = ch_to_ind[\"<S>\"]\n",
        "stop_ind = ch_to_ind[\"</S>\"]\n",
        "start_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = start_ind, depth = vocab_size), 0), 0)\n",
        "stop_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = stop_ind, depth = vocab_size), 0), 0)\n",
        "print(start_sequence_onehot_encode.shape)\n",
        "string = \"\"\n",
        "tmp = None\n",
        "index_list = list(range(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qx0JgYerL--",
        "colab_type": "code",
        "outputId": "87b14adf-0496-42bd-e0f0-05c1d135c65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "print(tf.expand_dims(start_sequence_onehot_encode, 0))\n",
        "out_one_time_step = tf.nn.softmax(axis=-1,logits=tf.expand_dims(start_sequence_onehot_encode, 0))\n",
        "out_one_time_step"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(1, 1, 1, 78), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1, 78), dtype=float32, numpy=\n",
              "array([[[[0.01254417, 0.0340986 , 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417, 0.01254417, 0.01254417,\n",
              "          0.01254417, 0.01254417, 0.01254417]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlRFUadbr-8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_output(logits):\n",
        "  return tf.nn.softmax(axis = -1, logits = logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efu6IVAioacF",
        "colab_type": "code",
        "outputId": "4e766091-2cc7-4cbd-8712-9660aed384c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for line in range(15):\n",
        "  string = \"\"\n",
        "  ind = 0\n",
        "  out_ind = None\n",
        "  language_model.reset_states()\n",
        "  while(out_ind != ch_to_ind[\"</S>\"]):\n",
        "    if(i == 0):\n",
        "      predicted_char_logits = language_model(start_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output(predicted_char_logits)\n",
        "      #print(predicted_char_softmax)\n",
        "    else:\n",
        "      next_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ind, depth = vocab_size), 0), 0)\n",
        "      predicted_char_logits = language_model(next_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output(predicted_char_logits)\n",
        "\n",
        "    np_array = predicted_char_softmax.numpy()\n",
        "    \n",
        "    ind = np.random.choice( index_list, p=np_array.flatten())\n",
        "    out_ind = ind\n",
        "    if(ind != ch_to_ind[\"</S>\"]):\n",
        "      #string = string + \"\\n\";\n",
        "      #break;    \n",
        "      string = string + ind_to_ch[ind]\n",
        "    \n",
        "    \n",
        "  \n",
        "  print(string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prot my night\n",
            "borth it ye beccut, The rewe\n",
            "to cumbors which his visclorright acrood; thy sight und\n",
            "mers, an which whence I may devagrises, thou\n",
            "mare bectud for the twere spirt.\n",
            "\n",
            "\n",
            "0d.\n",
            "\n",
            "\n",
            "re\n",
            "ghorsing agrait faturntimer for him; for this\n",
            "\n",
            "men.\n",
            "\n",
            "\n",
            ", When\n",
            "whear I shralive the chill seven the LORD, whather\n",
            "surils firve the Loble gves thim divich an that one incersife: \n",
            "Uzz.\n",
            "\n",
            "\n",
            "GD!\n",
            "\n",
            "(GOD Jesus have connottles?\n",
            "\n",
            "\n",
            "d:\n",
            "\n",
            "oven:\n",
            "\n",
            "6m\n",
            "go mosces. Thus gromong mud.\n",
            "\n",
            "\n",
            "res\n",
            "of our hove chuit the Spiright the Gibien we prame of\n",
            "herof, Go all the clirightwee, are Gals\n",
            "of manus in thinco Fetwered bringthing thine an his\n",
            "saccrich, all the doscerelves,\n",
            "and drighth him not be by cries.\n",
            "\n",
            "\n",
            "!\n",
            "\n",
            "\n",
            "ak\n",
            "thou had from Gisma.\n",
            "\n",
            "\n",
            "y marither.\n",
            "\n",
            "\n",
            "Th the\n",
            "Lept.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCqgHuSzia0f",
        "colab_type": "text"
      },
      "source": [
        "Language generated by loss function 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WSj_okBiaG2",
        "colab_type": "code",
        "outputId": "dc3a75cc-3061-42a5-87a0-d2ecb385dbd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "language_model2 = build_model(vocab_size=len(vocab),  rnn_units=448, batch_size=1)\n",
        "\n",
        "language_model2.load_weights(tf.train.latest_checkpoint(checkpoint_dir2))\n",
        "\n",
        "language_model2.build(tf.TensorShape([1, None, vocab_size]))\n",
        "\n",
        "language_model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_3 (GRU)                  multiple                  709632    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  35022     \n",
            "=================================================================\n",
            "Total params: 744,654\n",
            "Trainable params: 744,654\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukeweoowiaPT",
        "colab_type": "code",
        "outputId": "2c48ac56-64b1-4980-8c57-b96a789c5a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "line_size = 10\n",
        "max_char_per_line = 500\n",
        "stop_char = \"</S>\"\n",
        "start_char = \"<S>\"\n",
        "start_ind = ch_to_ind[\"<S>\"]\n",
        "stop_ind = ch_to_ind[\"</S>\"]\n",
        "start_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = start_ind, depth = vocab_size), 0), 0)\n",
        "stop_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = stop_ind, depth = vocab_size), 0), 0)\n",
        "print(start_sequence_onehot_encode.shape)\n",
        "string = \"\"\n",
        "tmp = None\n",
        "index_list = list(range(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EllyuPRfiaaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_output2(logits):\n",
        "  return tf.nn.softmax(axis = -1, logits = logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQIzpZKaiaYD",
        "colab_type": "code",
        "outputId": "f3f78916-0f21-4189-f7c0-353393fac23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for line in range(30):\n",
        "  string = \"\"\n",
        "  ind = 0\n",
        "  out_ind = None\n",
        "  language_model2.reset_states()\n",
        "  while(out_ind != ch_to_ind[\"</S>\"]):\n",
        "    if(i == 0):\n",
        "      predicted_char_logits = language_model2(start_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "      #print(predicted_char_softmax)\n",
        "    else:\n",
        "      next_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ind, depth = vocab_size), 0), 0)\n",
        "      predicted_char_logits = language_model2(next_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "\n",
        "    np_array = predicted_char_softmax.numpy()\n",
        "    \n",
        "    ind = np.random.choice( index_list, p=np_array.flatten())\n",
        "    out_ind = ind   \n",
        "    if(ind != ch_to_ind[\"</S>\"]):\n",
        "      string = string + ind_to_ch[ind]\n",
        " \n",
        "  print(string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RD of\n",
            "forther, braks it, goth the Kascoows God?\n",
            "\n",
            "\n",
            "w\n",
            "Glore from he bowr him not.\n",
            "\n",
            "\n",
            ",)\n",
            "\n",
            "p.\n",
            "\n",
            "\n",
            ", \n",
            "'\n",
            "had il all.\n",
            "\n",
            "\n",
            "Kwn\n",
            "whose is mormer you, a and things, thou\n",
            "for Ellariah, which worrifes when hopsed hound on\n",
            "hight up by a prayser.\n",
            "\n",
            "\n",
            ",) \n",
            "ys\n",
            "ramingur lam went is oul rices.\n",
            "\n",
            "\n",
            "y?\n",
            "\n",
            "\n",
            "e; \n",
            "'s\n",
            "hach which are her saypith as \n",
            "w, I\n",
            "beke whring ols ir that yiras see plest\n",
            "the dethiff: \n",
            "Je\n",
            "\n",
            "n.\n",
            "\n",
            "\n",
            ":\n",
            "\n",
            "zrow\n",
            "thor hear him great ambrati neming them unto wither,\n",
            "and even also his in thou ath\n",
            "which which from the law: but I chimitions.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Berusthong of the Lord of for hear; \n",
            "pp:\n",
            "but cupt Jush prock, thferews you la,\n",
            "dessige, the scerit, but it whichithan, \n",
            "0 Gaving.\n",
            "\n",
            "\n",
            "7Kortay;\n",
            "\n",
            "RD's that is firt be\n",
            "ghrand to aris even cortionsatich withat: thme ht Rair have\n",
            "glorce; \n",
            "8\n",
            "hand not wam him, What have he\n",
            "sebr is ab cruthing throm Eshrown with it; \n",
            "LORD:\n",
            "Behold the his rechipher there hath laamone; for\n",
            "his\n",
            "Leaziah they\n",
            "had God, arcambrings of Egypt the prostsong on live me,\n",
            "men theron of shrece be from Chramces of drinches\n",
            "croveth not my pried.\n",
            "\n",
            "\n",
            "prish\n",
            "was, \n",
            "ever whom\n",
            "God of whothrow, hos prechich\n",
            "shall petter huarcrood; and that he whill oll\n",
            "spurcy went great coft: Thes hath brother ome.\n",
            "\n",
            "\n",
            "I \n",
            "y the\n",
            "paspers, which he the holy.\n",
            "\n",
            "\n",
            "Prilch; and\n",
            "I \n",
            "ught\n",
            "froth, whom his hose in them sencl; \n",
            "ak\n",
            "chortingut every goving was even ye Lithabe.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ-kQ5RNj7Vu",
        "colab_type": "text"
      },
      "source": [
        "Without considering **</S>** as stop and allowing network to stop itself we see network breaks and overloads colab notebook with text data. hence we have set the char limit to 5000 after running without this limit. Basically network never stops, It keeps on going till we put a break  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kxv9IwiaU8",
        "colab_type": "code",
        "outputId": "1eccf28c-19f9-4796-ea84-47c76405823f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for line in range(10):\n",
        "  string = \"\"\n",
        "  ind = 0\n",
        "  out_ind = None\n",
        "  language_model2.reset_states()\n",
        "  for i in range(5000):\n",
        "    if(i == 0):\n",
        "      predicted_char_logits = language_model2(start_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "      #print(predicted_char_softmax)\n",
        "    else:\n",
        "      next_sequence_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ind, depth = vocab_size), 0), 0)\n",
        "      predicted_char_logits = language_model2(next_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "\n",
        "    np_array = predicted_char_softmax.numpy()\n",
        "    \n",
        "    ind = np.random.choice( index_list, p=np_array.flatten())\n",
        "    out_ind = ind\n",
        "    \n",
        "    if(ind == ch_to_ind[\"</S>\"]):\n",
        "      continue;\n",
        "      #string = string + \"\\n\";\n",
        "      #break;\n",
        "    \n",
        "    \n",
        "    string = string + ind_to_ch[ind]\n",
        "     \n",
        "  print(string)\n",
        "  print(\" ============ ============== ============= \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " And Gloard the forath all tood I reignd fuen int.\n",
            "\n",
            "e unto Jerusale, even the arebod of king.\n",
            "\n",
            "ooved do\n",
            "he the mere shall nwerthinse is stree eville mority thie said unto dost conce\n",
            "fime, by the LORD thou all two his lotedideth thy brought bub.\n",
            "\n",
            " leven attereth that ye rones;\n",
            "ous me was one down: gee pave a woll and deats and of have of ye, I whrece the head\n",
            "hand not enter into the son, The father, and thisterkel lifolive my for these basked in\n",
            "his joicray wncmitifey, aroumited from thereof,\n",
            "lve ys them that dlest for unto the though wosk\n",
            "go teelves, and, Ipren the man the are y atingh the faliet herver: eisnsely sholl not\n",
            "go vomesy wither them in my Gine; ide the sinilustion\n",
            "uncording, and the kingled the vissee in all; the\n",
            "God be say, youse thou hast in the\n",
            "Lord becaue it, and these, belony shall and at him.\n",
            "\n",
            "hose no go they\n",
            "abomen; and that every nive mes: worves saigh yorness forlaned the land\n",
            "wintite: and cled it an yourness of tirned to desterd.\n",
            "\n",
            "rea dest all head now ale him,\n",
            "was sted raught unto your fruth.\n",
            "\n",
            "\n",
            "Tils the hadsed be where we will son of Jerusase, lot: and\n",
            "thou son froth hant appartty from God, and everys ander of scring, and\n",
            "earth in the night in the beave.\n",
            "\n",
            "eded of earing and the wrrethrreseing the sine.\n",
            "\n",
            "he was I lieved wo loed house; he be from\n",
            "the plessile; that is in night debbers oved the Lord lijess.\n",
            "\n",
            " then hath sebleth in the seevoks also, behep\n",
            "the crul'sss be concender amend the la bring to said, blooks the dostedusears of\n",
            "Shew.\n",
            "\n",
            "her thous besalting man, uit there worshales, are wice not mane.\n",
            "\n",
            "elves the Jesus savand the\n",
            "hand of Gon, and is leavedusthnare fait: buith thus sad\n",
            "the hathbus fallinselle, that his gray remink with not fotherthermezn;\n",
            "Lord one sow, the set no on light: ans land from God.\n",
            "\n",
            "e which deat not the thim to nocce, when\n",
            "ketion the great their caur amoth called went not in inheth; but the Carmantsariai, that deevil\n",
            "fout knowlod of kindstes into loven, as is shall blood cut ave Isree of Koresione, be\n",
            "untablick:) oulivedies and the are becaiunce; and\n",
            "the peot take in the rejookedow time this that preccasionce ty men and days bedest of\n",
            "God.\n",
            "\n",
            "reth ow and for the chist; and then the peace whose the lemblant\n",
            "stoget his itgo to de stet evils tha gosted affe intreaies afting aifite.\n",
            "\n",
            "e not ye made\n",
            "counders of Mimpher light, and\n",
            "brectul, Of the timper sorn be was as yearnts, Egypheth croen to saying unto the fill\n",
            "upon the fealed up anes silve the dood propiagaid.\n",
            "\n",
            "hor way resso overed dramelipele.\n",
            "\n",
            "ecer unto hear their touct again,\n",
            "percales, and then shall thy fream faliny.\n",
            "\n",
            "rethis in destrowsed, that resingle is my God, and\n",
            "arr of the house, that that whrostey a are is is not nived.\n",
            "\n",
            "Wo shall say:\n",
            "us from the Levout them a remacern:) at in the\n",
            "mance laod in shincrisuschiry.\n",
            "\n",
            " saving to sading, soe\n",
            "brefore, he it was prot thor all them\n",
            "that certher tehord come Iscriens after this videch end of the sand, by dowrs,\n",
            "thirst and yourenges and say of anowed woureth be be sto: reescicered\n",
            "to them the seven ly nat of God.\n",
            "\n",
            "ray triest the roublation, even the childrefrernibeth them forth in Jerusalba: agrit the roded ogeenied he\n",
            "in the frotiat: the Lelation yau streng forst evetust.\n",
            "\n",
            "e not not alls, ye chimist of\n",
            "God, of Esron me the great into that hearn: and th wo loveth at and from the\n",
            "layont: hersaied therefore, I worron sain is in Jebuch, that they withir; whet\n",
            "my dwelly prayer it.\n",
            "\n",
            "sure the greciess into Galaints witherthors, butilse\n",
            "the LORD you they road: they have day said in tise be\n",
            "with the dost not Egollorkes. Gester thrue truth, and his prayer:\n",
            "eve their swilves yearin reaming spein; and awe the charets.\n",
            "\n",
            "hat cone tor thee lay my carmand whenole, and in the\n",
            "wibe for hadgeaven.\n",
            "\n",
            "reaven unto these and wee in his offered.\n",
            "\n",
            "hold that\n",
            "we before the enst the was lover lifted unsits, evep thou God.\n",
            "\n",
            "sure all scret, that calting the be one: ye ascer as is\n",
            "not also she us: and maince that is which main, with Gade called to do the frock in the here nett, and be\n",
            "he ward he hath you, which his doade: I worbore\n",
            "bughterses of youl carrons when thee God, and the\n",
            "chis made went is in the didnems.\n",
            "\n",
            "eve the verie, of thy cretturnilives are barst the goats your hast from\n",
            "thhrom in Christ: it up of the soar of the tards; and his hath the earty\n",
            "arver, what he seft op Chrethorth again the of is deethed: the gived ekelved meceay.\n",
            "\n",
            "ur be whene agem for me bepringurn all\n",
            "the bearth not of the LORD; se tree use.\n",
            "\n",
            "we his scront in his me, ar thou forth\n",
            "Himy; Forthirted hath our turnied shall trives oversedertion, I abonson,\n",
            "sot is event to the this that with have not unto the\n",
            "same, gluich shall made.\n",
            "\n",
            "evaller take there priessifees.\n",
            "\n",
            "e are bited answerelth do: sover me coth\n",
            "into the doing in dace unto his ong to the prisest me.\n",
            "\n",
            "reet be after that I his have besaretness the\n",
            "excothing, the Lord; Sees am whom see for the\n",
            "to man that me.\n",
            "\n",
            "hatirs abold, shimin, and bown the cholder abought: ny maks the Lord for shes in the\n",
            "leart in whather God out i\n",
            " ============ ============== ============= \n",
            " And GOd ament, not\n",
            "Eve you glorod, of that every on tor abref.\n",
            "\n",
            "e thou stregabied ain my not Jossh.\n",
            "\n",
            "net wheat my shremo thee, the Lord, and on a proce\n",
            "evingd here werrmy fiem nowy anoth.\n",
            "\n",
            "renty thing\n",
            "unto the comest; for God, and on the leath banking, and go be not: nus of\n",
            "the giveth be of the thou honisterveth their soulfu in them, and not?\n",
            "wethers the God on the swetern sleptlone, and days of the ceverning, that\n",
            "it ovey in the even alfore, even to called\n",
            "not weterstefer bright disted, not; and is of that agat thou sainn\n",
            "the goodled disched tisturnted of Judiah the his\n",
            "that saward. Se we be the with the thou and breot; eltoned to rive\n",
            "be rangecifes, and be com this covenid the art phoper.\n",
            "\n",
            "e agains that be fiest thing know that the\n",
            "folled for the breit: that should their very, and herst with people, bethers to\n",
            "mers aspervels whirccuchiscines a compayons, and to.\n",
            "\n",
            "rees the tunder the wisdayoed, that shall\n",
            "the sity said fullands.\n",
            "\n",
            "nath seever, name.\n",
            "\n",
            "ne sowed when therefore the share tulder on the greats, hos from Betarn,\n",
            "the LORD.\n",
            "\n",
            "e in the healt to reeby which with\n",
            "the forth mut: Lefied thou he not in the Carent, but deth vices.\n",
            "\n",
            " arked the given, and to\n",
            "rast not hadacled; not lves be lievel the stiar let of sons of the copting\n",
            "the ccorsuse triess of the workity witer revelseed you whoce?  ad us not out to anger.\n",
            "\n",
            "eed that ye to hus wook tut\n",
            "ma son of the Let the saidion, and\n",
            "bhead men intfather: himif the wrenderbaks of to reepteth, but Jesausury his sure: a that destornath be\n",
            "over buat himsel.\n",
            "\n",
            "antitepty, thol Gad, among and is they.\n",
            "\n",
            "ewed evill the sereath the Lurd, nor good seet forther is not\n",
            "in his excalled, thy son go away.\n",
            "\n",
            "ture of Chrim, that Sau have to the toge not evered of\n",
            "diedned.\n",
            "\n",
            "e the coorsa and shall nough stey.\n",
            "\n",
            "etlerinngs thborcturned the great we contine to in the son of a let.\n",
            "\n",
            " the the coven thereficeaby unto to prictions of\n",
            "an the found lipp. Th than his belforn that they have you that ann Garity, and dolecteth\n",
            "is are not were not them, to which as as in\n",
            "that knowh that yef of wheat.\n",
            "\n",
            "sageth you made levaings to doce, et what th you, he wither\n",
            "cit eving.\n",
            "\n",
            "hevoun all the youngrosmaught alfort took and the priestined, traken him to the loges, that\n",
            "is of diegd kinggolns\n",
            "sain, and weeling Jesubif also.\n",
            "\n",
            "hasp a wext assone days's son for thiseph, fiblessed by rain\n",
            "ploachethersezek in him, and fince for the edret.\n",
            "\n",
            "trece of belodyst Elirnath asmany of men.\n",
            "\n",
            "\n",
            "ever thereos, let thy art, placed you from heat be\n",
            "theed which agmingerpiaasness it; and shes\n",
            "the loprow, and whichs indo the innaring are froeth thine.\n",
            "\n",
            "elests not that he east of the gornise them: e\n",
            "is not was prings.\n",
            "\n",
            "lead: and dole of heath to the lear he have own livicougd unto he\n",
            "that them: for which dester unto the town that ye tob cording before\n",
            "the world with the Lord: ew thereod there that arterth.\n",
            "\n",
            "e unto man\n",
            "the cunces\n",
            "the semble many posur the nem ong with is geesion\n",
            "to beaved me, rethither that drether in housent, and lowed with his is dees\n",
            "foperifessed in thee, and them their conce good of not davernession: for thou ustimoutweredisuduse of of\n",
            "the Lord of discrets, and courts unto him: lighings\n",
            "the come the have be bode of God.\n",
            "\n",
            "sure: and we to a judges a cape unto his havastion: morsted\n",
            "poon thou lengts; yet father to perpoks, and pricustiay of God? belekeed\n",
            "good the Let Elint, he wrirster ye hath pops shall that will dor broogether you.\n",
            "\n",
            "gakerthatlest comest, mute beweed\n",
            "did bels that taken of foolfe.\n",
            "\n",
            "nare s as your consteth, this is borning) they scaye that destray of deing hit mand.\n",
            "\n",
            "reah to father.\n",
            "\n",
            "ections\n",
            "aw Beth the kingsont to said, Ged the viold.\n",
            "\n",
            "e any sow; net shable is diend, and they seve also.\n",
            "\n",
            "aded not thry net be for opeed felvel\n",
            "drevan stion thou astounceishe\n",
            "wherit, and ellath dound woll abseamed armed him. Let the minty unto the death\n",
            "yearite ome to them timed?  ut spoves as savin.\n",
            "\n",
            "his seave God, and doing be gone of the land,\n",
            "see of the werestray: the poolf, and hearted up the\n",
            "LORD, and the LORD apparty\n",
            "that I are in this my forder save them, and ten child be ingl.\n",
            "\n",
            " is and thouly\n",
            "dict owfe the dict of of hebsel. And that he seame through the kendred wear love what\n",
            "we at rejoice, we not dile.\n",
            "\n",
            "lesesed when I a son youroverech were be from Gravin stoin.\n",
            "\n",
            "ckening thou came aswete out them, the worce go now down\n",
            "the row in the having and from the suithful in thee anest this shall spited the\n",
            "comestatied on enden the Led; ad which Gereah swrein lived where\n",
            "childing bus breth sheee earth, ye God aboth propted your\n",
            "rese, there is doace lob-pavid unto the sickluch eot.\n",
            "\n",
            "ree not the wird, we compaying chording to right of the\n",
            "prayerthin be wasped, sow it inot Mosem alsor and flow and your law.\n",
            "\n",
            "e Lorded us ffering with the bewe in anothers brefore the fir had\n",
            "with the landsdinceless with in them to by with thee as given unto not God on all see.\n",
            "\n",
            " was givery; and with the dear ong unto the dwell\n",
            "race of his set of the children: ing people and thereof ToI God \n",
            " ============ ============== ============= \n",
            " Afore the Kenistime scom I blesten affire thee becond.\n",
            "\n",
            "ecree overing to receiveded, there be God ye bire this thourse word,\n",
            "thopherings wed maravinis token the redore dill, and cound a ghrictice or them, becais two\n",
            "corning for of them for the certan; and this priched them\n",
            "and a cheste.\n",
            "\n",
            "ee fruth, which when the worveer the Lord of Neabuse: noth I before fored ouch also the work of\n",
            "the sew of the world greats, and reduffemanceise ur.\n",
            "\n",
            "reas an wise we lot knocl wither the\n",
            "brothered, leave accoftings serve the shes with Git; and he yirsteofful nof of bring of\n",
            "the thereof shall be wents of God flewed unto the Gasily and many\n",
            "antortifets, tiglet afty Gide one fall.\n",
            "\n",
            "e set, and stounfey jerciay; rechossuit aserye an: when hath ain the leadgurnetions\n",
            "the shall tath fiar thou andsers had nare: aven when\n",
            "he is detions: eir steth spok to host the heed prayone of the Lord God sons of the story:\n",
            "hos that come tred: forth is it a way of you. go theit if the banstsest\n",
            "the deunter of him spore slee pased enqray me.\n",
            "\n",
            "not arty proestion wickid sain was in ay and comcars, and\n",
            "brother of the nestions to cast with hand.\n",
            "\n",
            "ee in the Lord that I that night weet my God shall desigethes: theal\n",
            "surely not it in God.\n",
            "\n",
            "rek when ye bearbitbunceson was ye also\n",
            "the land unto you trisuse mitt of the firr?\n",
            "\n",
            "new is go go on: the sayt of the dine.\n",
            "\n",
            "e was blataring unto this: foul\n",
            "congment limnsseth the thoun spircher of the might.\n",
            "\n",
            "reas so so wept worthernatr weel revercoriot that which\n",
            "thou, the rans reams whomy the seven his notwed.\n",
            "\n",
            "he unto that Sparcus the one of kings.\n",
            "\n",
            "e ever: be sest fen\n",
            "decen im corngrigd tits.\n",
            "\n",
            "hers\n",
            "of riven.\n",
            "\n",
            " the ellly ustood he strich is whatsel reigh njut spervel dest.\n",
            "\n",
            "et one yau God by wasked, gtereal was\n",
            "son, and the Lord whircharings, fore the wore\n",
            "fund sayior for David, thy twent the selve thou also begrour that thoum not is set all\n",
            "to he that he thou? and of I have a went gooroveth their know unto God,\n",
            "naston is spoary the dess; and he choloved not\n",
            "derred Dandained that be forthing this God in the lived.\n",
            "\n",
            "ean, which ear sheer that great it of\n",
            "ye redustered frosh: for with the wreater to mofes heaven was any was\n",
            "name and ince one sadred withrrifited; and th receaves on the therred tims.\n",
            "\n",
            " whother, I make the gold, ech forty priest\n",
            "ye heard of this firstinched, and the wasusnescrifess ma;\n",
            "ars and conty untith you saypidgurnens yor, If a denace\n",
            "becicuartion of ma, the\n",
            "willwered's day of he modle the heaed; and by heaventions to anover you to which the stoot\n",
            "brethan; whis of cunto sot manyst not my most.\n",
            "\n",
            "ling distions ascugdevons of mount the citided; it they soves word whon may found to resterco\n",
            "unto the Lasings.\n",
            "\n",
            "e priects of Judah of the Lord to mornil\n",
            "treeser and not, and it the chyerichatiled my\n",
            "stad fament revaia, that ye at deats in the God rewardercuceas to row turning\n",
            "the chirck that was thou in him; the strew as pense the asmests afterthings,\n",
            "to Jonchahin, thou man upon in the tom the wheread of God wreating.\n",
            "\n",
            "horthos fro aby, they\n",
            "the deray he shale byse the that have of one all, and ie was and unto the\n",
            "son of Tighteatien.\n",
            " he strount to palted of defurgivod of God mace\n",
            "day shall corn of the Lord Rach wither abomed his is and lirest thee.\n",
            "\n",
            "ree hor accorco that the wor shint amout the heave as evey cone of this\n",
            "druse will: eptandent fire his besoken love for.\n",
            "\n",
            "racentlareth unto the Lord, and is woll be friest, we hand\n",
            "to dest of the liess to eith that thy kight.\n",
            "\n",
            "ree said shall thece they ye sure all\n",
            "excest domen of ma.\n",
            "\n",
            " son, brefore mage not rest and him from uttoments our the steing that he contannecy.\n",
            "\n",
            "lep the pencregrovolore, Phipters of with the town out theie scrisg affee\n",
            "land.\n",
            "\n",
            "e finled shall son our longed vice to shen thou belicule ma\n",
            "ther becalite tweling the wood of his that, Asay awal, whrniteo\n",
            "bearth thinguse to rice on\n",
            "thy galany boftengmelfess, but as his\n",
            "driteth that for theneds; and set the will exatter which thee\n",
            "risted hirse tewlle for into the father.\n",
            "\n",
            "rew and seen thuse his stourn, by nebth not dided of in him;\n",
            "for the stitens and the Lavint deed out the arcain, ardartanden and even.\n",
            "\n",
            "us gry ssuch be dey\n",
            "sted, and werestlivides.\n",
            "\n",
            "ar by the disting? So\n",
            "and not I rise you for the son.\n",
            "\n",
            "ot name love their ye, soll unto the out of never, which\n",
            "was yearce brought beach dayeds of the trowar: they prisetked hels,\n",
            "that sershed Arvart sous named to on have servants: for the sine with justerthing in the Lord:\n",
            "say, ye also untifeess mace, but row denawr bit: eveth you maride also house: it my wixece dost not\n",
            "on judg yightatle, to ye with is that heselven the Jehusacabof hosever wordife.\n",
            "\n",
            "loosed ase seed his was to retu beful thee, blester the fort man,\n",
            "shale deserth of thy with. And the spori yet\n",
            "was thouwing the sies of Benotherienssiem, thee tithers thoul fortath nor\n",
            "hosked and deliver that was of him own.\n",
            "\n",
            "htaich saith is down, and from in them things thy prientdone of the\n",
            "liven and that dain he shildren oflesh.\n",
            "\n",
            "e ar the walked up thingsog\n",
            " ============ ============== ============= \n",
            " And beccime the lavo the Jedagar God to the great not the worrited that of that he\n",
            "eatar thath cast of merd; but thou art mace of\n",
            "he that comraganding to Joth.\n",
            "\n",
            " preselver with heatmere.\n",
            "\n",
            "y not ever, and to that fruth\n",
            "them goter: that as pasced us fountory out was: nome\n",
            "they he was an also for to them.\n",
            "\n",
            "e sleeph, y brey destroighe desure therefore, thou said\n",
            "firs the kingdeers.\n",
            "\n",
            "ned with an came mintions onf since to did call of the Liviff\n",
            "the samrity of the civen now lood that he evine a blestroms give in\n",
            "the look-wers tring is as yaur thered of my unto my\n",
            "compais, which Egibah, and dor say; their he childer him with ye it\n",
            "wasted to the one sot: o Jeae, whene reves: I lighther sinveut saker not of the\n",
            "balsess of God it.\n",
            "\n",
            "ed arould to nights of be bleas pruch is that\n",
            "he th he great. Tencestly mad says becray rake: and he with the casto in\n",
            "drest of at he said into the great.\n",
            "\n",
            "ferle also he was plest for Jerusaliam: hat live the all\n",
            "sut that thou, ye worls of\n",
            "coptiating an you, but their horders of the mace let watered in alm, the gohed\n",
            "unto is ence.\n",
            "\n",
            "herorded at an in Jochar of destraned, of the raders.\n",
            "\n",
            "her that shall distle, and not\n",
            "it the woss of he to mone the tours pejelver\n",
            "to shall spory and the eldiett in full nother.\n",
            "\n",
            "e me blessifeses to the fey great in tire all this crivenes consity\n",
            "of tehctlss. Seowived from he brecrourty earents you for it themfor the\n",
            "shimmand overterd: ne thee sour dethersely on vicimustinednand: it thou say, and\n",
            "we landed him, which his bahales sever them to them\n",
            "bransele, Speas wise\n",
            "that I riemsed and hus father his come trougl burn so as buln.\n",
            "\n",
            "rega down the sawaring the Chis, thus host from are\n",
            "vasiots of the namans in you together me, to the nibeth with affurity for\n",
            "the pornand, and their of thest afowrer the places of Grows.\n",
            "\n",
            "lorions to down to head him,\n",
            "even sto the swear of Taugh a bitted to the of God, that which we\n",
            "live them ly, o gome that be the sayong;\n",
            "\n",
            " thou also ape the pore wid high.\n",
            "\n",
            " wate haldwine therefort felfing to alsows: he may is sacriteriful of his swer\n",
            "and word.\n",
            "\n",
            "ard sains and exath that me, ene ly rade of God, arons\n",
            "asmay frommest which Tashak.\n",
            "\n",
            "u sek he stiby in the have your is an ever.\n",
            "\n",
            "e thereat, that blesed bankell ast oither to\n",
            "Jaw also end to are.\n",
            "\n",
            "et is desselve a lay loven.\n",
            "\n",
            "les streeeth hengeth ear of Barahause, and from\n",
            "shall be bread Israia, they sit anome falle, and things that of the land,\n",
            "soam is of the ry incress; the find ever.\n",
            "\n",
            "ruse our weated hand not in the stet: nestion atontiry hath, yeat\n",
            "thouse hergering he our the Habuch.\n",
            "\n",
            "hous hid faving, by clued ruter; bulderwed Pharten all the poises of assen.\n",
            "\n",
            "men buist?  leth saying the hrapied\n",
            "of the Lep; ang alls it God; Geth the wore from the son of Israel no\n",
            "but of his speacitenies\n",
            "what maces of the destrech.\n",
            "\n",
            "hos too land shall be gone his namised manering,\n",
            "Wo store an that a fingre men loved you, oned world.\n",
            "\n",
            "ecest\n",
            "thee fullounts of the criesty things; forgeth these his\n",
            "son of God, a sircould forle; he hatts dabite underth; and asset worked suidaint\n",
            "of the comman down of the the exthren, you it brejoms of man and mer.\n",
            "\n",
            "rumise of the ontous, seek tway dees\n",
            "under of the crether to knowh returned? For hor were fulledte, Is\n",
            "they broaven seftlore binted that shall right of peakete.\n",
            "\n",
            "loptessinath watel day: because he theit their every one of the samaled\n",
            "eat is in the seeve to may the procterts of what as a groth one bele and whow wreat go\n",
            "to judgeth wise of Israel unto mosk asty rationscein shell winther to astroy him: ye\n",
            "\n",
            "love sporketh.\n",
            "\n",
            "arbed hat bretherchaion it de of them that who\n",
            "she my voere, by the ling, ye garverss you fear\n",
            "the judgur the know digd in fioth.\n",
            "\n",
            "oup thing he bond are was\n",
            "sy.\n",
            "\n",
            "rough thened sle we I stey, that saith are\n",
            "wis he days\n",
            "of the mittiby of unto thise of snase them, and if you wa land.\n",
            "\n",
            "read to make, and hizzared unto his resed\n",
            "also thou I shalt came in them: nor\n",
            "unto his to man, af Azenters in the chist answo lop do by\n",
            "thoul son shall jo said the recont ansain the ear tear lang; Eve yet that\n",
            "eve ye mocts, ay into the son.  herso\n",
            "that thou be that was the six joightives of\n",
            "Nanabhen, judgurn you sold a many of Gosce, Thus his worlded takeon, and is not in\n",
            "the fimsely on deed.\n",
            "\n",
            "reaves the speat agrein gricked to him, and seevet this maveed thriest in courter; for they\n",
            "stoon of the daites.\n",
            "\n",
            "her that turnlighters of the judgtainstied be in is theircuple,\n",
            "I golders bott streming asfe the LORD wild ark a limed for God, luter of the\n",
            "preleth that wordteros.\n",
            "\n",
            " naided me were by which ye whee al mins deltipets, thy peockifep sa, the Lord was man away\n",
            "drethrred Christ to bad them of Israel.\n",
            "\n",
            "hold save sonce of God; rejoigtinn their that Mavers seave a nest of\n",
            "the Fetienswered the bast strong in Christ to cleans of God.\n",
            "\n",
            "con Rept as the twent the berefarity ance comfeti: yeason out\n",
            "whome heard, ifirded the saying,) hose thou\n",
            "his gotd dounts of whoul; ive entter with a greatend: fouth were worls iz and\n",
            "wijet made the gatiled \n",
            " ============ ============== ============= \n",
            " And hatt they mud unto yot yos, bestrenes.\n",
            "\n",
            "horwered\n",
            "dees and selvo of himised\n",
            "unto an galless.\n",
            "\n",
            "hor water, even ane anstise are ofterthrenserefingion: ean were\n",
            "wither that sexatter tiled the flest, but him riven watither; we therewituce ye\n",
            "son of freel\n",
            "reecr the fain to the dispites you, but thee, lied archiny, and feed God of\n",
            "Juri? brigh the ended and unto\n",
            "unto unto all the Lord said, hose hersuif: e pass.\n",
            "\n",
            " the dister my not not\n",
            "men and rands.\n",
            "\n",
            "e thou windom that is belord goodnes, and of the dwand\n",
            "the rows I name: hear that by the long among at ye mave the are\n",
            "searth consuring thr bekenty\n",
            "the dacinsen.\n",
            "\n",
            "e me, therefore in when the Lord our bredgod to regess,\n",
            "and thou bling and word other, Lover.\n",
            "\n",
            "e a curtany ling of\n",
            "the sain thwit sobled becayitly dosly you ma, to them not tom to saus\n",
            "bogneth her sons of the city in hit the dulled of beout.\n",
            "\n",
            "refore hen wheatsion le thered of the hings gover's\n",
            "done of the father them yet boness as creights Gest; that take which\n",
            "is veretions of meed unto alsow in the have.\n",
            "\n",
            "lorded he say he seft the glore, and\n",
            "rewauncess, saying of Jews; and is as the are of so of God, of\n",
            "the Levity, wniest satl of the distles God man anone sight are the son which\n",
            "efted shall fat of the them recone of the king.\n",
            "\n",
            "le names; now shall exalted forgering, heseavery God have even.\n",
            "\n",
            "he, that sand of the mare ap any me.\n",
            "\n",
            "o shen of Jorso have you? be reach into the any\n",
            "do enk, arver is abome is be\n",
            "him dill them: the sevants be heresoved up to him thou rewither perverchifarity of\n",
            "For, shelain to duelswife he hor evel, Yeat all douns with actood of the werverss our oare\n",
            "with. Les is not fathers: he brokens what soabs offentwent agtinth in protsod of the Let\n",
            "the wincknely shall be upon the verees for the Lord.\n",
            "\n",
            "rey will\n",
            "kigghar that a went the earth.\n",
            "\n",
            " the net of mowiteth wintion the distealiriffers.\n",
            "\n",
            "he besolece even an evil\n",
            "flessed.  lot wence sword treescors of heed auchudulfees to moruation,\n",
            "hill that the lake her ary by the conngrung agyspirs, but ther in the\n",
            "deeniricked after thee now outhers aron savente?  ay shrewse the deads whines) homen to\n",
            "the sseived unto the primitdo the recains, and sure their spakitunned.\n",
            "\n",
            " Abahil the Lord full the hwaly\n",
            "cometh ous, land ye sayoly in the blood ye shall a new he returnecin:)\n",
            "e alsee. Spoth ye have son what the host thriss for his out of theng, whose shent nor\n",
            "and died destersest of the the cruits.\n",
            "\n",
            "l rewed me ag the glored als weress bringeth.\n",
            "\n",
            "hervered is leat yover whon maith flest be\n",
            "his reeven.\n",
            "\n",
            "laby unto head, nessestes; that ye come ye chiny that\n",
            "which mongut: e the core at them and nave as the vister.\n",
            "\n",
            "he seve thise and Bar hearth, becautey; seven leted a coscen:\n",
            "ew even, and to with all these great in the wist beloned they mave shall one\n",
            "shall grethrenge offerof from the cerne.\n",
            "\n",
            "reat the sheep, tut of you priesse and wise be this the\n",
            "righ's strinkshord for my.\n",
            "\n",
            "reavest therefore.\n",
            "\n",
            "et were if ulpornt: and eleat thered, be not me, which is\n",
            "not of the nembern of Jod ny pass, to their neever compianing the down.\n",
            "\n",
            "e it eat in from which waur bowl: thot\n",
            "alonguse unto send withferiak for that saltigno to blesth's low made his one\n",
            "I reemuster; Threat all the ancere that cave of the tried, and at\n",
            "he that twoin come manchars of thee.\n",
            "\n",
            "artilfeely one priest ever?  Let us livery it med\n",
            "abod trused.\n",
            "\n",
            "e us the are saidy whtour for\n",
            "them by the shorcer regun eliveth ace do year;\n",
            "od I have curot pors of a gains, the feirl, as the lebthrref beseed the kince now\n",
            "beat, dretteningur wreeseded with the scringtss that, and aldment to puseles to\n",
            "kinged lot that spound: ewed a\n",
            "stogieth sayp thou house fights five in ap a good.\n",
            "\n",
            "he sciveny whenecitich ageithin that of the offerstrow: of do\n",
            "God son ole; belon?  eveth their nest own\n",
            "ore be, he stied, I have they gatiled goven, arted commaning,\n",
            "there come thee to place them cones.\n",
            "\n",
            " drave dreving fathers: e presincessed be doines unto the portings.\n",
            "\n",
            " slear forthing of the, that thow neming brefice the\n",
            "have not, we him that come temoh, whon thou shine his\n",
            "prayed that tesuse ye pound forty God truft, of heart is them; the vowhingersted his\n",
            "ingered Egypt.\n",
            "\n",
            "e be jund for in out Lord, and depite, the were on deceiverd: neving out the know\n",
            "doins ligetilnes, and, mase thy resure.\n",
            "\n",
            "hershied that ge aare, and the offure this from early will's\n",
            "spite.\n",
            "\n",
            "ree thou ask of reitert, ye shall before they utities, and of\n",
            "whiched to retorment.\n",
            "\n",
            "bes away spake not of\n",
            "inso thy but in waskind of who every, becumpereath unto the\n",
            "house your; whit jude to the word with with ye and to thou have way nor woud.\n",
            "\n",
            "able in favath\n",
            "as aninter the Lord is do not anosts on your placy\n",
            "be said comporsed thess, lot is not he this a\n",
            "suit, to the thouns in hisefers said treestress the recouch is\n",
            "land of as with whom that for the Lurdies of you bedengle on heavened all the; bover\n",
            "the peicerord their meard, becant foled Gyoble, and receiver wellicked said unto him.\n",
            "\n",
            "ed the evered rishad of Christ\n",
            "ain this mes everi\n",
            " ============ ============== ============= \n",
            " Mory have\n",
            "man lattercesed of besiaysalle, saidifies, that he lilved\n",
            "to he went Syoelver that they great ye that I low.\n",
            "\n",
            "\n",
            "a down that was ion judgery trus hid wor, according of hund,\n",
            "ording they make thy merely afferiting tand of in sana leve me, and\n",
            "unto though hear you.\n",
            "\n",
            "eteth's hadd give to knowlmented that thereourwoisalves if you, thou\n",
            "salt not moAntiaver: Bache tigilden fort on his is things agass of the dusnes, Gethaleth them in these\n",
            "varely arosur: bectoured he which Gesteth of\n",
            "God, by name of ysure unto this not his.\n",
            "\n",
            "hrine plece in the Gistion said: nor day\n",
            "of my man on the Lodress eliveth not in of\n",
            "Asing, and with amselves wiclount awa the Lel: not they thet higeas, let are not as gleaiteous\n",
            "of men sheel your the mendered of the house.\n",
            "\n",
            "nes wile a seeted to the golder.\n",
            "\n",
            "her thou what he have not\n",
            "the nain; but ever yet me thap have not for the heae nor soll praise:\n",
            "estersseve the words of the land in greeth: belot that he will man's limelt be she\n",
            "up, order a mare matle unto might: ove\n",
            "ever to you forth ye brothed wasdender.\n",
            "\n",
            "hrreer will our dour in the reet tith bion dwelled even not\n",
            "sprethren dod mace on the be unto\n",
            "the Lords is deatful spire yory galsem;  in and stoure his\n",
            "will the Lord up, they now ye shall have the lead: chirst of rogy, and inotherithus\n",
            "litiontsessen of the go them ly, to you blasenty sey my\n",
            "earm.\n",
            "\n",
            "ney that ye gevent of I just after me, the Son toy sity.\n",
            "\n",
            "e we to toishes answere; no he stoor uneration, unto the shall be given, and\n",
            "seever begeith that that aserst him forded.\n",
            "\n",
            " they reckentark: e which thou me: and\n",
            "said unto then, bechoun of God shall basts and Sinught cond.\n",
            "\n",
            "har thy mave youl felst med of Jebus, and before\n",
            "of bringkend compsiny not mare into the do that to men ye knows youn\n",
            "the not for a good blonden yet a grovo: age again that sereot; e for the\n",
            "heartany which dy be be desciten of of God, the knewo\n",
            "not cear nemonforns ow also the shrepale.\n",
            "\n",
            "eed ever a selvonts.\n",
            "\n",
            "rechomed bur tercat all reet uphattel.\n",
            "\n",
            "at God a twelves over\n",
            "a build, and the that out the mence aby or you betied\n",
            "not heart all sham low be dowel beered bekneileds of thy\n",
            "pade: the LORD child, and nor fefer they mout to hand.\n",
            "\n",
            " deeliveunest them which the lem thou arsouse, besorer\n",
            "ainto thy stain.  pe cat him yet thou things that all ansot: and\n",
            "thou scord we men wors of the.\n",
            "\n",
            "enne of them the striby and clorce, in thee wan whe wrild\n",
            "bore delighing not ye for untomy poirefforrest of him: that we go\n",
            "demover be worth bread not be not of Piold.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "God beenter is doon.\n",
            "\n",
            " he\n",
            "steth anow these shall explended to said\n",
            "was be yow deeples: they make the gosed unsoed the elthring, of\n",
            "\n",
            "Tebongue into have and bilder, by the have sots, I wather\n",
            "shent dosy of the hand of is not, and for for that which dild abo\n",
            "the chirctspister: bloy the divingeeth me: because\n",
            "all thas the Lord made pasel ye not that be aless the leandsalt belod are\n",
            "unto the keagden cut to him; Thil\n",
            "plesseth is eat under to sain us: this mans\n",
            "buldeth ordifuches of he saiph not I will become you: fountel said\n",
            "that shall coufty of the Lord: hear favis them the ever.\n",
            "\n",
            "arbite: becauthed to all youlf that will destan.\n",
            "\n",
            "tur wam Dew, and\n",
            "wa king, and limeth.\n",
            "\n",
            "hay hend me, we favoneds, and commanded: nubs of\n",
            "coins into: and the Lover forth now that were before him\n",
            "kempper after that men, and is dretinghed netty in; eish nezedation of the reive\n",
            "for him. For they came tentearitaketh over be ye read, e be therefule the will\n",
            "welves wherefoloth thewellf, thy bexterer whichel the fooldedney which leve that spead\n",
            "into thee, and let in his bewilon. strdence aftershized to accome not, thou have\n",
            "ruitanded of waxerorved brennest, of the sine; it me, brwainighty mamelted ye\n",
            "stering away.\n",
            "\n",
            "ruses deat of God all to you: are fefillested that it in the\n",
            "serithny.\n",
            "\n",
            "reat live to we litter the goodeding. Then wored the coundsty you almont.\n",
            "\n",
            "low let\n",
            "the dustle in the lot.\n",
            "\n",
            "ed whoat what tutnesseled tird, the said he mear to stiads of Ballest; which\n",
            "tebren abounismots into now\n",
            "their borndest thumny, su woul, and a fromed that had in the holy wherefore, that they gat\n",
            "God one do now then.\n",
            "\n",
            "e swere they verecaring: Godlore he dessieve all the\n",
            "daversloed bew ye gment fich eyes.\n",
            "\n",
            "eed that ye flow thoushen Jesusmai, and becaculsoun.\n",
            "\n",
            "hip ore stound of the not; and\n",
            "he God that chist the eat onrathers.\n",
            "\n",
            "elvers were is unto his saying.\n",
            "\n",
            "eexeth arver that is my\n",
            "froch him loon: that\n",
            "to his laided now end wice, and we troke with\n",
            "the bucrendied agains in the deluated.\n",
            "\n",
            "ever for ears bounst thie day\n",
            "speak unto the LORD blest three your wordes it we that thme have\n",
            "altied assoul.\n",
            "\n",
            "nether he sablann of the day whom shale hath with not is\n",
            "pronse to blooth braottered his nother, and tfese with the crues ont; is to ke\n",
            "of\n",
            "the beaven stoure; neigetherdechgd be from the have blode\n",
            "didned a tabery thing, twoy his goovoble for ascame.\n",
            "\n",
            "ebet tersid come: forth the said uftil the spoven\n",
            "live tase saatimithenisters, dmade sin, luttrerighbberothernored in the cours\n",
            " ============ ============== ============= \n",
            " Whose I we\n",
            "remintay of the Leftains, that they receine one owed dultifarth: lested theoed go do\n",
            "be spity that in had of the protes, and tartanding shall no\n",
            "not of daying, and amest not my evele charriketh, and the derat.\n",
            "\n",
            " house over alleth Jesum and us\n",
            "word rake despire spaciptcred blooder: e bedifuste not Lord, and ye\n",
            "\n",
            "els. go the set asa mastered.\n",
            "\n",
            "Wo ango\n",
            "will brether the kigntel they thou shein to alt of themengero vely.\n",
            "\n",
            "e verelves low: ee are\n",
            "sheys, it year is spoal it contileithinth that hen also\n",
            "should domest gasion: for heaven what of his lavers of the compass.\n",
            "\n",
            " the lifes, werrsmed unto that agethincees lotgry math\n",
            "that brother in the son unto me.\n",
            "\n",
            "hace ne, which should eseroth\n",
            "and commind tull to the sait: at it al three earth do devited\n",
            "covetuts lope this breing wither curined soll lop?  for is the\n",
            "God but to dowen and and alsoah: whos on the king standerlfes, by wordah priveth up\n",
            "not dryings of?  lesed is an was aglea, ye me word behusulsutated unto warketh come they\n",
            "dopatt them him in the preacle of his wast.\n",
            "\n",
            "hosed over the inothersel the savidimube God one of Jesus was\n",
            "in morut.\n",
            "\n",
            "as to afture the trublesteth is nation: the way of\n",
            "hafter thine.\n",
            "\n",
            "her whose of that surned worgit Fathers otheries; ut the heave of the land ot them\n",
            "with Lover thee, and thereforg thou man to brake the fee.\n",
            "\n",
            "us heselfoleved the LORD, (and land in the will godifieth, whick dase longus\n",
            "mile he? I he well things we reveither to the anwed for I\n",
            "regewernce in brother is poithing.\n",
            "\n",
            " he thine cornth to rights of uplasters to valleny shall be\n",
            "done be these almonty God, the Fave redenge in as in an with him hinst\n",
            "Koreb: and to ye with a dof: but that thather\n",
            "distion mone are stet it.\n",
            "\n",
            "rewillf peassel he trike hat strander longs of belly: with the heart\n",
            "thruist onot ly.\n",
            "\n",
            "hory their delivens ye there mead he say. Therose unto he cest's pothe, and beway\n",
            "also to the keard, we lace owher, is fans shalt net\n",
            "of the land, and which with God.\n",
            "\n",
            " may death into in stre sight he\n",
            "saut the Levouter to down of Joch, and the coming the\n",
            "dich, and the fereot, whot unto the done so we.\n",
            "\n",
            "rantes is all the Lord from the piows\n",
            "that stoks wliffey affeust if, net of Mye.\n",
            "\n",
            "rave that I seat sut consiter and, alde things, the entarbs of him their\n",
            "verus, but Begound, they which in mids in that not begeiffled by the frethershart.\n",
            "\n",
            "ed, Lare his of ryiring thircastarf not spake yet\n",
            "an desilever them wit and of the crething signted at\n",
            "which was of in the words: us own destable.\n",
            "\n",
            " God whele undah.\n",
            "\n",
            "\n",
            "Jerusalion to the name is them thee come the\n",
            "edoales from the therlipest the witder on eves: and\n",
            "on the derir, that thou bighed mire the Lorders of the labnes; and\n",
            "they the ponss the seek thee, a that hear to not that est of fore the Levizetif,\n",
            "that it I was sear me me not werrinst fastion the give to the drengerned bubtabe not terust, bedmy were\n",
            "frutters that wither.\n",
            "\n",
            "eltiful co the words are day ton trithour is fles: buttreth it caurled\n",
            "the Son of God judge intoor was day king, by the heard or them: stifepives, which\n",
            "first rose vesels;\n",
            "by the\n",
            "the s thiel feftey of a wey to ancerven thiness.\n",
            "\n",
            "reavest hosed of the Lord.\n",
            "\n",
            "hat thoun ashe that our\n",
            "him, and not hone orders; idnssed he brefore them abour herw wrutter Coolden,\n",
            "the Lord Led excressest for you, that all mekeathbimitter that a fowcters as the contance untoo thidle.\n",
            "\n",
            "sok with their frether the the strung the ) there in also terranablestles.\n",
            "\n",
            "reaved he mory lithers also hast of the\n",
            "sturn; for he crievined unto his spireth jessife.\n",
            "\n",
            "hous in strabured at the comring af the mugH the day\n",
            "sain byether hade by the leth blessisudled from the evin: edich were\n",
            "padarisaions; for you, baktenesl the tonge to reut good in at treessel: belowen, it\n",
            "great: als a son of as countightefest; even shen of the LORD,\n",
            "and the end voiny, as they hath of not God of God yet of criested for he\n",
            "stoo-ha.  esles, and hand to mudgutent be hands, that manish thou spirst and was: ive\n",
            "asto that shall be not that lot, our man to hand.\n",
            "\n",
            "e to kingth these awrot the Lord of the Gors.\n",
            "\n",
            " houther, and it which dayonces of\n",
            "Azma; a prepenscred he to nst be dreganded, that\n",
            "whol cong a thim known when thereoterssity, and carterifusincess.\n",
            "\n",
            "Torseoveth to his dower one lipefts of Chrims and looved ma said\n",
            "unto the be\n",
            "nement offeritweed, the earm, of your unto the mont kingnersten congenstified to doul into\n",
            "the eatenty alt great with we the poover for halt.\n",
            "\n",
            "es for toked his scit is as this\n",
            "viceings them theled me, wet of eiens words it eathey forth\n",
            "fillort the LORD his derett be unto the blope, end if the word which daid Have thy\n",
            "God, but shave not not not spire with his frethath: e of Renke which wheit, tries\n",
            "that dessive.\n",
            "\n",
            "ree spilion with aslymends, thy ponses, and the Lethants, hatweenter that the\n",
            "land, which we that whrether be bome deshark op hearth go divethed\n",
            "ap the stiong liest strink swords a gourded every for that maga mach mage.\n",
            "\n",
            "rok from that not me, which thous out\n",
            "one brechost, in thee litse\n",
            " ============ ============== ============= \n",
            " And he will said of merci.\n",
            "\n",
            "eed, stad: and\n",
            "afted them if the deadwently ken God delises: and bus gavenend ro of\n",
            "God soust takest house might in the ton our that strangeth: for\n",
            "the contted them treeproed betionstoem usnawity of God.)  e he\n",
            "will commangienss heard cometh forth on med forth to.\n",
            "\n",
            "as a sund\n",
            "be ore once filless of Gilraies: that say\n",
            "twening were free the ply in them ruen's ays: a were ale that cutture in\n",
            "the villacession of shall proegeth\n",
            "not sward returns rabered bulled up that ye, ram whone\n",
            "for the shall nethors; The LORD have\n",
            "in\n",
            "hy maligetent in the the veriline, are the will nod made loth.\n",
            "\n",
            "ree deir that all bread that them,\n",
            "whoel elveth up, thems he\n",
            "sand they himself, were we to the wasely gosto dronk one\n",
            "dead the braffes of God whose, srace twif reeser sut taken yirness is sulsriev, even desies\n",
            "the man and forth.\n",
            "\n",
            "trije shall ye with the owere also ever; eberought by the land into them that of from the o\n",
            "for thee sew wit moutile\n",
            "servele, ance even that well be flamings of that brother is be vasider\n",
            "shave, twiper wather wicked Kenabed all them dogeth hearmest them:\n",
            "and their nest are from his feth is into the reteruse, luw he to\n",
            "seed thereor.\n",
            "\n",
            "her, and be son unto you be will dassoves, he saught way you belsoy\n",
            "strestly bread not were now, ald God by mirst the\n",
            "that them that thesriont; I shen uneth the words into the fentther befoyed met.\n",
            "\n",
            "ree haver lamg not the resters, thoy not steth's should aglemusa\n",
            "ginded firleingy, lot they withorsey respeversureth the seobed\n",
            "folled.\n",
            "\n",
            "e we Son ipheth the warleds, and weitiop that which poon isousn we me mars what scord.\n",
            "\n",
            "out a work aoals and worns of\n",
            "Grvad we be on the veranselfore the inger hid sury?  hergoon ye\n",
            "havers.\n",
            "\n",
            "e unto thous hosces that in estayled are vieds, and the givery onos your stone\n",
            "it all that nown: but that abogn one rede: hat of the\n",
            "candeted also, on the mugeth to becreiquitears: ep in Jeuself they\n",
            "he charinges, moled the requimeify, and his have in also woolde.\n",
            "\n",
            "hramaster besoll the wellong beg undost word,\n",
            "be hord bornss of our ome atwithers aw the bread, and the worco meet it? or\n",
            "the sand to hind perso\n",
            "evillores.\n",
            "That ever semand in Masme, that dery more you bleellieh dwelling to men, affer me keetchest of the men a distres.\n",
            "\n",
            "e awaid\n",
            "heresered to with heirse have the sin down: ever an be use.\n",
            "\n",
            "less wherefort: ye and dwelven, sure\n",
            "abour of me, and we might I aro.\n",
            "\n",
            "leas to ar beron hat protion mught the wirmmesh be gieds, and\n",
            "down thou hast perpetes, and him weoke that whose upon thee, awar thing the Levite:\n",
            "at pesches, the houtnows as all witerstiny, and Jehsell: three words it of of the hong sheslders forth.\n",
            "\n",
            "reseed alort surnunt mhars from the mother\n",
            "fratheroning are therenores to here of juster that Jeym.\n",
            "\n",
            "le saus, even I is mang ap he; Be\n",
            "had the rey lipates.\n",
            "\n",
            " morst delive\n",
            "will goodledgo now disseifeth ingme.\n",
            "\n",
            "aie, which thece saith into\n",
            "the God you from the verinccessions, chived day to ehastery before me at are\n",
            "the son.\n",
            "\n",
            "ar thee all agaying that evin the Lord.\n",
            "\n",
            "e and ase man mave it: at stwer and the mord go inayoth awil:\n",
            "oust rigemonsh Nhat have to the sunce, brake not in mighty as fich it of titest.\n",
            "\n",
            "e to forath lausedn me: ny\n",
            "onother wal be with spake alt thy fartand of it, nameleth irnush me.\n",
            "\n",
            " Father he also whan ye werring take thus\n",
            "cast it of buend.  now the brockeknes of Itwere for in the al things,\n",
            "aftain this she to diege be given; and mare thou David eremsle\n",
            "prians of themed ambetthie together with them iftion it.\n",
            "\n",
            "now dessiffey answaide seltiness: that his stone out rejoicipce of unto the\n",
            "yea, and hud steny of stive over the concelfels: arder and care do, ewet\n",
            "for even the Bewar for of come crountighten; but that chits: which is well\n",
            "greatedies fea, and Is are the man that were not\n",
            "drethings I worsted: e thir spirk aiduch berdens one not be retherst.\n",
            "\n",
            "reaved wo bedathen in Mesortess; to dister therefurse\n",
            "the Lord is amce, and all the down with honamonif.\n",
            "\n",
            "\n",
            "be for they will not get of the fille of\n",
            "Grees; aked a shisneth it spireterets; Gos say of my year yow, and they\n",
            "nive unto he mamite, and but but it.\n",
            "\n",
            " heard the the LORD thine doined satt fored fie egourst this\n",
            "juadle arsfly hen from the pelserth saccure.\n",
            "\n",
            "rre he rulfucl asgarscheripless the gave\n",
            "them things when wither shall ble thous\n",
            "God,) that were he things, and Ged: e hom that the all also we\n",
            "perpithed thy berans thou serventifed, and and thereings of Gor; therefore will men of the seoved\n",
            "with me of from which the mugeth reparift.\n",
            "\n",
            "e nor for forth golome tomy one of God froth the not, my brefore wordeat there\n",
            "rescles and to thee peeas chupless Eleth, which in the the speaves is ceven\n",
            "the filled be aglest, let come: and you that which will was.\n",
            "\n",
            "hot went whirdifuketh all the fear hit from\n",
            "at was in the dused to youcl up that seot.\n",
            "\n",
            "\n",
            "compation one ome to to the Fath him, and that walk undor the\n",
            "lobse the consile affingeredom from to the swern abong me? e morthered of by thy me the sartion.\n",
            "\n",
            " do the verievoith\n",
            " ============ ============== ============= \n",
            " And he Spirtes beoturs at\n",
            "the Levingers; and the wonds of Chrith worlded not\n",
            "gars of that his ance wintorpstaint.\n",
            "\n",
            "reat now womy blorkinge to that not ye \n",
            "is heart that have baus also\n",
            "the sintoor of Jesus, and come lot was ave to them: and that I rape yef,\n",
            "intilf aft art.\n",
            "\n",
            "reet peresees: e left goth is ne.\n",
            "\n",
            "reaven unto the mingled, saian dwelt Mesolfeed: ehite firner be be shater do gter\n",
            "theee not the world gterseth treesried up not Jesus.\n",
            "\n",
            "eas speace grettetch of the said unto God.\n",
            "\n",
            "e is aname hish is be seen which are\n",
            "said unto them.\n",
            "\n",
            "e twelp, and of Jesusam? with the prosselver so becenness\n",
            "of Samobatteth: e saupt we say things bandy that Jeasured wine, thatty\n",
            "the beway exell the scace forgth the budgelt.\n",
            "\n",
            "ne to the have peated dom blowithed ance the Lord, and\n",
            "strecked that servened ahason of Samothet is ander my pats.\n",
            "\n",
            " to macond of for in also them: peing thou ams of\n",
            "riled sul not bloots as he tor after thy pentain.\n",
            "\n",
            "ent therefoeresh ghosisations and beasts; before know the kinds it you.\n",
            "\n",
            "he that a lwayed frot their nembineth; bewauress of the knowned not horse horever them\n",
            "forgether eveny, demoft regy word, see ssan passed whoy sanch,\n",
            "and all to be mong is there, and on the havongurns: to aneithed.\n",
            "\n",
            "now dead of hagring aboth?  hain men ast cattle mide, on\n",
            "accorded the son of he raw.\n",
            "\n",
            "ested bewife a isading uputringle.\n",
            "\n",
            "rave that therrefole the know that seeth you thisness, and\n",
            "thy from borketh ye sevonn my, and all the serth.\n",
            "\n",
            "lece, and seat down liffed us fe of the batti.\n",
            "\n",
            " spilick inded who stuns you the\n",
            "Lard.\n",
            "\n",
            "ream let unto the hises award preson to brenatablothed the peaceed all that whithoum\n",
            "reneid coptide.\n",
            "\n",
            "edateth the befely rection not ancered for whereas; in the holderifeds me egave.\n",
            "\n",
            "e af the Hithertab, he wor that eving unto discers.\n",
            "\n",
            "ere was thich for the knop that he staring\n",
            "a be thing emple.\n",
            "Leth, do one frumgs, a worldage alpows\n",
            "of GOD, the houteth with them unto nemsby, the sinels of ken bread shals: there\n",
            "with not water his ne thy people.\n",
            "\n",
            "redencessese, af the content\n",
            "hisaw up as heart, neither and whom be the surong, what\n",
            "whicce away from the sinixe.\n",
            "\n",
            " therefore he alto bott\n",
            "my on let to suruntry; hat eyese knowh in ty cley men bold to coveturn of hath it throu the blaws\n",
            "lew maly after the strenti eve sons one frovin hemon, the all dreading\n",
            "in the citeded the gas not them by strening in the\n",
            "sund to deus reseked forty unto the sto know that the Lord.\n",
            "\n",
            "rong the race valling of Lettangut ye betivether\n",
            "ma, that come nos was men.\n",
            "\n",
            "he will priphed wontingus cringed dest are brongutser and.\n",
            "\n",
            " have every sheel, before with\n",
            "that the Lord.\n",
            "\n",
            " untery: but have tow my not all to Jepheth to manklatedings, whene he than\n",
            "there is the Lord, but Judach spaby in a cruen me, sett with me Dedail of\n",
            "ope viest the devilees artath wonds shall before hear\n",
            "turin the that did the me be cornd of God, was shent bort\n",
            "fartinger and wise ustil?  well wore to they\n",
            "of this veire of ye two told, thou bread not, that my preved froth.\n",
            "\n",
            "evaty of child, we work is ye shrels, and\n",
            "ryse feese to stest cany let in lengt, and for the duster alcought, whritired us\n",
            "lored and the Lopred the conson; viaish ore\n",
            "kseth have not is dread you, but they to mare your\n",
            "flee aball spool, that wick ferstarion things of eving\n",
            "no evered ams thein sin to wherefore to many, sob them the earther.\n",
            "\n",
            "inso shesperten, who beway the every all that day, hast they sholl\n",
            "eaver I wemerns let he to do God. This worlafifle; ak unto myrs, and cauldest amopt, vile out of Edrans; by\n",
            "of hearles the nont mary do oh that you, no was\n",
            "I al lapely it retersed I will reful wame.\n",
            "\n",
            " I seeth with is youredge of God; and a\n",
            "reseiveeng egrefied the plees: to to the stake mant of bught\n",
            "ye a tom not mane, Leat not ye to whergis oveth net is, not is gotains threid\n",
            "peruselowens of Judol; and a froed to me, who will surat\n",
            "by alsied in bither with that alls.\n",
            "alst prasemave keep drinesses of even also be be dread.\n",
            "\n",
            "reught yekes, and\n",
            "the brought you pliscetherleth not propeers: at then pimeth tist won the\n",
            "high that lands on them, whickeat up the samiag,\n",
            "wil stweredgrounceness I read hemblesthe to have him stong, ye\n",
            "heard also shilder with istled, which taketh ry nay in had which\n",
            "wibles, on plech.\n",
            "\n",
            "reas evered thou sover in the spear for than from\n",
            "that as ye of from things; his king ob all not, Egabim, and ye vecalles: pero that\n",
            "whose shall not with the wated through the Lord rums, and Jesuses the\n",
            "Levis man.\n",
            "\n",
            "less and lits of Jephat brepurt for arn ox he.\n",
            "\n",
            "e thou ary is graces will be are be beaven spay not him\n",
            "the caup in all and on them with the child, and over ayour for with them, the\n",
            "ground: things forle off all the hournes over my burgine: ole is she\n",
            "peoble, whens I thinessel spore beserels.\n",
            "\n",
            "he will of his unto\n",
            "me hesred beces to bus buend fe the plest; astatsy with he rejow, whild,\n",
            "let keeppord net is thy me, the watered chirstiness dother dwend recaised.\n",
            "\n",
            "neste that hef his send was clause: hall\n",
            "that wny whish seve the judg\n",
            " ============ ============== ============= \n",
            " And eth not before to should to unilderthinfused let the surighters them lawfer.\n",
            "\n",
            "recaubithest thou hongeal gosts also shall them the prophe to the\n",
            "enty alfor unto the brodgs to vilime: lest suld porition.\n",
            "\n",
            "eed tryse frrefeoked this endeshed\n",
            "Jodugh he thoughe, Becitandunteth stone yourf\n",
            "conness, yem alsorspless: alfope be the prost of the own ve.\n",
            "\n",
            "rought frostern away treeselvest, and wattely give were from\n",
            "unto you.\n",
            "\n",
            "etubless.\n",
            "\n",
            "less blindinselves, even you have yourders went rans on to God and greectue shall me, and weer from\n",
            "Moroed any two for hise that make God.\n",
            "\n",
            "Pherewiffecaincel fouls to your and, but in the an weentise, and found\n",
            "deots, that is disted their nared and to dowe: for rase\n",
            "the vewelloth an how that their not be cither is chood of of the\n",
            "brethrrenger that the shall speakeflong I were be their discher, and in\n",
            "the wilders.\n",
            "\n",
            "eluciliveth trentanolders of God be have to all dreeve, arvyssefuse al woold\n",
            "me is thesraioren wete Jessel, whiche into the spetisedger, shour\n",
            "day of ye raised ustelfose spears, and his\n",
            "blone sart he soinn with yorniss thruist belor nother's down rlatt for for\n",
            "the earthed, and the offer a called.\n",
            "\n",
            "her and were ny be not blood met.\n",
            "\n",
            "her thoussand from hen horselvest let of the dear;\n",
            "ty retare in fulser with with in this heath?  herinesser onfe wat dareth the will\n",
            "of the Lommbah: and that take their wise see his ountwory own are\n",
            "speak the servent.\n",
            "\n",
            "e all thie zar and the sund and Coome also that they than that whosel froitseth\n",
            "enterth, he shall speak.\n",
            "\n",
            "ree the ellield, when for one not commanty was no no wee, and\n",
            "carted the fathy son of the was our ton maclars comfildiful of yestwaned the chirint: fellove the youst of hears vine.\n",
            "\n",
            "her stoured\n",
            "he shave, as Isself, and reknows: for things wace that his onnes sadancos out dryiress.\n",
            "\n",
            "ly be not havers: The at the lawed earin; and he\n",
            "mad unto the aspersely youbeftath it is when all, thece he you blap,\n",
            "yes is not I repef.\n",
            "\n",
            " left of the house nareath to mans with rifiededed.\n",
            "\n",
            "herders, we\n",
            "love thou sancce, and the have\n",
            "ga live alowle men not thous oward, eved it,\n",
            "and to the Mabanther, a sand: estatnimither the chollaming to hosh, and\n",
            "unto a mever things gebanced.\n",
            "\n",
            "hoursely unto God; you brought the glory of the boith say, be\n",
            "in God ferven: for they the excees: that an the sies, rave of\n",
            "your in the rewed gailtiffes be Egasisment with come them, Mosess asarmened tarke to ioned captord,\n",
            "who live again the douffilles of hand fladdersce.\n",
            "\n",
            "ruted.\n",
            "\n",
            "oun begoness one wmad falle.\n",
            "\n",
            "reas desire sow to judgether heark us unto the dwanging be\n",
            "hast the chus it have to amest: but they dey are fathers.\n",
            "\n",
            "reat you unto the fold the share goives of, and\n",
            "resed med hath\n",
            "the Shiel, and fear thy God of father from the me land.\n",
            "\n",
            " not be poss. Hah an mught; and there cast also his\n",
            "in the send but them loogeth there; with they ritcyeth voinsied\n",
            "thriest a ander his pears.\n",
            "\n",
            "her make whose sholleveth bringeth know agrictes, brey thou\n",
            "suring detrow, ard chitle.\n",
            "\n",
            "ed not ye dread in the Pet; feefive the bead when have leaved\n",
            "them the Saul him had to them that hesragie youdgrers, and Phapistes.\n",
            "\n",
            "ly with\n",
            "was the LORD, and to a gooved bogeth:\n",
            "for have veraw. They wome is that it I wilt the fismaeithed: tome not sack, and wheny\n",
            "at dive the streed unto the God of the jugrtied ble into\n",
            "nation as that the weecinstarin a gaith youse for also the demoref of tie alsobles timestle\n",
            "orreffess from the live bew you be given into the rewelt, and thou shalt\n",
            "every God, their plectale thereson of Kire's of the comcass of Zeb and urnost of to hims; that I\n",
            "have I satlern dwerkitess this from that evoly than thife to dway wete coongmudiffest foot\n",
            "his chars, but loveth deard come is insther come the bredgringe to the king outh\n",
            "wast dill dendansuren ftale them.\n",
            "\n",
            "howlat is arts yoby, come thy\n",
            "bonce, that thened the long I childerledge, Resur\n",
            "called favilfich brendren.\n",
            "\n",
            "ie by the Let rettered congnase of\n",
            "your: thingso them that to commands, was piteth shameth ga me.\n",
            "\n",
            "les when ye e there, that he gaste fourny on\n",
            "them, and tued frain.\n",
            "\n",
            "ether have no hous bemortime\n",
            "overing to she from to disterciteoned Hordes werewn rai.\n",
            "\n",
            "e thousand chave them.\n",
            "\n",
            "ouse the wosdares, we liliever by his burded\n",
            "it, burnoth somord unto the LORD: they wak he for me\n",
            "the ghthy.\n",
            "\n",
            "ries wrom owher whear ye pet chle or aron treear.\n",
            "\n",
            " ale the Lorder rovee;  in\n",
            "as agas son of fraystitie.\n",
            "\n",
            " is yet Judatile mane pleeppusedwe all arkion are\n",
            "you, the hund bronhortirds, even everen.\n",
            "\n",
            "ar dripeed went were as headmer tfering that bley these\n",
            "that whoce unto not into the end wisond the lapa, the seft Chil; ied unto.\n",
            "\n",
            "ated when their hen\n",
            "coment to Elincens we daverk; now debred unto hive mane, that the\n",
            "lether into that God contooms: e me upa, because accosed the heath with as that\n",
            "we thisbought, is som seept nouf the hoving, which he that thy nace unto me, and\n",
            "the law buints wrickesther is our fiest me.\n",
            "\n",
            "hear out recevie to do\n",
            "which bucle tries from the halt by them wont of the God.\n",
            "\n",
            "\n",
            " ============ ============== ============= \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ZYmtHeiaMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ytpJLTjl7wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(line_size):\n",
        "  ch_to_ind(start_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wepVU2woiFJ",
        "colab_type": "text"
      },
      "source": [
        "# **Bonus Section - Applying Language model**\n",
        "\n",
        "Please note that we haven't tried to directly replace LORD with LOOD in bble ---> retrain the model and compare due to compute resource crunch. Instead we would be usoing the same model to compare between different words. Most interesting results were found for Abrams vs abrams.\n",
        "\n",
        "**There is also probability of start sequence, but since we always give the first character as start sequence, its probability is artifically 1.**\n",
        "\n",
        "**Without stateful = True, Rnn might not remember its predecessors important information** and hence cannot use the historical info to update current information. This may generate characters with probability independent of each other. More like what happens in Naive Bayes where there is an assumption of **conditional independence**.\n",
        "\n",
        "Probabilities for model of 450 activations and loss function 2:\n",
        "\n",
        "**Take LORD and LOOD for example**\n",
        "**Summary:** \n",
        "Feed network with start_sequence_char S_tag ---> Find probability of L. (a)\n",
        "\n",
        "\n",
        "Now feed L to network --> Find probability of O (b)\n",
        "\n",
        "Feed O to network again ---> find probability of R (c)\n",
        "\n",
        "Feed R to network again ---> find probability of D (d)\n",
        "\n",
        "Probability of word LORD is a*b*c*d and take log of it according to the formula.\n",
        "\n",
        "Now to the same with word LOOD.\n",
        "\n",
        "The log probability of LORD > log prob of LOOD.\n",
        "\n",
        "Do this for words \n",
        "\n",
        "2) **GOD vs DOG vs dog**\n",
        "\n",
        "3) **Abram vs abram vs Arbam**\n",
        "\n",
        "**Summary:** As expected for case 1 log prob of LORD > LOOD and same holds for 2ndd words pro(dog) > prob(GOD)\n",
        "\n",
        "\n",
        "**Log probability for GOD vs DOG vs dog are : -27.079181671142578 , -47.95939636230469, -34.88587951660156**\n",
        "\n",
        "**Log probability for LORD vs LOOD are : -27.39081382751465 , -45.164154052734375** \n",
        "\n",
        "**But we see that prob(Abram) > prob(abram) > prob(Arbam) Log probability for Abram vs abram vs Arbam are : -33.820594787597656 , -28.015478134155273, -41.56006240844726**\n",
        "\n",
        "**Reason:** Even though Abram is a name/Noun and starts with capital A and probability of whole word Abram > abram as per simple word search in the king james bible.\n",
        "\n",
        "**But thing is we are generating character by character, hence probability of start --> A --> b  << start --> a --> b as seen below. This results in small \"abrams\" having more probability than camel case \"Abrams\"**\n",
        "\n",
        "current character and proba: A, -18.6795711517334\n",
        "\n",
        "current character and proba: b, -25.245803833007812\n",
        "\n",
        "current character and proba: r, -25.974218368530273\n",
        "\n",
        "current character and proba: a, -30.421810150146484\n",
        "\n",
        "current character and proba: m, -33.820594787597656\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: a, -11.571039199829102\n",
        "\n",
        "current character and proba: b, -15.479758262634277\n",
        "\n",
        "current character and proba: r, -19.45343017578125\n",
        "\n",
        "current character and proba: a, -24.344633102416992\n",
        "\n",
        "current character and proba: m, -28.015478134155273\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "Probabilities for model of 512 activations and loss function 2:\n",
        "\n",
        "**Take LORD and LOOD for example**\n",
        "**Summary:** \n",
        "Feed network with start_sequence_char S_tag ---> Find probability of L. (a)\n",
        "\n",
        "\n",
        "Now feed L to network --> Find probability of O (b)\n",
        "\n",
        "Feed O to network again ---> find probability of R (c)\n",
        "\n",
        "Feed R to network again ---> find probability of D (d)\n",
        "\n",
        "Probability of word LORD is a*b*c*d and take log of it according to the formula.\n",
        "\n",
        "Now to the same with word LOOD.\n",
        "\n",
        "The log probability of LORD > log prob of LOOD.\n",
        "\n",
        "current character and proba: L, -15.66189193725586\n",
        "\n",
        "current character and proba: O, -15.690693855285645\n",
        "\n",
        "current character and proba: R, -19.119600296020508\n",
        "\n",
        "current character and proba: D, -19.31654167175293\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: L, -13.107548713684082\n",
        "\n",
        "current character and proba: O, -15.677525520324707\n",
        "\n",
        "current character and proba: O, -21.368053436279297\n",
        "\n",
        "current character and proba: D, -23.021589279174805\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "Log probability for LORD vs LOOD are : -19.31654167175293 , -23.021589279174805\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2) **GOD vs DOG vs dog vs god**\n",
        "\n",
        "current character and proba: G, -11.321189880371094\n",
        "\n",
        "current character and proba: O, -13.071130752563477\n",
        "\n",
        "current character and proba: D, -14.047483444213867\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: D, -2.5910139083862305\n",
        "\n",
        "current character and proba: O, -8.607620239257812\n",
        "current character and proba: G, -15.203033447265625\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: d, -11.219135284423828\n",
        "\n",
        "current character and proba: o, -20.93477439880371\n",
        "\n",
        "current character and proba: g, -29.923145294189453\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: g, -8.381122589111328\n",
        "\n",
        "current character and proba: o, -16.48155975341797\n",
        "\n",
        "current character and proba: d, -22.021503448486328\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "Log probability for GOD vs DOG vs dog vs god are : -14.047483444213867 , -15.203033447265625, -29.923145294189453, -22.021503448486328\n",
        "\n",
        "3) **Abram vs abram vs Arbam**\n",
        "\n",
        "current character and proba: A, -5.761716365814209\n",
        "\n",
        "current character and proba: b, -10.800869941711426\n",
        "\n",
        "current character and proba: r, -14.162717819213867\n",
        "\n",
        "current character and proba: a, -19.593551635742188\n",
        "\n",
        "current character and proba: m, -22.035017013549805\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: a, -12.653196334838867\n",
        "\n",
        "current character and proba: b, -17.244342803955078\n",
        "\n",
        "current character and proba: r, -25.514951705932617\n",
        "\n",
        "current character and proba: a, -28.499645233154297\n",
        "\n",
        "current character and proba: m, -32.45140075683594\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "current character and proba: A, -19.785926818847656\n",
        "\n",
        "current character and proba: r, -31.840320587158203\n",
        "\n",
        "current character and proba: b, -39.851715087890625\n",
        "\n",
        "current character and proba: a, -43.39006423950195\n",
        "\n",
        "current character and proba: m, -50.917354583740234\n",
        "\n",
        "=========== ============= ========\n",
        "\n",
        "Log probability for Abram vs abram vs Arbam are : -22.035017013549805 , -32.45140075683594, -50.917354583740234\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmGSCozMqIXl",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1eL5U6eyZihECmWErZHoQx0nzrIe62UOJ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z0O_3zyonJY",
        "colab_type": "code",
        "outputId": "393b06e4-b86d-4e08-89f0-5929a513082f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "predicted_char_logits = language_model2(start_sequence_onehot_encode)\n",
        "predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "print(predicted_char_softmax)\n",
        "np_array = predicted_char_softmax.numpy()\n",
        "tmp_array = np_array.flatten()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[2.12039536e-14 8.37471809e-15 6.85631348e-13 8.74304291e-08\n",
            "   1.15932475e-09 4.41184334e-08 7.35895123e-09 2.03050476e-11\n",
            "   1.02917417e-08 2.89543340e-08 8.89758336e-11 7.39147077e-10\n",
            "   1.96617167e-11 3.30223280e-08 2.45882481e-10 2.64938649e-09\n",
            "   1.79954870e-11 2.02320621e-07 9.12394049e-10 2.75503509e-10\n",
            "   5.95913874e-10 1.63898321e-07 8.41319991e-09 1.05264723e-14\n",
            "   2.74374468e-10 3.19579263e-14 6.40001451e-15 1.45123458e-09\n",
            "   1.19471666e-09 4.23740348e-11 2.39680470e-10 2.94248048e-06\n",
            "   4.62800775e-09 1.62578018e-09 6.62761512e-11 2.94862925e-08\n",
            "   2.98785552e-09 4.48625270e-09 1.33363084e-14 3.70850739e-10\n",
            "   2.02745709e-09 2.27049828e-08 1.49620067e-08 3.19294591e-11\n",
            "   3.44046369e-09 1.51796242e-09 9.96089935e-01 7.26451954e-10\n",
            "   6.70469347e-10 3.90631286e-03 5.33783739e-10 8.51387290e-15\n",
            "   6.16130980e-10 2.25722260e-10 3.43686092e-14 1.74746224e-11\n",
            "   1.45528505e-08 4.94860366e-08 1.35537581e-08 1.97638461e-10\n",
            "   7.24767329e-11 2.70808848e-10 1.57273938e-09 1.30726638e-08\n",
            "   5.70780809e-14 2.36131736e-09 4.16624374e-12 3.01990863e-11\n",
            "   7.32685617e-15 9.03677300e-10 7.29559635e-10 3.93786763e-08\n",
            "   1.20540355e-09 8.72895889e-09 2.66994018e-14 5.32292521e-09\n",
            "   9.71954976e-15 4.11855584e-11]]], shape=(1, 1, 78), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.627615e-11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsV07S6PonTc",
        "colab_type": "code",
        "outputId": "1ac11daa-b52f-4663-d79c-66c8757b085f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ch_to_ind[\"L\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ6DD73KonRB",
        "colab_type": "code",
        "outputId": "9c025f83-d5fd-4197-9e35-1ca6d67b5514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Character index and the highest probability is : {}, {}\".format(46, tmp_array[46]))\n",
        "print(\"Character string with highrst probability: {}\".format(ind_to_ch[46]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character index and the highest probability is : 46, 0.9960899353027344\n",
            "Characyer string with highrst probability:  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9lVoIAoonNQ",
        "colab_type": "code",
        "outputId": "38f6bb14-3fcc-4f20-cd66-fdba9b02bd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# tf.expand_dims(tf.expand_dims(tf.one_hot(indices = start_ind, depth = vocab_size), 0), 0)\n",
        "log_prob = 1.0\n",
        "predicted_char_logits=None\n",
        "predicted_char_softmax=None\n",
        "string = \"LORD\"\n",
        "current_char = None\n",
        "for i in range(len(string)):\n",
        "  if(i == 0):\n",
        "    predicted_char_logits = language_model2(start_sequence_onehot_encode)\n",
        "    predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "    np_array = predicted_char_softmax.numpy()\n",
        "    tmp_array = np_array.flatten()\n",
        "    \n",
        "    actual_prob = tmp_array[ch_to_ind[string[i]]]\n",
        "    log_prob = log_prob * tf.math.log(actual_prob)\n",
        "    print(\"current character and proba: {}, {}\".format(string[i], log_prob))\n",
        "    current_char = string[i]\n",
        "  else:\n",
        "    current_char_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ch_to_ind[current_char], depth = vocab_size), 0), 0)\n",
        "    predicted_char_logits = language_model2(current_char_onehot_encode)\n",
        "    predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "    np_array = predicted_char_softmax.numpy()\n",
        "    tmp_array = np_array.flatten()\n",
        "    \n",
        "    actual_prob = tmp_array[ch_to_ind[string[i]]]\n",
        "    log_prob = log_prob * tf.math.log(actual_prob)\n",
        "    print(\"current character and proba: {}, {}\".format(string[i], log_prob))\n",
        "    current_char = string[i]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: L, -23.39385414123535\n",
            "current character and proba: O, 9.980377197265625\n",
            "current character and proba: R, -17.146482467651367\n",
            "current character and proba: D, 0.0199663657695055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L5vZaaG0Gw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_log_prob_for_string(string):\n",
        "  log_prob = 0.0\n",
        "  predicted_char_logits=None\n",
        "  predicted_char_softmax=None\n",
        "  string = string\n",
        "  current_char = None\n",
        "  for i in range(len(string)):\n",
        "    if(i == 0):\n",
        "      predicted_char_logits = language_model2(start_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "      np_array = predicted_char_softmax.numpy()\n",
        "      tmp_array = np_array.flatten()\n",
        "      \n",
        "      actual_prob = tmp_array[ch_to_ind[string[i]]]\n",
        "      log_prob = log_prob + tf.math.log(actual_prob)\n",
        "      print(\"current character and proba: {}, {}\".format(string[i], log_prob))\n",
        "      current_char = string[i]\n",
        "    else:\n",
        "      current_char_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ch_to_ind[current_char], depth = vocab_size), 0), 0)\n",
        "      predicted_char_logits = language_model2(current_char_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output2(predicted_char_logits)\n",
        "      np_array = predicted_char_softmax.numpy()\n",
        "      tmp_array = np_array.flatten()\n",
        "      \n",
        "      actual_prob = tmp_array[ch_to_ind[string[i]]]\n",
        "      log_prob = log_prob + tf.math.log(actual_prob)\n",
        "      print(\"current character and proba: {}, {}\".format(string[i], log_prob))\n",
        "      current_char = string[i]\n",
        "  print(\"=========== ============= ========\")\n",
        "  print()\n",
        "  return log_prob;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHb_77LY0S46",
        "colab_type": "code",
        "outputId": "5307cbfb-f621-46dc-ab80-157367e3dbc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "print(\"Log probability for LORD vs LOOD are : {} , {}\".format(find_log_prob_for_string(\"LORD\"),find_log_prob_for_string(\"LOOD\")))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: L, -25.012290954589844\n",
            "current character and proba: O, -26.0313720703125\n",
            "current character and proba: R, -27.389360427856445\n",
            "current character and proba: D, -27.39081382751465\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: L, -26.758562088012695\n",
            "current character and proba: O, -27.96279525756836\n",
            "current character and proba: O, -44.068145751953125\n",
            "current character and proba: D, -45.164154052734375\n",
            "=========== ============= ========\n",
            "\n",
            "Log probability for LORD vs LOOD are : -27.39081382751465 , -45.164154052734375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BFCCvWz08bY",
        "colab_type": "code",
        "outputId": "31cfb9ab-eeb0-4fa9-c871-5f60d4ac52c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "print(\"Log probability for GOD vs DOG vs dog are : {} , {}, {}\".format(find_log_prob_for_string(\"GOD\"),find_log_prob_for_string(\"DOG\"),find_log_prob_for_string(\"dog\")))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: G, -26.17877197265625\n",
            "current character and proba: O, -26.806764602661133\n",
            "current character and proba: D, -27.079181671142578\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: D, -20.507095336914062\n",
            "current character and proba: O, -33.156158447265625\n",
            "current character and proba: G, -47.95939636230469\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: d, -18.361841201782227\n",
            "current character and proba: o, -30.30622100830078\n",
            "current character and proba: g, -34.88587951660156\n",
            "=========== ============= ========\n",
            "\n",
            "Log probability for GOD vs DOG vs dog are : -27.079181671142578 , -47.95939636230469, -34.88587951660156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUAjzvEr08iq",
        "colab_type": "code",
        "outputId": "babfe409-b898-4634-f29b-7b8e0e1367d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "print(\"Log probability for Abram vs abram vs Arbam are : {} , {}, {}\".format(find_log_prob_for_string(\"Abram\"),find_log_prob_for_string(\"abram\"),find_log_prob_for_string(\"Arbam\")))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: A, -18.6795711517334\n",
            "current character and proba: b, -25.245803833007812\n",
            "current character and proba: r, -25.974218368530273\n",
            "current character and proba: a, -30.421810150146484\n",
            "current character and proba: m, -33.820594787597656\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: a, -11.571039199829102\n",
            "current character and proba: b, -15.479758262634277\n",
            "current character and proba: r, -19.45343017578125\n",
            "current character and proba: a, -24.344633102416992\n",
            "current character and proba: m, -28.015478134155273\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: A, -19.575712203979492\n",
            "current character and proba: r, -28.545684814453125\n",
            "current character and proba: b, -34.47380828857422\n",
            "current character and proba: a, -36.628562927246094\n",
            "current character and proba: m, -41.560062408447266\n",
            "=========== ============= ========\n",
            "\n",
            "Log probability for Abram vs abram vs Arbam are : -33.820594787597656 , -28.015478134155273, -41.560062408447266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkr3d8qk-69U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_log_prob_for_string_512(string):\n",
        "  log_prob = 0.0\n",
        "  predicted_char_logits=None\n",
        "  predicted_char_softmax=None\n",
        "  string = string\n",
        "  current_char = None\n",
        "  for i in range(len(string)):\n",
        "    if(i == 0):\n",
        "      predicted_char_logits = language_model_512(start_sequence_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output(predicted_char_logits)\n",
        "      np_array = predicted_char_softmax.numpy()\n",
        "      tmp_array = np_array.flatten()\n",
        "      \n",
        "      actual_prob = tmp_array[ch_to_ind[string[i]]]\n",
        "      log_prob = log_prob + tf.math.log(actual_prob)\n",
        "      print(\"current character and proba: {}, {}\".format(string[i], log_prob))\n",
        "      current_char = string[i]\n",
        "    else:\n",
        "      current_char_onehot_encode = tf.expand_dims(tf.expand_dims(tf.one_hot(indices = ch_to_ind[current_char], depth = vocab_size), 0), 0)\n",
        "      predicted_char_logits = language_model_512(current_char_onehot_encode)\n",
        "      predicted_char_softmax = softmax_output(predicted_char_logits)\n",
        "      np_array = predicted_char_softmax.numpy()\n",
        "      tmp_array = np_array.flatten()\n",
        "      \n",
        "      actual_prob = tmp_array[ch_to_ind[string[i]]]\n",
        "      log_prob = log_prob + tf.math.log(actual_prob)\n",
        "      print(\"current character and proba: {}, {}\".format(string[i], log_prob))\n",
        "      current_char = string[i]\n",
        "  print(\"=========== ============= ========\")\n",
        "  print()\n",
        "  return log_prob;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRsAC0H--7EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "986c5975-c0f8-41be-fa03-5ae6abd851f2"
      },
      "source": [
        "print(\"Log probability for LORD vs LOOD are : {} , {}\".format(find_log_prob_for_string_512(\"LORD\"),find_log_prob_for_string_512(\"LOOD\")))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: L, -15.66189193725586\n",
            "current character and proba: O, -15.690693855285645\n",
            "current character and proba: R, -19.119600296020508\n",
            "current character and proba: D, -19.31654167175293\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: L, -13.107548713684082\n",
            "current character and proba: O, -15.677525520324707\n",
            "current character and proba: O, -21.368053436279297\n",
            "current character and proba: D, -23.021589279174805\n",
            "=========== ============= ========\n",
            "\n",
            "Log probability for LORD vs LOOD are : -19.31654167175293 , -23.021589279174805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5ZEVKGw-7Ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "1fcac220-eaf9-4803-895e-cd4e5d15fa83"
      },
      "source": [
        "print(\"Log probability for GOD vs DOG vs dog are : {} , {}, {}, {}\".format(find_log_prob_for_string_512(\"GOD\"),find_log_prob_for_string_512(\"DOG\"),find_log_prob_for_string_512(\"dog\"), find_log_prob_for_string_512(\"god\")))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: G, -11.321189880371094\n",
            "current character and proba: O, -13.071130752563477\n",
            "current character and proba: D, -14.047483444213867\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: D, -2.5910139083862305\n",
            "current character and proba: O, -8.607620239257812\n",
            "current character and proba: G, -15.203033447265625\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: d, -11.219135284423828\n",
            "current character and proba: o, -20.93477439880371\n",
            "current character and proba: g, -29.923145294189453\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: g, -8.381122589111328\n",
            "current character and proba: o, -16.48155975341797\n",
            "current character and proba: d, -22.021503448486328\n",
            "=========== ============= ========\n",
            "\n",
            "Log probability for GOD vs DOG vs dog are : -14.047483444213867 , -15.203033447265625, -29.923145294189453, -22.021503448486328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrXZJMRs_LTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "75951e65-c8a2-4a49-9010-5072b2d6cf4b"
      },
      "source": [
        "print(\"Log probability for Abram vs abram vs Arbam are : {} , {}, {}\".format(find_log_prob_for_string_512(\"Abram\"),find_log_prob_for_string_512(\"abram\"),find_log_prob_for_string_512(\"Arbam\")))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current character and proba: A, -5.761716365814209\n",
            "current character and proba: b, -10.800869941711426\n",
            "current character and proba: r, -14.162717819213867\n",
            "current character and proba: a, -19.593551635742188\n",
            "current character and proba: m, -22.035017013549805\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: a, -12.653196334838867\n",
            "current character and proba: b, -17.244342803955078\n",
            "current character and proba: r, -25.514951705932617\n",
            "current character and proba: a, -28.499645233154297\n",
            "current character and proba: m, -32.45140075683594\n",
            "=========== ============= ========\n",
            "\n",
            "current character and proba: A, -19.785926818847656\n",
            "current character and proba: r, -31.840320587158203\n",
            "current character and proba: b, -39.851715087890625\n",
            "current character and proba: a, -43.39006423950195\n",
            "current character and proba: m, -50.917354583740234\n",
            "=========== ============= ========\n",
            "\n",
            "Log probability for Abram vs abram vs Arbam are : -22.035017013549805 , -32.45140075683594, -50.917354583740234\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}